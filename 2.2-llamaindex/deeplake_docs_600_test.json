{
    "queries": {
        "548cf161-c5c0-47f8-b1a0-0a9d4e671e50": "Here are five questions based on the provided context:",
        "51d0ca50-c8cf-46cb-8766-0f2de56842d8": "Here are some potential questions that can be asked based on the provided context information:",
        "4470ecb5-f093-4712-882a-2049668d4aac": "Here are some potential questions that cover diverse aspects of the context information:",
        "369e312c-8ece-4c92-a81d-0b9541b50db8": "Based on the provided context, here's one question:",
        "5f510598-e49a-44e9-8f0a-907ee8cba780": "Here are some potential questions that could be included in an upcoming quiz/examination based on the provided context:",
        "05bf8662-c342-4155-8e16-d2ac29419d19": "Here are five questions based on the context information:",
        "17205696-7940-48a6-84d3-f99fc18e445f": "Here are some potential questions that you could ask based on the provided context information:",
        "7b3d35cc-b857-454a-888b-1114b205b64b": "Here are five questions based on the provided context information:",
        "aa3a6b46-b6fe-4971-b2c6-8dd39b77c58f": "Here are five questions that can be asked based on the given context:",
        "7025dd5c-4152-4a85-b7e9-168d66ae235f": "Here are five questions based on the provided context information:",
        "9ecc8d12-4aa2-4e72-a2bf-08331422307d": "Here are five questions based on the provided context:",
        "53075fb7-fbce-4684-9e0c-941c9f5bb866": "Here are five questions based on the provided context:",
        "4a741322-b003-4c08-8ff4-35588179315e": "Here are some potential questions based on the provided context information:",
        "900981c7-e057-468a-870e-4f73280bea01": "Here are five questions based on the provided context information:",
        "b97e8ac1-c41c-4935-b6aa-922ae1ae6ed6": "Here are five questions based on the provided context:",
        "cf6b3251-7b2e-4320-b7f4-8cad958f5197": "Here are some questions that can be asked based on the provided context:",
        "86a9b548-f701-4193-8ed4-c7682f57c642": "Here are five questions based on the provided context information:",
        "528576d4-513f-4e25-b20d-47a20097d3ed": "Here are five potential questions that can be asked based on the given context:",
        "d5cb09c2-7b9b-4063-98f1-57b62c97b202": "Here are five diverse questions based on the provided context information:",
        "9c30e118-5037-4e5c-ae5c-f95920b800e7": "Here are some questions based on the provided context information:",
        "b7fdb6c9-0ebf-4a68-a70d-a632e833fb8e": "Here are some potential questions that can be derived from the given context:",
        "bb917a44-5c8f-4a13-9d37-6ba99390e62a": "Here are five questions that can be asked based on the given context:",
        "5ad1c81f-2a1e-4aad-87da-d73237be3620": "Here are 5 questions based on the context information:",
        "389e65a5-fb2e-46c4-8b75-f86656c42a79": "Here are 5 questions that can be asked based on the provided context:",
        "b82ad47e-8afb-41e8-b249-5d826b2bead5": "Here are five questions based on the provided context:",
        "eedf68c4-2601-4798-b0da-1d0287705904": "Here are some potential questions that can be generated based on the provided context:",
        "eb190a76-8248-4e97-b7bb-4e58c7d0d7da": "Here are five questions that can be generated based on the provided context information:",
        "75cfbddc-358a-4298-bf2f-c7649911a69f": "Here are five questions based on the provided context information:",
        "b1e5e50a-b0be-43f1-8d09-f28d0b201a20": "Here are some potential questions based on the context information:",
        "4635ef1c-dbbd-45bc-86fe-95f9572fea59": "Here are some potential quiz questions based on the provided context information:",
        "2e624717-4c1f-4ccf-802e-6786a22602a9": "Here are five questions based on the context information:",
        "519434dc-33ec-4d0d-abc3-529dc590424b": "Here are some potential questions based on the context information:",
        "039e73fc-7d4e-4abc-aa07-a45738b7de98": "Here are five questions that can be asked based on the provided context:",
        "2bed4a78-4771-4382-8d11-6f227af3b0e8": "Here are some potential questions that can be asked based on the provided context information:",
        "7a5450f9-9277-4e32-8c5f-0d7b35ca5197": "Here are some potential questions that can be asked based on the given context:",
        "c0060d19-7900-464d-a440-d7346bc6cf77": "Here are some potential questions based on the provided context:",
        "7ddeb6fd-16d9-4307-b2da-7b2ca4ecd812": "Here are some potential questions that can be generated based on the provided context:",
        "8fed457d-66a7-453c-a25b-0f1ce034dd8c": "Here are some questions based on the provided context information:",
        "884430b3-c9e5-46c8-b867-5db63fd0b5d9": "Based on the provided context information, here's a question that could be used as part of an upcoming quiz/examination:",
        "c00608bc-686f-46b0-8bfc-f608e73bc0c0": "Here are five questions based on the provided context information:",
        "03928690-c671-4483-bd1f-68922ce5a880": "Here are five questions based on the provided context information:",
        "4567d9bd-d459-4089-a960-987b4f006794": "Here are five potential questions that can be asked based on the provided context information:",
        "6717cf82-84c7-4c11-bb6a-eebce8921b0e": "Here are five questions based on the context information:",
        "16f630eb-d88d-4e6f-8112-13fed3fbd0d6": "Here are five questions that can be generated based on the provided context:",
        "5f8c85dd-3817-4130-846e-b40b26d0a7af": "Here are some potential questions that can be asked based on the given context:",
        "0bb4c13e-a799-494e-b5d7-06b52fab9464": "Based on the context, here's a possible question that can be generated:",
        "d6ba3c56-8de2-4cb4-89f9-5ddd6f6d8f42": "Here are five potential questions that can be asked based on the provided context information:",
        "dffa15d9-aca4-4523-afb8-8b9224844d32": "Here are five questions that can be asked based on the provided context:",
        "2264f239-65ea-4bf0-a1f0-788eb4b38bd7": "Here's a question that I'd like my students to answer:",
        "840f5739-71e2-48d1-9a26-d7d819586369": "Here are five questions based on the provided context information:",
        "37e444e4-c1b5-45e0-a2ab-ff0484ffc276": "Here are some potential questions that can be asked based on the context information:",
        "d6082df1-b545-4def-8f62-442dd02c431a": "Here are five diverse questions based on the given context information:",
        "144c8438-83fe-4baf-96f1-69c70ab54ee2": "Here are five questions that can be generated based on the context information:",
        "ca6f2569-fe93-4e0d-a7cc-b2aff16e57a3": "Based on the provided context, here's a generated question:",
        "d08f1531-f43c-4b5d-ae37-9c9756669338": "Here are five potential questions based on the provided context information:",
        "da43a813-35b1-4ef6-81d4-322346018039": "Here are five questions that can be asked based on the given context information:",
        "22ad02b5-6492-4847-83a8-c36881e2831e": "Here are five potential questions based on the context information:",
        "cfdbc698-dd7d-435e-8a36-45978ce7ae92": "Here are five questions based on the provided context:",
        "0f8d3903-530e-471f-9506-cbdf19b20b55": "Here's a question based on the provided context:",
        "b796c72a-7489-4b88-b3aa-fc9758f4c049": "Here are five questions that can be formed based on the given context:",
        "678d1745-09ab-4a17-8691-d5f2cc23d64a": "Based on the provided context, here's a question that can be generated for an upcoming quiz/examination as a Teacher/Professor:",
        "63c89274-1a10-4474-b9ea-005411f9be01": "Here are five questions that can be asked based on the provided context:",
        "8321bf8e-cdea-47e8-940a-051ecaf3b7d4": "Here are 5 questions that cover different aspects of the provided context:",
        "69ed11b9-8c9e-45c5-9167-faa9228f7385": "Here are some question options for the quiz/examination based on the provided context:",
        "5955feeb-2141-4c91-87c7-80332fbfcab8": "Here are five questions based on the provided context information:",
        "bf4458c3-4a36-4b7d-aaa5-f84482cc8877": "Here are 7 questions based on the provided context:",
        "bb61708f-731b-49aa-8982-b73338bd2fb0": "Here are five questions based on the context information:",
        "c56e0b7c-0f18-4669-b6da-e63408423704": "Here are five questions based on the provided context:",
        "a8455a4d-75c5-426b-a13a-a1869ed59332": "Here are some potential questions that can be asked based on the provided context:",
        "45db230d-b93a-4563-9274-b91446217944": "Here's a possible question that can be asked based on the given context information:",
        "99311678-d015-4e67-b3c9-6571cf047afe": "Here's a question that covers different aspects of the document:",
        "6397b3eb-822f-4d85-86ce-b4aaffc23181": "Based on the provided context, here's a question that covers various aspects of the query:",
        "a333f680-90a3-473f-8cfa-377c0b593986": "Here's a question that can be generated based on the given context:",
        "d9f762f0-9b2b-420b-a40b-a70a250867d3": "Here are five questions based on the provided context:",
        "fa619f76-791d-4a31-8484-aa106cfcadb8": "Here are some potential questions based on the provided context:",
        "09cdd057-6fc5-4d61-accf-a69ae2bec4f6": "Here are some potential questions that can be asked based on the given context:",
        "c8fc8986-8d23-4b57-a9be-946a79fa5b3d": "Here are some potential questions that can be asked based on the provided context:",
        "9b829e2b-09da-4271-afc9-936fbb03623f": "Here are five questions based on the provided context:",
        "5173a539-f60d-485a-9aa3-4b1c48b41df4": "Here's a question that covers a range of topics:",
        "c65c8076-fa32-4b4b-9090-db03728f2b7a": "Here are five questions based on the provided context:",
        "24c33cee-5f78-4b06-813e-14cc812f0adc": "Here's how you can create this system using Python, OpenAI, and JSON.",
        "89e33d7c-c1c1-4920-a7ae-745a8bc0de56": "Here are some questions that can be asked based on the context information:",
        "c2cf08a5-01d9-4682-9c5c-0a43d022dc69": "Here are some potential questions that you could ask a student based on the provided context information:",
        "d640f74b-413e-4d86-bb4f-d517cce43fb6": "Here are five questions based on the context information:",
        "29b2d6c1-497a-4e26-a1a3-565fcf6b1f03": "Here are five questions that can be asked based on the provided context:",
        "e460b898-c6f2-45fb-90a8-fc82d7134076": "Here are some questions based on the provided context information:",
        "0dc871d5-2918-4916-bbc8-c25d0897b606": "Here are some question ideas based on the provided context:",
        "cfbcb460-d14c-4469-aecb-2faeaba2c3ef": "Here are five questions that can be asked based on the given context information:",
        "49c36df2-d971-4e9a-8f89-46e401d1ec48": "Here's a question based on the provided context:",
        "ffecdacc-d265-4f23-91a4-bdfd14992e3b": "Here are five potential questions that can be generated based on the provided context information:",
        "b5c0f043-3368-4124-b3ea-5db5f1288883": "Here are five questions that can be asked based on the given context information:",
        "36904a92-538a-42f5-a3df-a64431a7a17e": "Here are some questions that can be generated based on the provided context:",
        "d3692c77-6207-497b-a2c6-82e2e0e62f51": "Here are five potential questions that can be used for an upcoming quiz/examination based on the provided context information:",
        "634da38f-6d46-4cc8-ac3a-f9bd4463aafc": "Here are five questions that can be generated from the given context:",
        "eb9d9695-55f7-452b-9ae8-4ce8783f9d0d": "Here are five questions based on the context information:",
        "6114c8dd-0d8e-4091-a776-349eb7a9d5e3": "Here's a question based on the context information:",
        "3ece9c39-d25a-4e32-841f-76ffecf7b80c": "Here's a question that can be asked based on the given context information:",
        "bb80039c-be9a-4d4e-bb91-c9e5e309fab1": "Here are five questions based on the provided context information:",
        "9f78f2e3-c7bc-44d8-9a82-c7dfa9f8ca8b": "Here are five questions based on the provided context:",
        "06f80e4c-c839-4ba1-abb4-e682998acb65": "Here are some potential questions based on the context information:",
        "0f867d6b-bab8-45b2-8d1b-6eb32577f018": "Here are five questions that can be asked based on the given context information:",
        "f1d0a124-7588-4dc4-91d8-6b5f2dd85392": "Here are some potential questions that can be asked based on the context information:",
        "879ee449-c314-4b9f-a2b6-880381a6c718": "Here are some questions that can be used for a quiz based on the provided context:",
        "454edc7f-20dd-43ca-b920-6e7a14ec918a": "Here are five questions based on the provided context information:",
        "9f85cb16-0d5c-4a22-bac3-fc4346aabd59": "Here are five questions based on the provided context information:",
        "8d9ac316-0834-4ace-982e-3ad797f286d2": "Here are some questions based on the provided context:",
        "d7a69ac6-8df3-4a5a-9e2a-07b5b9f52ead": "Here are five diverse questions based on the provided context information:",
        "e0dec67e-1447-4385-8dec-746f975f001a": "Here are five questions that can be asked based on the provided context information:",
        "1981027e-2b82-4152-ad29-0f21064cc7f9": "Here are five questions based on the context information:",
        "16954472-7d92-4db4-896f-bd7a8dc8d8d5": "Here are five questions based on the provided context information:",
        "e676ee97-61ff-4e23-9ec4-8dbedeed234f": "Here are five questions based on the given context information:",
        "a59d87ec-4c20-488c-87d2-a7e23f763270": "Here are five questions based on the provided context information:",
        "d8884331-76e5-40d9-81c5-2782d560fabf": "Here are some potential questions that can be generated based on the context information:",
        "a3a5c9b0-e992-4226-95ba-dc3851fa3cea": "Here's a question based on the provided context:",
        "5367a861-a1b0-4d91-a1e6-71c61878619a": "Here's a question that covers various aspects of the context information:",
        "057df539-a3b6-4d74-a597-303d8404aabd": "Here are five questions that can be asked based on the provided context information:",
        "67ca98e6-70a2-4ac5-8636-35a5ee68c432": "Here are five questions based on the provided context information:",
        "daa3a46a-e47f-4485-8514-fded793ad63a": "Here are five questions based on the provided context information:",
        "dfc174f7-a0b8-4d0d-86e4-c6cc0d0eec38": "Here's a question that can be formed from the context:",
        "953400aa-4a27-48f0-9e36-b46db02c03c2": "Here's a question that tests understanding of the code:"
    },
    "corpus": {
        "node_323": "### Avoid accessing the data of the whole column\u00b6\n\nThe column data can be accessed directly by `ds[\"column_name\"][:]`. For the\nlarge datasets this can lead to memory issues. Prefer divide the data into\nbatches and process them separately.\n\n### Consider using async data access\u00b6\n\nDeep Lake supports async data access and query. If your workflow allows async\nprocessing and benefits from that, consider using async data access. Please\nrefer to the Async Data Loader guide for the details.\n\n## Storage and Data Management\u00b6\n\n### Understand the storage differences\u00b6\n\nDeep Lake supports multiple storage backends, which are differentiated by url\nschema.\n\n  * In memory datasets: `mem://dataset_id`. These datasets are stored in memory and are not persisted. They are useful for temporary data storage and testing.\n  * Local datasets: `file://path/to/dataset`. These datasets are stored on the local disk. They are useful for local development and testing.\n  * Cloud datasets:\n\n    * AWS S3: `s3://bucket/dataset`.\n    * Azure Blob Storage: `az://container/dataset`.\n    * Google Cloud Storage: `gs://bucket/dataset`.\n\nThese datasets are useful for storing large datasets and sharing them across\nmultiple machines.\n\n### Use `mem://` for temporary data and testing\u00b6\n\nIn memory datasets are not persisted and are useful for temporary data storage\nand testing. They are automatically deleted when the process is terminated.\n\nIf you created in memory dataset and you want to persist it you can use\ndeeplake.copy method to copy the dataset to the local or cloud storage.\n\n### Avoid local storage for large datasets\u00b6\n\nLocal storage shows better latency compared to the cloud dataset. However it\nis not recommended to use local storage for large dataset as with the scale\nthe performance can be degraded.\n\nIf you plan to store large data (>=100GB) cloud storage will be more efficient\nand reliable.\n\nIf you have local dataset and you want to move it to the cloud you can use\ndeeplake.copy method to copy the dataset to the cloud storage.",
        "node_35": "We\nwould like to thank William Silversmith @SeungLab for his awesome cloud-volume\ntool.\n\n## About\n\nDatabase for AI. Store Vectors, Images, Texts, Videos, etc. Use with\nLLMs/LangChain. Store, query, version, & visualize any AI data. Stream data in\nreal-time to PyTorch/TensorFlow. https://activeloop.ai\n\nactiveloop.ai\n\n### Topics\n\npython  data-science  machine-learning  ai  computer-vision  deep-learning\ntensorflow  cv  image-processing  ml  pytorch  datasets  multi-modal  datalake\nmlops  vector-search  vector-database  large-language-models  llm  langchain\n\n### Resources\n\nReadme\n\n### License\n\nApache-2.0 license\n\n### Security policy\n\nSecurity policy\n\nActivity\n\nCustom properties\n\n### Stars\n\n**8.5k** stars\n\n### Watchers\n\n**93** watching\n\n### Forks\n\n**656** forks\n\nReport repository\n\n##  Releases 238\n\nv4.1.17 \ud83c\udf08 Latest\n\nMar 31, 2025\n\n\\+ 237 releases\n\n##  Packages 0\n\nNo packages published  \n\n##  Used by 3.2k\n\n  *   *   *   *   *   *   *   * \n\\+ 3,216\n\n##  Contributors 95\n\n  *   *   *   *   *   *   *   *   *   *   *   *   *   * \n\n\\+ 81 contributors\n\n## Languages\n\n  * Python 100.0%\n\n## Footer\n\n(C) 2025 GitHub, Inc.",
        "node_725": "In traditional information retrieval, a\nmodel often computes detailed interactions between the query and every\ndocument at an early stage, which is computationally expensive, especially\nwith large datasets. **Late interaction** , however, postpones this detailed\ninteraction until a later stage.\n\nAt the final stage of retrieval, **late interaction** occurs: each query token\nembedding interacts with the most relevant document token embeddings, using a\nsimplified comparison (e.g., cosine similarity or max similarity).\n\nThis targeted, late-stage interaction allows the model to capture fine-grained\nrelationships between query and document content without requiring full-scale\ninteractions upfront.\n\nTo use ColBERT, we can leverage the `colbert-ai` library. We'll start by\ninstalling it:\n\n    \n    \n    !pip install -U colbert-ai torch\n    \n\nIn this snippet, we are loading a pretrained ColBERT model checkpoint for use\nin information retrieval tasks. Here's what each part does:\n\n  1. **Importing Modules** : \n\n     * `Checkpoint` is a utility from ColBERT that allows loading and managing pretrained model checkpoints.\n\n     * `ColBERTConfig` provides configuration options for the ColBERT model, such as directory paths and other settings.\n\n  2. **Initializing the Checkpoint** : \n\n     * `\"colbert-ir/colbertv2.0\"` specifies the name of the pretrained checkpoint to load. This could be a path to a local model file or a remote model identifier, depending on your setup.\n\n     * `ColBERTConfig(root=\"experiments\")` sets the root directory where model-related experiments will be saved or accessed. This is useful for organizing logs, results, and intermediate files.\n\n  3. **Purpose** : \n\n     * The `ckpt` object now contains the pretrained ColBERT model and its configuration, ready to be used for tasks like ranking or embedding documents in information retrieval pipelines.\n\nThis step sets up the foundation for using ColBERT's capabilities in semantic\nsearch and ranking tasks efficiently.",
        "node_4": "Redirecting to latest/...",
        "node_201": "Running the following code will create a Deep Lake\ndataset inside of the `./animals_dl` folder.\n\nIn [58]:\n\nCopied!\n\n    \n    \n    import deeplake\n    import numpy as np\n    import os\n    \n    ds = deeplake.create('./animals_dl') # Creates the dataset\n    \n\nimport deeplake import numpy as np import os ds =\ndeeplake.create('./animals_dl') # Creates the dataset\n\nNext, let's inspect the folder structure for the source dataset './animals' to\nfind the class names and the files that need to be uploaded to the Deep Lake\ndataset.\n\nIn [59]:\n\nCopied!\n\n    \n    \n    # Find the class_names and list of files that need to be uploaded\n    dataset_folder = '/Users/istranic/ActiveloopCode/Datasets/animals'\n    \n    # Find the subfolders, but filter additional files like DS_Store that are added on Mac machines.\n    class_names = [item for item in os.listdir(dataset_folder) if os.path.isdir(os.path.join(dataset_folder, item))]\n    \n    files_list = []\n    for dirpath, dirnames, filenames in os.walk(dataset_folder):\n        for filename in filenames:\n            files_list.append(os.path.join(dirpath, filename))\n    \n\n# Find the class_names and list of files that need to be uploaded\ndataset_folder = '/Users/istranic/ActiveloopCode/Datasets/animals' # Find the\nsubfolders, but filter additional files like DS_Store that are added on Mac\nmachines. class_names = [item for item in os.listdir(dataset_folder) if\nos.path.isdir(os.path.join(dataset_folder, item))] files_list = [] for\ndirpath, dirnames, filenames in os.walk(dataset_folder): for filename in\nfilenames: files_list.append(os.path.join(dirpath, filename))\n\nNext, let's **create the dataset columns and upload data**.\n\nIn [ ]:\n\nCopied!",
        "node_580": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_529": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_639": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_764": "Deep Lake is a Database for AI powered by a storage format optimized for deep-\nlearning applications. Deep Lake can be used for:\n\n  1. Storing and searching data plus vectors while building LLM applications\n  2. Managing datasets while training deep learning models\n\nDeep Lake simplifies the deployment of enterprise-grade LLM-based products by\noffering storage for all data types (embeddings, audio, text, videos, images,\ndicom, pdfs, annotations, and more), querying and vector search, data\nstreaming while training models at scale, data versioning and lineage, and\nintegrations with popular tools such as LangChain, LlamaIndex, Weights &\nBiases, and many more. Deep Lake works with data of any size, it is\nserverless, and it enables you to store all of your data in your own cloud and\nin one place. Deep Lake is used by Intel, Bayer Radiology, Matterport, ZERO\nSystems, Red Cross, Yale, & Oxford.\n\n### Deep Lake includes the following features:\n\n**Multi-Cloud Support (S3, GCP, Azure)** Use one API to upload, download, and\nstream datasets to/from S3, Azure, GCP, Activeloop cloud, local storage, or\nin-memory storage. Compatible with any S3-compatible storage such as MinIO.\n**Native Compression with Lazy NumPy-like Indexing** Store images, audio, and\nvideos in their native compression. Slice, index, iterate, and interact with\nyour data like a collection of NumPy arrays in your system's memory.",
        "node_278": "Returns:\n\nName | Type | Description  \n---|---|---  \n`Type` |  `Type` |  A new dictionary data type.  \nSee Also\n\n:func:`deeplake.types.Struct` for a type that supports defining allowed keys.\n\nExamples:\n\nCreate and use a dictionary column:\n\n    \n    \n    ds.add_column(\"col1\", types.Dict)\n    ds.append([{\"col1\": {\"a\": 1, \"b\": 2}}])\n    ds.append([{\"col1\": {\"b\": 3, \"c\": 4}}])\n    \n    \n    \n    # Store arbitrary key/value pairs\n    ds.add_column(\"metadata\", deeplake.types.Dict())\n    \n    # Add data\n    ds.append([{\n        \"metadata\": {\n            \"timestamp\": \"2024-01-01\",\n            \"source\": \"camera_1\",\n            \"settings\": {\"exposure\": 1.5}\n        }\n    }])\n    \n\n##  `` deeplake.types.Array \u00b6\n\n    \n    \n    Array(dtype: DataType | str, dimensions: int) -> DataType\n    \n    \n    \n    Array(dtype: DataType | str, shape: list[int]) -> DataType\n    \n    \n    \n    Array(dtype: DataType | str) -> DataType\n    \n    \n    \n    Array(\n        dtype: DataType | str,\n        dimensions: int | None,\n        shape: list[int] | None,\n    ) -> DataType\n    \n\nCreates a generic array of data.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`dtype` |  `DataType | str` |  DataType | str The datatype of values in the array |  _required_  \n`dimensions` |  `int | None` |  int | None The number of dimensions/axes in the array. Unlike specifying `shape`, there is no constraint on the size of each dimension. |  _required_  \n`shape` |  `list[int] | None` |  list[int] | None Constrain the size of each dimension in the array |  _required_  \n  \nReturns:\n\nName | Type | Description  \n---|---|---  \n`DataType` |  `DataType` |  A new array data type with the specified parameters.",
        "node_646": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_606": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_713": "Restaurant name: St. Stephen's Green\n    Review: Nice place for a drink\n    Restaurant name: St. Stephen's Green\n    Review: Good drinks, good food\n    Restaurant name: Eureka! Mountain View\n    Review: Good drinks and burgers\n    Restaurant name: St. Stephen's Green\n    Review: Good drinks an easy going bartenders\n    Restaurant name: Scratch\n    Review: Just had drinks. They were good!\n    Restaurant name: Mifen101 \u82b1\u6eaa\u7c73\u7c89\u738b\n    Review: Feel like I\u2019m back in China.\n    Restaurant name: Ludwigs Biergarten Mountain View\n    Review: Beer is fresh tables are big feel like a proper beer garden\n    Restaurant name: Seasons Noodles & Dumplings Garden\n    Review: Comfort food, excellent service! Feel like back to home.\n    Restaurant name: Casa Lupe\n    Review: Run by a family that makes you feel like part of the family. Awesome food. I love their wet Chili Verde burritos\n    \n\n### Comparison of Sync vs Async Query Performance\u00b6\n\nThis code performs an asynchronous query on a Deep Lake dataset. It begins by\nopening the dataset asynchronously using `await deeplake.open_async()`,\nspecifying `org_id` and `dataset_name_vs`.\n\n    \n    \n    ds_async = await deeplake.open_async(f\"al://{org_id}/{dataset_name_vs}\")\n    ds_async_results = ds_async.query_async(tql_vs).result()\n    \n\nThis following code compares the execution times of synchronous and\nasynchronous queries on a Deep Lake dataset:\n\n  * First, it records the start time `start_sync` for the synchronous query, executes the query with `vector_search.query(tql_vs)`, and then records the end time `end_sync`. It calculates and prints the total time taken for the synchronous query by subtracting `start_sync` from `end_sync`.",
        "node_293": "image_id\n        JOIN \"azure://container/labels\" AS l ON i.id = l.image_id\n        WHERE l.confidence > 0.9\n        ORDER BY COSINE_SIMILARITY(e.embedding, ARRAY[0.1, 0.2, 0.3]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Filtering\u00b6\n\nFilter data using WHERE clauses:\n\n    \n    \n    # Simple filters\n    results = deeplake.query(\"\"\"\n        SELECT *\n        FROM \"s3://bucket/dataset\"\n        WHERE label = 'cat'\n        AND confidence > 0.9\n    \"\"\")\n    \n    # Combine with vector search\n    results = deeplake.query(\"\"\"\n        SELECT *\n        FROM \"s3://bucket/dataset\"\n        WHERE label IN ('cat', 'dog')\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[0.1, 0.2, 0.3]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Query Results\u00b6\n\nProcess query results:\n\n    \n    \n    # Iterate through results\n    for item in results:\n        image = item[\"images\"]\n        label = item[\"label\"]\n    \n    # Direct column access (recommended for performance)\n    images = results[\"images\"][:]\n    labels = results[\"labels\"][:]\n    \n\n## Async Queries\u00b6\n\nExecute queries asynchronously:\n\n    \n    \n    # Run query asynchronously\n    future = deeplake.query_async(\"\"\"\n        SELECT *\n        FROM \"s3://bucket/dataset\"\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[0.1,0.2, 0.3]) DESC\n    \"\"\")\n    \n    # Get results when ready\n    results = future.result()\n    \n    # Check completion\n    if future.is_completed():\n        results = future.result()\n    else:\n        print(\"Query still running\")\n    \n\n## Querying Views\u00b6\n\nChain queries on views:\n\n    \n    \n    # Initial query\n    view = deeplake.query(\"SELECT * FROM \\\"s3://bucket/animals\\\"\")\n    \n    # Query on view\n    cats = view.query(\"SELECT * WHERE species = 'cat'\")\n    \n    # Further filter\n    large_cats = cats.query('SELECT * WHERE weight > 10')\n    \n\nBack to top",
        "node_720": "The dataset includes\nan `embedding` column for 512-dimensional image embeddings, a\n`restaurant_name` column for names, and an `image` column for storing images\nin UInt8 format. After defining the structure, `vector_search_images.commit()`\nsaves it, making the dataset ready for storing data for multi-modal search\ntasks with images and metadata.\n\n    \n    \n    import deeplake\n    scraped_data = deeplake.open_read_only(\"al://activeloop/restaurant_dataset_complete\")\n    \n\nThis code extracts restaurant details from `scraped_data` into separate lists:\n\n  1. **Initialize Lists** : `restaurant_name` and `images` are initialized to store respective data for each restaurant.\n\n  2. **Populate Lists** : For each entry (`el`) in `scraped_data`, the code appends: \n\n     * `el['restaurant_name']` to `restaurant_name`,\n     * `el['images']['urls']` to `images`.\n\nAfter running, each list holds a specific field from all restaurants, ready\nfor further processing.\n\n    \n    \n    restaurant_name = []\n    images = []\n    for el in scraped_data:\n        restaurant_name.append(el['restaurant_name'])\n        images.append(el['images']['urls'])\n    \n    \n    \n    image_dataset_name = \"restaurant_dataset_with_images\"\n    vector_search_images = deeplake.create(f\"al://{org_id}/{image_dataset_name}\")\n    \n    vector_search_images.add_column(name=\"embedding\", dtype=types.Embedding(512))\n    vector_search_images.add_column(name=\"restaurant_name\", dtype=types.Text())\n    vector_search_images.add_column(name=\"image\", dtype=types.Image(dtype=types.UInt8()))\n    \n    vector_search_images.commit()\n    \n\n### Convert the URLs into images\u00b6\n\nWe retrieve images for each restaurant from URLs in scraped_data and store\nthem in restaurants_images. For each restaurant, we extract image URLs,\nrequest each URL, and filter for successful responses (status code 200). These\nresponses are then converted to PIL images and added to restaurants_images as\nlists of images, with each sublist containing the images for one restaurant.",
        "node_524": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_672": "* 6) ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT \n      * 7) Discover Restaurants Using ColPali and the Late Interaction Mechanism \n        * Download the ColPali model \n        * Create a new dataset to store the ColPali embeddings \n        * Save the data in the dataset \n        * Chat with images \n        * Retrieve the most similar images \n        * VQA: Visual Question Answering \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Load the Data from Deep Lake \n  * 1) Create the Dataset and Use an Inverted Index for Filtering \n    * Extract the data \n    * Add the data to the dataset \n    * Search for the restaurant using a specific word \n    * Show the results \n  * 2) Create the Dataset and use BM25 to Retrieve the Data \n    * Add data to the dataset \n    * Search for the restaurant using a specific sentence \n    * Show the results \n  * 3) Create the",
        "node_509": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_608": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_244": "URLs can be specified using the following protocols:\n\n  * `file://path` local filesystem storage\n  * `al://org_id/dataset_name` A dataset on app.activeloop.ai\n  * `azure://bucket/path` or `az://bucket/path` Azure storage\n  * `gs://bucket/path` or `gcs://bucket/path` or `gcp://bucket/path` Google Cloud storage\n  * `s3://bucket/path` S3 storage\n  * `mem://name` In-memory storage that lasts the life of the process\n\nA URL without a protocol is assumed to be a file:// URL |  _required_  \n`creds` |  `(dict, str)` |  The string `ENV` or a dictionary containing credentials used to access the dataset at the path.\n\n  * If 'aws_access_key_id', 'aws_secret_access_key', 'aws_session_token' are present, these take precedence over credentials present in the environment or in credentials file. Currently only works with s3 paths.\n  * It supports 'aws_access_key_id', 'aws_secret_access_key', 'aws_session_token', 'endpoint_url', 'aws_region', 'profile_name' as keys.\n  * To use credentials managed in your Activeloop organization, use they key 'creds_key': 'managed_key_name'. This requires the org_id dataset argument to be set.\n  * If nothing is given is, credentials are fetched from the environment variables. This is also the case when creds is not passed for cloud datasets\n\n|  `None`  \n`token` |  `str` |  Activeloop token to authenticate user.",
        "node_369": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_85": "Once\na dataset is connected to Deep Lake, it is assigned a Deep Lake path\n`al://org_id/dataset_name`, and it can be accessed using API tokens and\nmanaged credentials from Deep Lake, without continuously having to specify\ncloud credentials.\n\n#### Connecting Datasets in the Python API\u00b6\n\n    \n    \n    # Use deeplake.connect to connect a dataset in your cloud to the Deep Lake App\n    # Managed Credentials (creds_key) for accessing the data \n    # (See Managed Credentials above)\n    ds = deeplake.create('s3://my_bucket/dataset_name',\n    creds={'creds_key': 'managed_creds_key'}) # or deeplake.open\n    \n    # Specify your own path and dataset name for \n    # future access to the dataset.\n    # You can also specify different managed credentials, if desired\n    deeplake.connect(src_path='s3://my_bucket/dataset_name',\n                     dst_path='al://org_id/dataset_name',\n                     creds={'creds_key': 'managed_creds_key'})\n    ds = deeplake.open_read_only('al://org_id/dataset_name', token='my_token')\n    \n\n## Using Manage Credentials with Linked Tensors\u00b6\n\nManaged credentials can be used for accessing data stored in linked tensors.\nSimply add the managed credentials to the dataset's creds_keys and assign them\nto each sample.\n\n    \n    \n    ds.add_column('images', deeplake.types.Link(deeplake.types.Image()))\n    ds.set_creds_key('my_creds_key')\n    ds.append([{\"images\": link_to_sample}])\n    \n\n## Next Steps\u00b6\n\n  * Provisioning AWS\n  * Provisioning Azure\n  * Provisioning GCP\n\nBack to top",
        "node_40": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_575": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_459": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_7": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.",
        "node_250": "`UnevenColumnsError` |  If the input data columns are not the same length.  \n`InvalidTypeDimensions` |  If the input data does not match the column's dimensions.  \n  \n####  `` commit \u00b6\n\n    \n    \n    commit(message: str | None = None) -> None\n    \n\nAtomically commits changes you have made to the dataset. After commit, other\nusers will see your changes to the dataset.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`message` |  `str` |  A message to store in history describing the changes made in the version |  `None`  \n  \nExamples:\n\n    \n    \n    ds.commit()\n    \n    ds.commit(\"Added data from updated documents\")\n    \n\n####  `` commit_async \u00b6\n\n    \n    \n    commit_async(message: str | None = None) -> FutureVoid\n    \n\nAsynchronously commits changes you have made to the dataset.\n\nSee deeplake.Dataset.commit for more information.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`message` |  `str` |  A message to store in history describing the changes made in the commit |  `None`  \n  \nExamples:\n\n    \n    \n    ds.commit_async().wait()\n    \n    ds.commit_async(\"Added data from updated documents\").wait()\n    \n    async def do_commit():\n        await ds.commit_async()\n    \n    future = ds.commit_async() # then you can check if the future is completed using future.is_completed()\n    \n\n####  `` current_branch `property` \u00b6\n\n    \n    \n    current_branch: Branch\n    \n\nThe current active branch\n\n####  `` branch \u00b6\n\n    \n    \n    branch(name: str, version: str | None = None) -> Branch\n    \n\nCreates a branch with the given version of the current branch. If no version\nis given, the current version will be picked up.",
        "node_328": "* Query Syntax \n  * Vector Operations \n    * Similarity Search \n    * ColPali MAXSIM Search \n    * Text Search \n  * Advanced Features \n    * Cross-Cloud Dataset Joins \n    * Virtual Columns \n    * Logical Operations \n    * Data Sampling \n    * Grouping and Sequences \n  * Built-in Functions \n    * Array Operations \n    * Row Information \n    * Array Logic \n    * Aggregations \n    * RANDOM Numbers \n  * Custom Functions \n\n# TQL Syntax\u00b6\n\n## Overview\u00b6\n\nDeep Lake offers a performant SQL-based query engine called \"TQL\" (Tensor\nQuery Language) optimized for machine learning and AI workloads. TQL combines\nfamiliar SQL syntax with powerful tensor operations, enabling efficient\nquerying of embeddings, images, and other multi-modal data.\n\n## Basic Usage\u00b6\n\n### Dataset Queries\u00b6\n\nTQL can be used directly on a dataset or across multiple datasets:\n\n    \n    \n    # Query on a single dataset (no FROM needed)\n    ds = deeplake.open(\"al://org_name/dataset_name\")\n    result = ds.query(\"SELECT * WHERE id > 10\")\n    \n    # Query across datasets (requires FROM)\n    result = deeplake.query('SELECT * FROM \"al://my_org/dataset_name\" WHERE id > 10')\n    \n\n### Query Syntax\u00b6\n\n#### String Values\u00b6\n\nString literals must use single quotes:\n\n    \n    \n    SELECT * WHERE contains(column_name, 'text_value')\n    \n\n#### Special Characters\u00b6\n\nColumn or dataset names with special characters need double quotes:\n\n    \n    \n    SELECT * WHERE contains(\"column-name\", 'text_value')\n    SELECT * FROM \"al://my_org/dataset\" WHERE id > 10\n    \n\nTip\n\nWhen writing queries in Python, remember to properly escape quotes:\n\n    \n    \n    # Using escape characters\n    query = \"SELECT * WHERE contains(\\\"column-name\\\", 'text_value')\"\n    \n    # Using different quote types\n    query = 'SELECT * WHERE contains(\"column-name\",",
        "node_727": "1. **Dataset Copy and Setup** : \n\n     * The `deeplake.copy()` function duplicates the `medical_dataset` from the Activeloop repository into your organization's workspace.\n\n     * `deeplake.open()` then opens the dataset for modifications, allowing us to add or manipulate columns.\n\n  2. **Adding an Embedding Column** : \n\n     * A new column named `embedding` is added to the dataset with the data type `types.Array(types.Float32(), dimensions=2)`, preparing it to store 2D embeddings generated from the medical text.\n\n    \n    \n    deeplake.copy(f\"al://activeloop/medical_dataset\", f\"al://{org_id}/medical_dataset\")\n    \n    \n    \n    medical_dataset = deeplake.open(f\"al://{org_id}/medical_dataset\")\n    medical_dataset.summary()\n    \n\nOutput:\n\n    \n    \n    Dataset(columns=(text,embedding), length=19719)\n    +---------+---------------------------------------+\n    | column  |                 type                  |\n    +---------+---------------------------------------+\n    |  text   |                 text                  |\n    +---------+---------------------------------------+\n    |embedding|array(dtype=float32, shape=[None,None])|\n    +---------+---------------------------------------+\n    \n    \n    \n    medical_dataset.add_column(name=\"embedding\", dtype=types.Array(types.Float32(),dimensions=2))\n    medical_dataset.commit()\n    \n\n  1. **Text Extraction** : \n\n     * The text data from the medical dataset is extracted into a list (`medical_text`) by iterating over the dataset and pulling the `text` field for each entry.\n  2. **Batch Embedding Generation** : \n\n     * The text data is processed in batches of 1,000 entries using the ColBERT model (`ckpt.docFromText`), which generates embeddings for each batch.\n\n     * The embeddings are appended to a list (`all_vectors`) for later use.\n\n  3. **Efficient Processing** :\n\n     * Batching ensures efficient processing, especially when dealing with large datasets, as it prevents memory overload and speeds up embedding generation.",
        "node_208": "We'll use a YOLOv3 model trained on ImageNet\ndata to demonstrate the workflow.\n\n## Prerequisites\u00b6\n\nFirst, let's install the required packages:\n\n    \n    \n    python -m pip install torch==1.12.0+cu116 torchvision==0.13.0+cu116 -f https://download.pytorch.org/whl/torch_stable.html\n    python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu116/torch1.12.0/index.html\n    \n    git clone  -b dev-2.x https://github.com/open-mmlab/mmdetection.git\n    cd mmdetection\n    python3 -m pip install -e .\n    \n\nNote: We use MMDetection 2.x versions as they're currently supported by the\nDeep Lake integration.\n\n## Setup\u00b6\n\nLet's set up our imports and authentication:\n\n    \n    \n    import deeplake\n    from mmcv import Config\n    from mmdet.models import build_detector\n    import os\n    import mmcv\n    \n    # Set your Deep Lake token\n    token = os.environ[\"ACTIVELOOP_TOKEN\"]\n    \n\n## Configuration\u00b6\n\nMMDetection uses config files to define models and training parameters. Here's\nour YOLOv3 config with Deep Lake integration:\n\n    \n    \n    _base_ = \"<mmdetection_path>/configs/yolo/yolov3_d53_mstrain-416_273e_coco.py\"\n    \n    # use caffe img_norm\n    img_norm_cfg = dict(mean=[0, 0, 0], std=[255., 255., 255.",
        "node_618": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_157": "The dataset includes\nan `embedding` column for 512-dimensional image embeddings, a\n`restaurant_name` column for names, and an `image` column for storing images\nin UInt8 format. After defining the structure, `vector_search_images.commit()`\nsaves it, making the dataset ready for storing data for multi-modal search\ntasks with images and metadata.\n\n    \n    \n    import deeplake\n    scraped_data = deeplake.open_read_only(\"al://activeloop/restaurant_dataset_complete\")\n    \n\nThis code extracts restaurant details from `scraped_data` into separate lists:\n\n  1. **Initialize Lists** : `restaurant_name` and `images` are initialized to store respective data for each restaurant.\n\n  2. **Populate Lists** : For each entry (`el`) in `scraped_data`, the code appends: \n\n     * `el['restaurant_name']` to `restaurant_name`,\n     * `el['images']['urls']` to `images`.\n\nAfter running, each list holds a specific field from all restaurants, ready\nfor further processing.\n\n    \n    \n    restaurant_name = []\n    images = []\n    for el in scraped_data:\n        restaurant_name.append(el['restaurant_name'])\n        images.append(el['images']['urls'])\n    \n    \n    \n    image_dataset_name = \"restaurant_dataset_with_images\"\n    vector_search_images = deeplake.create(f\"al://{org_id}/{image_dataset_name}\")\n    \n    vector_search_images.add_column(name=\"embedding\", dtype=types.Embedding(512))\n    vector_search_images.add_column(name=\"restaurant_name\", dtype=types.Text())\n    vector_search_images.add_column(name=\"image\", dtype=types.Image(dtype=types.UInt8()))\n    \n    vector_search_images.commit()\n    \n\n### Convert the URLs into images\u00b6\n\nWe retrieve images for each restaurant from URLs in scraped_data and store\nthem in restaurants_images. For each restaurant, we extract image URLs,\nrequest each URL, and filter for successful responses (status code 200). These\nresponses are then converted to PIL images and added to restaurants_images as\nlists of images, with each sublist containing the images for one restaurant.",
        "node_747": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nQuickstart\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart  Quickstart  Table of contents \n        * Installing Deep Lake \n        * Opening Your First Deep Lake Dataset \n        * Reading Data \n        * Visualizing Datasets \n        * Creating Your Own Datasets \n\nNotebook\n\n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Installing Deep Lake \n  * Opening Your First Deep Lake Dataset \n  * Reading Data \n  * Visualizing Datasets \n  * Creating Your Own Datasets \n\nNotebook\n\n# Deep Learning QuickStart\u00b6\n\n## Installing Deep Lake\u00b6\n\nDeep Lake can be installed using PyPi.\n\nIn [ ]:\n\nCopied!",
        "node_429": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_430": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_411": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_638": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_262": "open_read_only(\"s3://bucket/dataset\")\n    \n    # Can read\n    image = ds[\"images\"][0]\n    metadata = ds.metadata\n    \n    # Cannot modify\n    # ds.append([.])  # Would raise error\n    \n\n### DatasetView\u00b6\n\n  * Read-only access\n  * Cannot modify data\n  * Optimized for query results\n  * Direct integration with ML frameworks\n  * Returned by `query()`\n\n    \n    \n    # Get view through query\n    view = ds.query(\"SELECT *\")\n    \n    # Access data\n    image = view[\"images\"][0]\n    \n    # ML framework integration\n    torch_dataset = view.pytorch()\n    tf_dataset = view.tensorflow()\n    \n\n## Examples\u00b6\n\n### Querying Data\u00b6\n\n    \n    \n    # Using Dataset\n    ds = deeplake.open(\"s3://bucket/dataset\")\n    results = ds.query(\"SELECT * WHERE labels = 'cat'\")\n    \n    # Using ReadOnlyDataset\n    ds = deeplake.open_read_only(\"s3://bucket/dataset\")\n    results = ds.query(\"SELECT * WHERE labels = 'cat'\")\n    \n    # Using DatasetView\n    view = ds.query(\"SELECT * WHERE labels = 'cat'\")\n    subset = view.query(\"SELECT * WHERE confidence > 0.9\")\n    \n\n### Data Access\u00b6\n\n    \n    \n    # Common access patterns work on all types\n    for row in ds:  # Works for Dataset, ReadOnlyDataset, and DatasetView\n        image = row[\"images\"]\n        label = row[\"labels\"]\n    \n    # Column access works on all types\n    images = ds[\"images\"][:]\n    labels = ds[\"labels\"][:]\n    \n\n### Async Operations\u00b6\n\n    \n    \n    # Async query works on all types\n    future = ds.query_async(\"SELECT * WHERE labels = 'cat'\")\n    results = future.result()\n    \n    # Async data access\n    future = ds[\"images\"].get_async(slice(0, 1000))\n    images = future.result()\n    \n\nBack to top",
        "node_688": "dataset_name_vs = \"vector_indexes\"\n    vector_search = deeplake.create(f\"al://{org_id}/{dataset_name_vs}\")\n    \n    # Add columns to the dataset\n    vector_search.add_column(name=\"embedding\", dtype=types.Embedding(3072))\n    vector_search.add_column(name=\"restaurant_name\", dtype=types.Text(index_type=types.BM25))\n    vector_search.add_column(name=\"restaurant_review\", dtype=types.Text(index_type=types.BM25))\n    vector_search.add_column(name=\"owner_answer\", dtype=types.Text(index_type=types.Inverted))\n    \n    vector_search.commit()\n    \n\nThis function processes each review in `restaurant_review` and converts it\ninto a numerical embedding. These embeddings, stored in\n`embeddings_restaurant_review`, represent each review as a vector, enabling us\nto perform cosine similarity searches and comparisons within the dataset.\n\nDeep Lake will handle the search computations, providing us with the final\nresults.\n\n    \n    \n    # Create embeddings\n    batch_size = 500\n    embeddings_restaurant_review = []\n    for i in range(0, len(restaurant_review), batch_size):\n        embeddings_restaurant_review += embedding_function(restaurant_review[i : i + batch_size])\n    \n    \n    \n    # Add data to the dataset\n    vector_search.append({\n        \"restaurant_name\": restaurant_name, \n        \"restaurant_review\": restaurant_review, \n        \"embedding\": embeddings_restaurant_review, \n        \"owner_answer\": owner_answer\n    })\n    vector_search.commit()\n    vector_search.summary()\n    \n\nOutput:\n\n    \n    \n    Dataset(columns=(embedding,restaurant_name,restaurant_review,owner_answer), length=18625)\n    +-----------------+---------------------+\n    |     column      |        type         |\n    +-----------------+---------------------+\n    |    embedding    |   embedding(3072)   |\n    +-----------------+---------------------+\n    | restaurant_name |  text (bm25 Index)  |\n    +-----------------+---------------------+\n    |restaurant_review|  text (bm25 Index)  |\n    +-----------------+---------------------+\n    |  owner_answer   |text (Inverted Index)|\n    +-----------------+---------------------+\n    \n\n### Search for the restaurant using a specific sentence\u00b6\n\nWe start by defining a search query, `\"A restaurant that serves good\nburritos.\"`.",
        "node_227": "## Migration Options\u00b6\n\n### Option 1: Automatic Migration (Recommended)\u00b6\n\nUse the built-in conversion tool to automatically migrate your dataset:\n\n    \n    \n    deeplake.convert(\n        src='al://org_name/v3_dataset', \n        dst='al://org_name/v4_dataset'\n    )\n    \n\n### Option 2: Manual Migration\u00b6\n\nFor custom schemas or complex ML datasets:\n\n    \n    \n    # 1. Create v4 dataset with desired schema\n    ds = deeplake.create(\"s3://new/dataset\")\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"images\", deeplake.types.Image()) \n    ds.commit()\n    \n    # 2. Load v3 data through query\n    source = deeplake.query(f'SELECT * FROM \"{old_ds_path}\"')\n    \n    # 3. Migrate in batches with progress tracking\n    for i in range(0, len(source), batch_size):\n        batch = source[i:i+batch_size]\n        ds.append(batch)\n        if i % 10000 == 0:\n            ds.commit()\n    \n\n## Validating Your Migration\u00b6\n\nAfter migration, verify your ML workflows:\n\n  1. Check vector search functionality: \n    \n        # Verify similarity search\n    array_str = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT * \n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{array_str}]) \n        LIMIT 10\n    \"\"\")\n    \n\n  2. Validate ML training pipelines: \n    \n        # Test PyTorch/TensorFlow integration\n    train_loader = ds.pytorch(transform=transforms)\n    \n\n  3.",
        "node_81": "ds = deeplake.open('s3://...',\n       creds = {\n       'aws_access_key_id': \"my_access_key_id\",\n       'aws_secret_access_key': \"my_aws_secret_access_key\",\n       'aws_session_token': \"my_aws_session_token\", # Optional\n       'endpoint_url': 'http://localhost:8888'\n       })\n    \n\n### Microsoft Azure\u00b6\n\nAuthentication with Microsoft Azure has 4 options:\n\n  1. Log in from your machine's CLI using az login.\n\n  2. Save the `AZURE_STORAGE_ACCOUNT`, `AZURE_STORAGE_KEY`, or other credentials in environmental variables of the same name, which are loaded as default credentials if no other credentials are specified.\n\n  3. Create a dictionary with the `ACCOUNT_KEY` or `SAS_TOKEN` and pass it to Deep Lake using:\n    \n        ds = deeplake.open('azure://<account_name>/<container_name>/<dataset_name>',\n       creds = {\n           'account_key': \"my_account_key\",\n           #OR\n           'sas_token': \"your_sas_token\",\n    })\n    \n\n### Google Cloud Storage\u00b6\n\nAuthentication with Google Cloud Storage has 2 options:\n\n  1. Create a service account, download the JSON file containing the keys, and then pass that file to the `creds` parameter in `deeplake.open('gcs://.....', creds = 'path_to_keys.json')`. It is also possible to manually pass the information from the JSON file into the creds parameter using:\n    \n        ds = deeplake.open('gcs://.....',\n         creds = content_of_json_file\n    )\n    \n\n  2. Authenticate through the browser using the steps below. This requires that the project credentials are stored on your machine, which happens after gcloud is initialized and logged in through the CLI. Afterwards, creds can be switched to creds = 'cache'.\n    \n           ds = deeplake.open('gcs://.....',\n          creds = 'browser' # Switch to 'cache' after doing this once\n       )\n    \n\nBack to top",
        "node_389": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_32": "We do not host or distribute these datasets, vouch for their quality or\nfairness, or claim that you have a license to use the datasets. It is your\nresponsibility to determine whether you have permission to use the datasets\nunder their license.\n\nIf you're a dataset owner and do not want your dataset to be included in this\nlibrary, please get in touch through a GitHub issue. Thank you for your\ncontribution to the ML community!\n\n**Usage Tracking**\n\nBy default, we collect usage data using Bugout (here's the code that does it).\nIt does not collect user data other than anonymized IP address data, and it\nonly logs the Deep Lake library's own actions. This helps our team understand\nhow the tool is used and how to build features that matter to you! After you\nregister with Activeloop, data is no longer anonymous.",
        "node_390": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_421": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_125": "dataset_name_vs = \"vector_indexes\"\n    vector_search = deeplake.create(f\"al://{org_id}/{dataset_name_vs}\")\n    \n    # Add columns to the dataset\n    vector_search.add_column(name=\"embedding\", dtype=types.Embedding(3072))\n    vector_search.add_column(name=\"restaurant_name\", dtype=types.Text(index_type=types.BM25))\n    vector_search.add_column(name=\"restaurant_review\", dtype=types.Text(index_type=types.BM25))\n    vector_search.add_column(name=\"owner_answer\", dtype=types.Text(index_type=types.Inverted))\n    \n    vector_search.commit()\n    \n\nThis function processes each review in `restaurant_review` and converts it\ninto a numerical embedding. These embeddings, stored in\n`embeddings_restaurant_review`, represent each review as a vector, enabling us\nto perform cosine similarity searches and comparisons within the dataset.\n\nDeep Lake will handle the search computations, providing us with the final\nresults.\n\n    \n    \n    # Create embeddings\n    batch_size = 500\n    embeddings_restaurant_review = []\n    for i in range(0, len(restaurant_review), batch_size):\n        embeddings_restaurant_review += embedding_function(restaurant_review[i : i + batch_size])\n    \n    \n    \n    # Add data to the dataset\n    vector_search.append({\n        \"restaurant_name\": restaurant_name, \n        \"restaurant_review\": restaurant_review, \n        \"embedding\": embeddings_restaurant_review, \n        \"owner_answer\": owner_answer\n    })\n    vector_search.commit()\n    vector_search.summary()\n    \n\nOutput:\n\n    \n    \n    Dataset(columns=(embedding,restaurant_name,restaurant_review,owner_answer), length=18625)\n    +-----------------+---------------------+\n    |     column      |        type         |\n    +-----------------+---------------------+\n    |    embedding    |   embedding(3072)   |\n    +-----------------+---------------------+\n    | restaurant_name |  text (bm25 Index)  |\n    +-----------------+---------------------+\n    |restaurant_review|  text (bm25 Index)  |\n    +-----------------+---------------------+\n    |  owner_answer   |text (Inverted Index)|\n    +-----------------+---------------------+\n    \n\n### Search for the restaurant using a specific sentence\u00b6\n\nWe start by defining a search query, `\"A restaurant that serves good\nburritos.\"`.",
        "node_485": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_440": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_771": "All computations run client-side,\nwhich enables users to support lightweight production apps in seconds. Unlike\nChromaDB, Deep Lake\u2019s data format can store raw data such as images, videos,\nand text, in addition to embeddings. ChromaDB is limited to light metadata on\ntop of the embeddings and has no visualization. Deep Lake datasets can be\nvisualized and version controlled. Deep Lake also has a performant dataloader\nfor fine-tuning your Large Language Models.\n\n**Deep Lake vs Pinecone**\n\nBoth Deep Lake and Pinecone enable users to store and search vectors\n(embeddings) and offer integrations with LangChain and LlamaIndex. However,\nthey are architecturally very different. Pinecone is a fully-managed Vector\nDatabase that is optimized for highly demanding applications requiring a\nsearch for billions of vectors. Deep Lake is serverless. All computations run\nclient-side, which enables users to get started in seconds. Unlike Pinecone,\nDeep Lake\u2019s data format can store raw data such as images, videos, and text,\nin addition to embeddings. Deep Lake datasets can be visualized and version\ncontrolled. Pinecone is limited to light metadata on top of the embeddings and\nhas no visualization. Deep Lake also has a performant dataloader for fine-\ntuning your Large Language Models.\n\n**Deep Lake vs Weaviate**\n\nBoth Deep Lake and Weaviate enable users to store and search vectors\n(embeddings) and offer integrations with LangChain and LlamaIndex. However,\nthey are architecturally very different.",
        "node_216": "Deeplake adopted this strategy, and in\norder to train MMSeg models, you need to create/specify your model and\ntraining/validation config. Deep Lake integration's logic is almost the same\nas MMSegmentation's with some minor modifications. The integrations with MMSeg\noccurs in the deeplake.integrations.mmseg module. At a high-level, Deep Lake\nis responsible for the pytorch dataloader that streams data to the training\nframework, while MMSeg is used for the training, transformation, and\nevaluation logic. Let us take a look at the config with deeplake changes:\n\nLearn more about MMSegmentation here.\n\n### Example Configuration with Deep Lake\u00b6\n\nThis tutorial shows how to train a semantic segmentation model using\nMMSegmentation with data stored in Deep Lake. We'll use a PSPNet model with\nResNet-101 backbone trained on COCO data.\n\n## Prerequisites\u00b6\n\nInstall the required packages:\n\n    \n    \n    python -m pip install torch==1.12.0+cu116 torchvision==0.13.0+cu116 -f https://download.pytorch.org/whl/torch_stable.html\n    python -m pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu116/torch1.12.0/index.html\n    \n    git clone https://github.com/open-mmlab/mmsegmentation.git\n    cd mmsegmentation\n    git checkout v0.30.0\n    python -m pip install -e .\n    # Old pytorch version does not work with the new numpy versions\n    python -m pip install numpy==1.24.4 --force-reinstall\n    \n\nNote: We use MMSegmentation versions compatible with Deep Lake's integration.\n\n## Setup\u00b6\n\n    \n    \n    import os\n    import deeplake\n    from mmcv import Config\n    import mmcv\n    from deeplake.integrations import mmseg as mmseg_deeplake\n    \n    # Set your Deep Lake token\n    token = os.",
        "node_442": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_636": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_162": "In traditional information retrieval, a\nmodel often computes detailed interactions between the query and every\ndocument at an early stage, which is computationally expensive, especially\nwith large datasets. **Late interaction** , however, postpones this detailed\ninteraction until a later stage.\n\nAt the final stage of retrieval, **late interaction** occurs: each query token\nembedding interacts with the most relevant document token embeddings, using a\nsimplified comparison (e.g., cosine similarity or max similarity).\n\nThis targeted, late-stage interaction allows the model to capture fine-grained\nrelationships between query and document content without requiring full-scale\ninteractions upfront.\n\nTo use ColBERT, we can leverage the `colbert-ai` library. We'll start by\ninstalling it:\n\n    \n    \n    !pip install -U colbert-ai torch\n    \n\nIn this snippet, we are loading a pretrained ColBERT model checkpoint for use\nin information retrieval tasks. Here's what each part does:\n\n  1. **Importing Modules** : \n\n     * `Checkpoint` is a utility from ColBERT that allows loading and managing pretrained model checkpoints.\n\n     * `ColBERTConfig` provides configuration options for the ColBERT model, such as directory paths and other settings.\n\n  2. **Initializing the Checkpoint** : \n\n     * `\"colbert-ir/colbertv2.0\"` specifies the name of the pretrained checkpoint to load. This could be a path to a local model file or a remote model identifier, depending on your setup.\n\n     * `ColBERTConfig(root=\"experiments\")` sets the root directory where model-related experiments will be saved or accessed. This is useful for organizing logs, results, and intermediate files.\n\n  3. **Purpose** : \n\n     * The `ckpt` object now contains the pretrained ColBERT model and its configuration, ready to be used for tasks like ranking or embedding documents in information retrieval pipelines.\n\nThis step sets up the foundation for using ColBERT's capabilities in semantic\nsearch and ranking tasks efficiently.",
        "node_180": "It then converts the `image` data back to an image format\nwith `Image.fromarray(el[\"image\"])` and displays it using `el_img.show()`.\nThis loop visually presents each query's closest matches alongside their\nsimilarity scores.\n\n    \n    \n    import matplotlib.pyplot as plt\n    \n    num_columns = n_res\n    num_rows = len(colpali_results)\n    \n    fig, axes = plt.subplots(num_rows, num_columns, figsize=(15, 5 * num_rows))\n    axes = axes.flatten()  # Flatten for easier access to cells\n    \n    idx_plot = 0\n    for res, query in zip(colpali_results, queries):\n        for el in res: \n            img = Image.fromarray(el[\"image\"])\n            axes[idx_plot].imshow(img)\n            axes[idx_plot].set_title(f\"Query: {query}, Similarity: {el['score']:.4f}\")\n            axes[idx_plot].axis('off')  # Turn off axes for a cleaner look\n            idx_plot += 1\n    for ax in axes[len(colpali_results):]:\n        ax.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n\nOutput:\n\n### VQA: Visual Question Answering\u00b6\n\nThe following function, `generate_VQA`, creates a visual question-answering\n(VQA) system that takes an image and a question, then analyzes the image to\nprovide an answer based on visual cues.\n\n  1. **Convert Image to Base64** : The image (`img`) is encoded to a base64 string, allowing it to be embedded in the API request.\n\n  2. **System Prompt** : A structured prompt instructs the model to analyze the image, focusing on visual details that can answer the question.\n\n  3. **Payload and Headers** : The request payload includes the model (`gpt-4o-mini`), the system prompt, and the base64-encoded image. The model is expected to respond in JSON format, specifically returning an `answer` field with insights based on the image.\n\n  4.",
        "node_292": "query(\"\"\"\n        SELECT AVG(features, axis=0)\n        FROM \"s3://bucket/features\"\n    \"\"\")\n    \n\n## Joining Datasets\u00b6\n\nJoin data across different datasets and across different clouds:\n\n    \n    \n    # Join datasets from different storage\n    results = deeplake.query(\"\"\"\n        SELECT i.image, i.embedding, m.labels, m.metadata\n        FROM \"s3://bucket1/images\" AS i\n        JOIN \"s3://bucket2/metadata\" AS m \n        ON i.id = m.image_id\n        WHERE m.verified = true\n    \"\"\")\n    \n    # Complex join with filtering\n    results = deeplake.query(\"\"\"\n        SELECT \n            i.image,\n            e.embedding,\n            l.label\n        FROM \"s3://bucket1/images\" AS i\n        JOIN \"gcs://bucket2/embeddings\" AS e ON i.id = e.image_id\n        JOIN \"azure://container/labels\" AS l ON i.id = l.image_id\n        WHERE l.confidence > 0.9\n        ORDER BY COSINE_SIMILARITY(e.embedding, ARRAY[0.1, 0.2, 0.3]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Filtering\u00b6\n\nFilter data using WHERE clauses:\n\n    \n    \n    # Simple filters\n    results = deeplake.query(\"\"\"\n        SELECT *\n        FROM \"s3://bucket/dataset\"\n        WHERE label = 'cat'\n        AND confidence > 0.9\n    \"\"\")\n    \n    # Combine with vector search\n    results = deeplake.query(\"\"\"\n        SELECT *\n        FROM \"s3://bucket/dataset\"\n        WHERE label IN ('cat', 'dog')\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[0.1, 0.2, 0.",
        "node_766": "**Native Compression with Lazy NumPy-like Indexing** Store images, audio, and\nvideos in their native compression. Slice, index, iterate, and interact with\nyour data like a collection of NumPy arrays in your system's memory. Deep Lake\nlazily loads data only when needed, e.g., when training a model or running\nqueries.  **Dataloaders for Popular Deep Learning Frameworks** Deep Lake comes\nwith built-in dataloaders for Pytorch and TensorFlow. Train your model with a\nfew lines of code - we even take care of dataset shuffling. :)  **Integrations\nwith Powerful Tools** Deep Lake has integrations with Langchain and LLamaIndex\nas a vector store for LLM apps, Weights & Biases for data lineage during model\ntraining, MMDetection for training object detection models, and MMSegmentation\nfor training semantic segmentation models.  **100+ most-popular image, video,\nand audio datasets available in seconds** Deep Lake community has uploaded\n100+ image, video and audio datasets like MNIST, COCO, ImageNet, CIFAR, GTZAN\nand others.  **Instant Visualization Support in theDeep Lake App** Deep Lake\ndatasets are instantly visualized with bounding boxes, masks, annotations,\netc. in Deep Lake Visualizer (see below).\n\n## \ud83d\ude80 How to install Deep Lake\n\nDeep Lake can be installed using pip:\n\n    \n    \n    pip install deeplake\n\n### To access all of Deep Lake's features, please register in the Deep Lake\nApp.",
        "node_53": "Chat in on our website: to claim the access!\n\n## \ud83d\udc69\u200d\ud83d\udcbb Comparisons to Familiar Tools\n\n**Deep Lake vs Chroma**\n\nBoth Deep Lake & ChromaDB enable users to store and search vectors\n(embeddings) and offer integrations with LangChain and LlamaIndex. However,\nthey are architecturally very different. ChromaDB is a Vector Database that\ncan be deployed locally or on a server using Docker and will offer a hosted\nsolution shortly. Deep Lake is a serverless Vector Store deployed on the\nuser\u2019s own cloud, locally, or in-memory. All computations run client-side,\nwhich enables users to support lightweight production apps in seconds. Unlike\nChromaDB, Deep Lake\u2019s data format can store raw data such as images, videos,\nand text, in addition to embeddings. ChromaDB is limited to light metadata on\ntop of the embeddings and has no visualization. Deep Lake datasets can be\nvisualized and version controlled. Deep Lake also has a performant dataloader\nfor fine-tuning your Large Language Models.\n\n**Deep Lake vs Pinecone**\n\nBoth Deep Lake and Pinecone enable users to store and search vectors\n(embeddings) and offer integrations with LangChain and LlamaIndex. However,\nthey are architecturally very different. Pinecone is a fully-managed Vector\nDatabase that is optimized for highly demanding applications requiring a\nsearch for billions of vectors. Deep Lake is serverless. All computations run\nclient-side, which enables users to get started in seconds.",
        "node_748": "In [ ]:\n\nCopied!\n\n    \n    \n    !pip3 install deeplake\n    \n\n!pip3 install deeplake\n\n## Opening Your First Deep Lake Dataset\u00b6\n\nLet's load the Visdrone dataset, a rich dataset with many object detections\nper image. Datasets hosted by Activeloop are identified by the host\norganization id followed by the dataset name: <org_id>/<dataset_name>.\n\nIn [2]:\n\nCopied!\n\n    \n    \n    import deeplake\n    import getpass\n    import os\n    from deeplake import types\n    \n    os.environ['ACTIVELOOP_TOKEN'] = getpass.getpass()\n    \n\nimport deeplake import getpass import os from deeplake import types\nos.environ['ACTIVELOOP_TOKEN'] = getpass.getpass()\n\nIn [3]:\n\nCopied!\n\n    \n    \n    dataset_path = 'al://activeloop/visdrone-det-train-v4'\n    ds = deeplake.open(dataset_path)\n    \n\ndataset_path = 'al://activeloop/visdrone-det-train-v4' ds =\ndeeplake.open(dataset_path)\n\nThe dataset has 3 columns, the images, labels, and bounding boxes:\n\nIn [4]:\n\nCopied!\n\n    \n    \n    ds.summary()\n    \n\nds.summary()\n\n    \n    \n    Dataset(columns=(images,labels,boxes), length=6471)\n    +------+--------------------------------------------+\n    |column|                    type                    |\n    +------+--------------------------------------------+\n    |images| array(dtype=uint8, shape=[None,None,None]) |\n    +------+--------------------------------------------+\n    |labels|     array(dtype=uint32, shape=[None])      |\n    +------+--------------------------------------------+\n    |boxes |array(dtype=float32, shape=[None,None,None])|\n    +------+--------------------------------------------+\n    \n    \n\n## Reading Data\u00b6\n\nDeep Lake does not download any data in advance. Data is fetched lazily from\nlong-term storage based on row numbers in the dataset:\n\nIn [5]:\n\nCopied!",
        "node_41": "Skip to content\n\n## Navigation Menu\n\nToggle navigation\n\nSign in\n\n  * Product \n\n    * GitHub Copilot\n\nWrite better code with AI\n\n    * GitHub Advanced Security\n\nFind and fix vulnerabilities\n\n    * Actions\n\nAutomate any workflow\n\n    * Codespaces\n\nInstant dev environments\n\n    * Issues\n\nPlan and track work\n\n    * Code Review\n\nManage code changes\n\n    * Discussions\n\nCollaborate outside of code\n\n    * Code Search\n\nFind more,",
        "node_152": "The asynchronous query time is calculated as the difference between `end_async` and `start_async`, and is printed.\n\nThe code executes two queries both synchronously and asynchronously, measuring\nthe execution time for each method. In the synchronous part, the queries are\nexecuted one after the other, and the execution time is recorded. In the\nasynchronous part, the queries are run concurrently using `asyncio.gather()`\nto parallelize the asynchronous calls, and the execution time is also\nmeasured. The \"speed factor\" is then calculated by comparing the execution\ntimes, showing how much faster the asynchronous execution is compared to the\nsynchronous one. Using `asyncio.gather()` allows the asynchronous queries to\nrun in parallel, reducing the overall execution time.\n\n    \n    \n    import time\n    import asyncio\n    import nest_asyncio\n    \n    nest_asyncio.apply()\n    \n    async def run_async_queries():\n        # Use asyncio.gather to run queries concurrently\n        ds_async_results, ds_bm25_async_results = await asyncio.gather(\n            vector_search.query_async(tql_vs),\n            ds_bm25.query_async(tql_bm25)\n        )\n        return ds_async_results, ds_bm25_async_results\n    \n    # Measure synchronous execution time\n    start_sync = time.time()\n    ds_sync_results = vector_search.query(tql_vs)\n    ds_bm25_sync_results = ds_bm25.query(tql_bm25)\n    end_sync = time.time()\n    print(f\"Sync query time: {end_sync - start_sync}\")\n    \n    # Measure asynchronous execution time\n    start_async = time.time()\n    # Run the async queries concurrently using asyncio.gather\n    ds_async_results, ds_bm25_async_results = asyncio.run(run_async_queries())\n    end_async = time.time()\n    print(f\"Async query time: {end_async - start_async}\")\n    \n    sync_time = end_sync - start_sync\n    async_time = end_async - start_async\n    \n    # Calculate speed factor\n    speed_factor = sync_time / async_time\n    \n    # Print the result\n    print(f\"The async query is {speed_factor:.2f} times faster than the sync query.\")",
        "node_743": "It then converts the `image` data back to an image format\nwith `Image.fromarray(el[\"image\"])` and displays it using `el_img.show()`.\nThis loop visually presents each query's closest matches alongside their\nsimilarity scores.\n\n    \n    \n    import matplotlib.pyplot as plt\n    \n    num_columns = n_res\n    num_rows = len(colpali_results)\n    \n    fig, axes = plt.subplots(num_rows, num_columns, figsize=(15, 5 * num_rows))\n    axes = axes.flatten()  # Flatten for easier access to cells\n    \n    idx_plot = 0\n    for res, query in zip(colpali_results, queries):\n        for el in res: \n            img = Image.fromarray(el[\"image\"])\n            axes[idx_plot].imshow(img)\n            axes[idx_plot].set_title(f\"Query: {query}, Similarity: {el['score']:.4f}\")\n            axes[idx_plot].axis('off')  # Turn off axes for a cleaner look\n            idx_plot += 1\n    for ax in axes[len(colpali_results):]:\n        ax.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n\nOutput:\n\n### VQA: Visual Question Answering\u00b6\n\nThe following function, `generate_VQA`, creates a visual question-answering\n(VQA) system that takes an image and a question, then analyzes the image to\nprovide an answer based on visual cues.\n\n  1. **Convert Image to Base64** : The image (`img`) is encoded to a base64 string, allowing it to be embedded in the API request.\n\n  2. **System Prompt** : A structured prompt instructs the model to analyze the image, focusing on visual details that can answer the question.\n\n  3. **Payload and Headers** : The request payload includes the model (`gpt-4o-mini`), the system prompt, and the base64-encoded image. The model is expected to respond in JSON format, specifically returning an `answer` field with insights based on the image.\n\n  4.",
        "node_117": "The portions are monstrous. The wet burritos are as big as a football.\n    \n\nAI data retrieval systems today face 3 challenges: `limited modalities`, `lack\nof accuracy`, and `high costs at scale`. Deep Lake 4.0 fixes this by enabling\ntrue multi-modality, enhancing accuracy, and reducing query costs by 2x with\nindex-on-the-lake technology.\n\nConsider a scenario where we store all our data locally on a computer.\nInitially, this may be adequate, but as the volume of data grows, managing it\nbecomes increasingly challenging. The computer's storage becomes limited, data\naccess slows, and sharing information with others is less efficient.\n\nTo address these challenges, we can transition our data storage to the cloud\nusing Deep Lake. Designed specifically for handling large-scale datasets and\nAI workloads, Deep Lake enables up to 10 times faster data access. With cloud\nstorage, hardware limitations are no longer a concern: Deep Lake offers ample\nstorage capacity, secure access from any location, and streamlined data\nsharing.\n\nThis approach provides a robust and scalable infrastructure that can grow\nalongside our projects, minimizing the need for frequent hardware upgrades and\nensuring efficient data management.\n\n## 2) Create the Dataset and use BM25 to Retrieve the Data\u00b6\n\nOur advanced `\"Index-On-The-Lake\"` technology enables sub-second query\nperformance directly from object storage, such as `S3`, using minimal compute\npower and memory resources. Achieve up to `10x greater cost efficiency`\ncompared to in-memory databases and `2x faster performance` than other object\nstorage solutions, all without requiring additional disk-based caching.\n\nWith Deep Lake, you benefit from rapid streaming columnar access to train deep\nlearning models directly, while also executing sub-second indexed queries for\nretrieval-augmented generation.\n\nIn this stage, the system uses BM25 for a straightforward lexical search. This\napproach is efficient for retrieving documents based on exact or partial\nkeyword matches.",
        "node_375": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_12": "Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\n{{ message }}\n\nactiveloopai  / **deeplake ** Public\n\n  * Notifications  You must be signed in to change notification settings\n  * Fork 656\n  * Star  8.5k\n\nDatabase for AI. Store Vectors, Images, Texts, Videos, etc. Use with\nLLMs/LangChain. Store, query, version, & visualize any AI data. Stream data in\nreal-time to PyTorch/TensorFlow. https://activeloop.ai\n\nactiveloop.ai\n\n### License\n\nApache-2.0 license\n\n8.5k stars  656 forks  Branches Tags Activity\n\nStar\n\nNotifications  You must be signed in to change notification settings\n\n  * Code\n  * Issues 49\n  * Pull requests 9\n  * Discussions\n  * Actions\n  * Projects 0\n  * Wiki\n  * Security\n  * Insights\n\nAdditional navigation options\n\n  * Code \n  * Issues \n  * Pull requests \n  * Discussions \n  * Actions \n  * Projects \n  * Wiki \n  * Security \n  * Insights \n\n# activeloopai/deeplake\n\nmain\n\nBranchesTags\n\nGo to file\n\nCode\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\n## History\n\n9,213 Commits  \n.github| .",
        "node_587": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_673": "* Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Load the Data from Deep Lake \n  * 1) Create the Dataset and Use an Inverted Index for Filtering \n    * Extract the data \n    * Add the data to the dataset \n    * Search for the restaurant using a specific word \n    * Show the results \n  * 2) Create the Dataset and use BM25 to Retrieve the Data \n    * Add data to the dataset \n    * Search for the restaurant using a specific sentence \n    * Show the results \n  * 3) Create the Dataset and use Vector Similarity Search \n    * Create the dataset and add the columns \n    * Search for the restaurant using a specific sentence \n  * 4) Explore Results with Hybrid Search \n    * Search for the correct restaurant using a specific sentence \n    * Show the scores \n    * Normalize the score \n    * Fusion method \n    * Show the results \n    * Let's run a search on a multiple dataset \n    * Comparison of Sync vs Async Query Performance \n  * 5) Integrating Image Embeddings for Multi-Modal Search \n    * Create the embedding function for images \n    * Create a new dataset to save the images \n    * Convert the URLs into images \n    * Search similar images \n    * Performing a similar image search based on a specific image \n    * Show similar images and the their respective restaurants \n  * 6) ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT \n  * 7) Discover Restaurants Using ColPali and the Late Interaction Mechanism",
        "node_478": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_3": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_122": "Wolderful!!!\n    Restaurant name: Los Amigos\n    Review: Fantastic burritos!\n    Restaurant name: Cheztakos!!!\n    Review: Great burritos\n    Restaurant name: La Coste\u00f1a\n    Review: Awesome burritos!\n    Restaurant name: La Coste\u00f1a\n    Review: Awesome burritos\n    Restaurant name: La Coste\u00f1a\n    Review: Bomb burritos\n    \n\n## 3) Create the Dataset and use Vector Similarity Search\u00b6\n\nIf you want to generate text embeddings for similarity search, you can choose\na proprietary model like `text-embedding-3-large` from `OpenAI`, or you can\nopt for an `open-source` model. The MTEB leaderboard on Hugging Face provides\na selection of open-source models that have been tested for their\neffectiveness at converting text into embeddings, which are numerical\nrepresentations that capture the meaning and nuances of words and sentences.\nUsing these embeddings, you can perform similarity search, grouping similar\npieces of text (like sentences or documents) based on their meaning.\n\nSelecting a model from the MTEB leaderboard offers several benefits: these\nmodels are ranked based on performance across a variety of tasks and\nlanguages, ensuring that you're choosing a model that's both accurate and\nversatile. If you prefer not to use a proprietary model, a high-performing\nmodel from this list is an excellent alternative.\n\nWe start by installing and importing the `openai` library to access OpenAI's\nAPI for generating embeddings. Next, we define the function\n`embedding_function`, which takes `texts` as input (either a single string or\na list of strings) and a model name, defaulting to `\"text-embedding-3-large\"`.\nThen, for each text, we replace newline characters with spaces to maintain\nclean, uniform text. Finally, we use `openai.embeddings.create()` to generate\nembeddings for each text and return a list of these embeddings, which can be\nused for cosine similarity comparisons.",
        "node_397": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_774": "Deep Lake datasets can be visualized and version\ncontrolled. Weaviate is limited to light metadata on top of the embeddings and\nhas no visualization. Deep Lake also has a performant dataloader for fine-\ntuning your Large Language Models.\n\n**Deep Lake vs DVC**\n\nDeep Lake and DVC offer dataset version control similar to git for data, but\ntheir methods for storing data differ significantly. Deep Lake converts and\nstores data as chunked compressed arrays, which enables rapid streaming to ML\nmodels, whereas DVC operates on top of data stored in less efficient\ntraditional file structures. The Deep Lake format makes dataset versioning\nsignificantly easier compared to traditional file structures by DVC when\ndatasets are composed of many files (i.e., many images). An additional\ndistinction is that DVC primarily uses a command-line interface, whereas Deep\nLake is a Python package. Lastly, Deep Lake offers an API to easily connect\ndatasets to ML frameworks and other common ML tools and enables instant\ndataset visualization through Activeloop's visualization tool.\n\n**Deep Lake vs MosaicML MDS format**\n\n  * **Data Storage Format:** Deep Lake operates on a columnar storage format, whereas MDS utilizes a row-wise storage approach. This fundamentally impacts how data is read, written, and organized in each system.\n  * **Compression:** Deep Lake offers a more flexible compression scheme, allowing control over both chunk-level and sample-level compression for each column or tensor. This feature eliminates the need for additional compressions like zstd, which would otherwise demand more CPU cycles for decompressing on top of formats like jpeg.",
        "node_131": "We then execute both queries, storing vector results in `vs_results` and BM25\nresults in `bm25_results`. This allows us to compare results from both search\nmethods.\n\n    \n    \n    tql_vs = f\"\"\"\n        SELECT *, cosine_similarity(embedding, ARRAY[{embedding_string}]) as score\n        FROM (\n            SELECT *, ROW_NUMBER() AS row_id\n        )\n        ORDER BY cosine_similarity(embedding, ARRAY[{embedding_string}]) DESC \n        LIMIT 5\n    \"\"\"\n    \n    tql_bm25 = f\"\"\"\n        SELECT *, BM25_SIMILARITY(restaurant_review, '{query}') as score\n        FROM (\n            SELECT *, ROW_NUMBER() AS row_id\n        ) \n        ORDER BY BM25_SIMILARITY(restaurant_review, '{query}') DESC \n        LIMIT 5\n    \"\"\"\n    \n    vs_results = vector_search.query(tql_vs)\n    bm25_results = vector_search.query(tql_bm25)\n    print(vs_results)\n    print(bm25_results)\n    \n\nOutput:\n\n    \n    \n    Dataset(columns=(embedding,restaurant_name,restaurant_review,owner_answer,row_id,score), length=5)\n    Dataset(columns=(embedding,restaurant_name,restaurant_review,owner_answer,row_id,score), length=5)\n    \n\n### Show the scores\u00b6\n\n    \n    \n    for el_vs in vs_results:\n        print(f\"vector search score: {el_vs['score']}\")\n    \n    for el_bm25 in bm25_results:\n        print(f\"bm25 score: {el_bm25['score']}\")\n    \n\nOutput:\n\n    \n    \n    vector search score: 0.5322654247283936\n    vector search score: 0.46281781792640686\n    vector search score: 0.4580579102039337\n    vector search score: 0.45585304498672485\n    vector search score: 0.4528498649597168\n    bm25 score: 13.076177597045898\n    bm25 score: 11.206666946411133\n    bm25 score: 11.",
        "node_318": "This page provides tips for optimizing your usage of Deep\nLake for best experience and performance.\n\n## Data Ingestion\u00b6\n\n### Do commits for version control only\u00b6\n\nWhen adding new data to the dataset, Deep Lake automatically flushes the data\nto the storage. No need to commit each time you add data to the dataset.\nCommit is only needed when you want to create a new version or checkpoint of\nthe dataset.\n\n### Prefer creating schema before adding data\u00b6\n\nIt is recommended to create the dataset schema before ingestion. Deep Lake\nsupports schema evolution by adding columns when there is already data in the\ndataset.\n\nHowever, schema evolution can lead to non-intuitive behavior and performance\ndegradation. It is recommended to create the schema before adding data.\n\nThere are cases when the values of specific columns are not known before\ningestion. For example if you create embeddings dataset. Usually you have text\nupfront and you need to create embeddings later (maybe on another machine). In\nthis case distinguish the two steps of dataset creation. First create the\ndataset with text data, append all texts and then have dedicated step which\nwill create embeddings column and fill it with embeddings.\n\n### Select the right data type for your data\u00b6\n\nDeep Lake provides a wide range of type to store your data. Some data types\ncan be stored with multiple deeplake types. For example you can store images\nas `deeplake.types.Array(dimensions=3)` or `deeplake.types.Image()`. For this\ncase, it is recommended to use `deeplake.types.Image()` as it allows to store\nimages in compressed format and stores the data more efficiently.\n\nIn general if there is a specific type for your data prefer to use that\ninstead of generic arrays.\n\n  * Use `deeplake.types.Image()` for images. This allows efficient image compression and decompression.\n  * Use `deeplake.types.Text()` for text data. This allows efficient text search and indexing.\n  * Use `deeplake.types.Embedding()` for embeddings. This allows efficient vector similarity search with query.\n\n### Prefer appending data in batches\u00b6\n\nThere are two ways to append data to the dataset.\n\n  1.",
        "node_383": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_39": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.",
        "node_681": "This approach provides a robust and scalable infrastructure that can grow\nalongside our projects, minimizing the need for frequent hardware upgrades and\nensuring efficient data management.\n\n## 2) Create the Dataset and use BM25 to Retrieve the Data\u00b6\n\nOur advanced `\"Index-On-The-Lake\"` technology enables sub-second query\nperformance directly from object storage, such as `S3`, using minimal compute\npower and memory resources. Achieve up to `10x greater cost efficiency`\ncompared to in-memory databases and `2x faster performance` than other object\nstorage solutions, all without requiring additional disk-based caching.\n\nWith Deep Lake, you benefit from rapid streaming columnar access to train deep\nlearning models directly, while also executing sub-second indexed queries for\nretrieval-augmented generation.\n\nIn this stage, the system uses BM25 for a straightforward lexical search. This\napproach is efficient for retrieving documents based on exact or partial\nkeyword matches.\n\nWe start by importing deeplake and setting up an organization ID `org_id` and\ndataset name `dataset_name_bm25`. Next, we create a new dataset with the\nspecified name and location in Deep Lake storage.\n\nWe then add two columns to the dataset: `restaurant_name` and\n`restaurant_review`. Both columns use a BM25 index, which optimizes them for\nrelevance-based searches, enhancing the ability to rank results based on how\nwell they match search terms.\n\nFinally, we use `ds_bm25.commit()` to save these changes to the dataset and\n`ds_bm25.summary()` to display an overview of the dataset's structure and\ncontents.\n\nIf you don't have a token yet, you can sign up and then log in on the official\nActiveloop website, then click the `Create API token` button to obtain a new\nAPI token. Here, under `Select organization`, you can also find your\norganization ID(s).",
        "node_645": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_622": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_333": "'machine learning')\n        AND confidence > 0.9)\n        OR label IN ('cat', 'dog')\n    \n    -- Array operations\n    SELECT * \n    WHERE any(logical_and(\n        bounding_boxes[:,3] > 0.5,\n        confidence > 0.8\n    ))\n    \n\n### Data Sampling\u00b6\n\n    \n    \n    -- Weighted random sampling\n    SELECT * \n    SAMPLE BY MAX_WEIGHT(\n        high_confidence: 0.7,\n        medium_confidence: 0.2,\n        low_confidence: 0.1\n    ) LIMIT 1000\n    \n    -- Sampling with replacement\n    SELECT * \n    SAMPLE BY MAX_WEIGHT(\n        positive_samples: 0.5,\n        negative_samples: 0.5\n    ) replace True LIMIT 2000\n    \n\n### Grouping and Sequences\u00b6\n\n    \n    \n    -- Group frames into videos\n    SELECT * \n    GROUP BY video_id, camera_id\n    \n    -- Split videos into frames\n    SELECT * \n    UNGROUP BY split\n    \n\n## Built-in Functions\u00b6\n\n### Array Operations\u00b6\n\n  * `SHAPE(array)`: Returns array dimensions \n    \n        SELECT * WHERE SHAPE(embedding)[0] = 768\n    \n\n  * `DATA(column, index)`: Access specific array elements \n    \n        SELECT * ORDER BY L2_NORM(embedding - data(embedding, 10))\n    \n\n### Row Information\u00b6\n\n  * `ROW_NUMBER()`: Returns zero-based row offset \n    \n        SELECT *, ROW_NUMBER() WHERE ROW_NUMBER() < 100\n    \n\n### Array Logic\u00b6\n\n  * `ANY()`, `ALL()`: Array-wise logical operations \n    \n        SELECT * WHERE ANY(confidence > 0.9)\n    SELECT * WHERE ALL(scores > 0.",
        "node_718": "**Define Transformations (`tform`)**:\n\n     * The transformation pipeline includes:\n       * **Resize** : Scales images to 224x224 pixels.\n       * **ToTensor** : Converts images to tensor format.\n       * **Lambda** : Ensures grayscale images are replicated across three channels to match the RGB format.\n       * **Normalize** : Standardizes pixel values based on common RGB means and standard deviations.\n  2. **Define`embedding_function_images`**:\n\n     * This function generates embeddings for a list of image.\n     * If `images` is a single filename, it's converted to a list.\n     * **Batch Processing** : Images are processed in batches (default size 4), with transformations applied to each image. The batch is then loaded to the device.\n     * **Embedding Creation** : The model encodes each batch into embeddings, stored in the `embeddings` list, which is returned as a single list.\n\nThis function supports efficient, batched embedding generation, useful for\nmulti-modal tasks like image-based search.\n\n    \n    \n    from torchvision import transforms\n    \n    tform = transforms.Compose([\n        transforms.Resize((224,224)), \n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: torch.cat([x, x, x], dim=0) if x.shape[0] == 1 else x),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ])\n    \n    def embedding_function_images(images, model = model, transform = tform, batch_size = 4):\n        \"\"\"Creates a list of embeddings based on a list of image. Images are processed in batches.\"\"\"",
        "node_709": "Here's how it works:\n\n  1. **Setup and Initialization** : \n\n     * We import `json` for handling JSON responses and initialize the `OpenAI` client to interact with the language model.\n  2. **Define`generate_question` Function**: \n\n     * This function accepts: \n       * `question`: The user's question.\n       * `information`: A list relevant chunks retrieved previously, providing context.\n  3. **System and User Prompts** : \n\n     * The `system_prompt` instructs the model to act as a restaurant assistant, using the provided chunks to answer clearly and without repetition.\n\n     * The model is directed to format its response in JSON.\n\n     * The `user_prompt` combines the user's question and the information chunks.\n\n  4. **Generate and Parse the Response** : \n\n     * Using `client.chat.completions.create()`, the system and user prompts are sent to the LLM (specified as `gpt-4o-mini`).\n\n     * The response is parsed as JSON, extracting the `answer` field. If parsing fails, `False` is returned.\n\n    \n    \n    import json\n    from openai import OpenAI\n    \n    client = OpenAI()\n    \n    def generate_question(question:str, information:list):\n        system_prompt = f\"\"\"You are a helpful assistant specialized in providing answers to questions about restaurants. Below is a question from a user, along with the top four relevant information chunks about restaurants from a Deep Lake database. Using these chunks, construct a clear and informative answer that addresses the question, incorporating key details without repeating information.\n        The output must be in JSON format with the following structure:\n        {{\n            \"answer\": \"The answer to the question.\"",
        "node_219": "dict(type='ImageToTensor', keys=['img']),\n                dict(type='Collect', keys=['img']),\n            ])\n    ]\n    \n    evaluation = dict(metric=[\"mIoU\"], interval=10000)\n    \n    data = dict(\n        samples_per_gpu=4,\n        workers_per_gpu=8,\n        train=dict(\n            pipeline=train_pipeline,\n            deeplake_path=\"hub://activeloop/coco-train-seg-mask\",\n            deeplake_tensors = {\"img\": \"images\", \"gt_semantic_seg\": \"seg_masks\"},\n            deeplake_dataloader={\"shuffle\": False, \"num_workers\": 0, \"drop_last\": True}\n        ),\n        val=dict(\n            pipeline=test_pipeline,\n            deeplake_path=\"hub://activeloop/coco-val-seg-mask\"\",\n            deeplake_tensors = {\"img\": \"images\", \"gt_semantic_seg\": \"seg_masks\"},\n            deeplake_dataloader={\"shuffle\": False, \"batch_size\": 1, \"num_workers\": 0, \"drop_last\": True}\n        )\n    )\n    \n    work_dir = \"./deeplake_logs\"\n    \n    optimizer = dict(lr=0.02 / 8)\n    lr_config = dict(warmup=None)\n    log_config = dict(interval=50)\n    checkpoint_config = dict(interval=5000)\n    \n    runner = dict(type=\"IterBasedRunner\", max_iters=100000, max_epochs=None)\n    device = \"cuda\"\n    \n\n## Training\u00b6\n\nNow we can start the training:\n\n    \n    \n    if __name__ == \"__main__\":\n        current_loc = os.getcwd()\n        cfg_file = f\"{current_loc}/seg_mask_config.py\"\n    \n        # Read the config file\n        cfg = Config.fromfile(cfg_file)\n        cfg.model.decode_head.num_classes = 81\n        cfg.model.auxiliary_head.num_classes = 81\n    \n        # build segmentor\n        model = mmseg_deeplake.build_segmentor(\n            cfg.model\n        )\n    \n        # Create work directory\n        mmcv.mkdir_or_exist(os.path.",
        "node_461": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_368": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_330": "ARRAY[0.1, 0.2, .]) DESC\n    LIMIT 100\n    \n    -- L2 norm/Euclidean distance (lower is more similar)\n    SELECT * \n    ORDER BY L2_NORM(embeddings - ARRAY[0.1, 0.2, .]) ASC\n    LIMIT 100\n    \n    -- L1 norm/Manhattan distance\n    SELECT * \n    ORDER BY L1_NORM(embeddings - ARRAY[0.1, 0.2, .]) ASC\n    LIMIT 100\n    \n    -- L\u221e norm/Chebyshev distance\n    SELECT * \n    ORDER BY LINF_NORM(embeddings - ARRAY[0.1, 0.2, .]) ASC\n    LIMIT 100\n    \n\n### ColPali MAXSIM Search\u00b6\n\nTQL supports the MAXSIM operator for efficient similarity search with ColPali\nembeddings:\n\n    \n    \n    -- Using MAXSIM with ColPali embeddings\n    SELECT *, MAXSIM(\n        document_embeddings,\n        ARRAY[\n            ARRAY[0.1, 0.2, 0.3],\n            ARRAY[0.4, 0.5, 0.6],\n            ARRAY[0.7, 0.8, 0.9]\n        ]\n    ) AS score\n    ORDER BY MAXSIM(\n        document_embeddings,\n        ARRAY[\n            ARRAY[0.1, 0.2, 0.3],\n            ARRAY[0.4, 0.5, 0.6],\n            ARRAY[0.7, 0.8, 0.9]\n        ]\n    ) DESC\n    LIMIT 10\n    \n\n### Text Search\u00b6\n\n#### Semantic Search with BM25\u00b6\n\n    \n    \n    -- Find semantically similar text\n    SELECT * \n    ORDER BY BM25_SIMILARITY(text_column, 'search query text') DESC\n    LIMIT 10\n    \n\n#### Keyword Search\u00b6\n\n    \n    \n    -- Find exact keyword matches\n    SELECT * WHERE CONTAINS(text_column,",
        "node_788": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_111": "* Search for the correct restaurant using a specific sentence \n    * Show the scores \n    * Normalize the score \n    * Fusion method \n    * Show the results \n    * Let's run a search on a multiple dataset \n    * Comparison of Sync vs Async Query Performance \n  * 5) Integrating Image Embeddings for Multi-Modal Search \n    * Create the embedding function for images \n    * Create a new dataset to save the images \n    * Convert the URLs into images \n    * Search similar images \n    * Performing a similar image search based on a specific image \n    * Show similar images and the their respective restaurants \n  * 6) ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT \n  * 7) Discover Restaurants Using ColPali and the Late Interaction Mechanism \n    * Download the ColPali model \n    * Create a new dataset to store the ColPali embeddings \n    * Save the data in the dataset \n    * Chat with images \n    * Retrieve the most similar images \n    * VQA: Visual Question Answering \n\n# Advancing Search Capabilities: From Lexical to Multi-Modal with Deep Lake\u00b6\n\nInstall the main libraries:\n\n    \n    \n    pip install deeplake\n    \n\n## Load the Data from Deep Lake\u00b6\n\nThe following code opens the dataset in read-only mode from Deep Lake at the\nspecified path `al://activeloop/restaurant_reviews_complete`. The\n`scraped_data` object now contains the complete restaurant dataset, featuring\n160 restaurants and over 24,000 images, ready for data extraction and\nprocessing.",
        "node_153": "We can execute asynchronous queries even after loading the dataset\nsynchronously. In the following example, we perform a BM25 query\nasynchronously on a dataset `ds_bm25` that was loaded synchronously.\n\n    \n    \n    result_async_with_bm25 = ds_bm25.query_async(tql_bm25).result()\n    print(result_async_with_bm25)\n    \n\nOutput:\n\n    \n    \n    Dataset(columns=(restaurant_name,restaurant_review,owner_answer,row_id,score), length=5)\n    \n\n## 5) Integrating Image Embeddings for Multi-Modal Search\u00b6\n\nIn the third stage, the system gains multi-modal retrieval capabilities,\nhandling both papers and images. This setup allows for the retrieval of images\nalongside text, making it suitable for fields that require visual data, such\nas medicine and science. The use of cosine similarity on image embeddings\nenables it to rank images based on similarity to the query, while the Vision\nLanguage Model (VLM) allows the system to provide visual answers as well as\ntext.\n\nInstall required libraries\n\n    \n    \n    !pip install -U torchvision\n    !pip install git+https://github.com/openai/CLIP.git\n    \n\nTo set up for image embedding generation, we start by importing necessary\nlibraries.\n\n  1. **Set Device** :\n\n     * We define `device` to use GPU if available, otherwise defaulting to CPU, ensuring compatibility across hardware.\n  2. **Load CLIP Model** :\n\n     * We load the CLIP model (`ViT-B/32`) with its associated preprocessing steps using `clip.load()`. This model is optimized for multi-modal tasks and is set to run on the specified `device`.\n\nThis setup allows us to efficiently process images for embedding, supporting\nmulti-modal applications like image-text similarity.\n\nThe following image illustrates the `CLIP` (Contrastive Language-Image\nPretraining) model's structure, which aligns text and images in a shared\nembedding space, enabling cross-modal understanding.",
        "node_104": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nProvisioning Federated Credentials\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials  Provisioning Federated Credentials  Table of contents \n            * Setting up Federated Credentials in Google Cloud Platform \n            * Next Steps \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Setting up Federated Credentials in Google Cloud Platform \n  * Next Steps \n\n# Provisioning Federated Credentials\u00b6\n\n## Setting up Federated Credentials in Google Cloud Platform\u00b6\n\nThe most secure method for connecting data from your Azure storage to Deep\nLake is using Federated Credentials, which are set up using the steps below:\n\n#### Step 1: Create Google Cloud Service Account\u00b6\n\n1\\. **If you already have a service account, skip to Step 2**\n\n2\\.",
        "node_130": "The introduction of a Large Language Model\n(LLM) allows the system to generate text-based answers, delivering direct\nresponses instead of simply listing relevant documents.\n\nWe open the `vector_search` dataset to perform a hybrid search. First, we\ndefine a query `\"Let's grab a drink\"` and generate its embedding using\n`embedding_function(query)[0]`. We then convert this embedding into a comma-\nseparated string `embedding_string`, preparing it for use in combined text and\nvector-based searches.\n\n    \n    \n    vector_search = deeplake.open(f\"al://{org_id}/{dataset_name_vs}\")\n    \n\n### Search for the correct restaurant using a specific sentence\u00b6\n\n    \n    \n    query = \"I feel like a drink\"\n    embed_query = embedding_function(query)[0]\n    embedding_string = \",\".join(str(c) for c in embed_query)\n    \n\nWe create two queries:\n\n  1. **Vector Search** (`tql_vs`): Calculates cosine similarity with `embedding_string` and returns the top 5 matches by score.\n\n  2. **BM25 Search** (`tql_bm25`): Ranks `restaurant_review` by BM25 similarity to `query`, also limited to the top 5.\n\nWe then execute both queries, storing vector results in `vs_results` and BM25\nresults in `bm25_results`. This allows us to compare results from both search\nmethods.\n\n    \n    \n    tql_vs = f\"\"\"\n        SELECT *, cosine_similarity(embedding, ARRAY[{embedding_string}]) as score\n        FROM (\n            SELECT *, ROW_NUMBER() AS row_id\n        )\n        ORDER BY cosine_similarity(embedding, ARRAY[{embedding_string}]) DESC \n        LIMIT 5\n    \"\"\"\n    \n    tql_bm25 = f\"\"\"\n        SELECT *, BM25_SIMILARITY(restaurant_review, '{query}') as score\n        FROM (\n            SELECT *, ROW_NUMBER() AS row_id\n        ) \n        ORDER BY BM25_SIMILARITY(restaurant_review, '{query}') DESC \n        LIMIT 5\n    \"\"\"\n    \n    vs_results = vector_search.query(tql_vs)\n    bm25_results = vector_search.",
        "node_768": "## \ud83e\udde0 Deep Lake Code Examples by Application\n\n### Vector Store Applications\n\nUsing Deep Lake as a Vector Store for building LLM applications:\n\n### \\- Vector Store Quickstart\n\n### \\- Vector Store Tutorials\n\n### \\- LangChain Integration\n\n### \\- LlamaIndex Integration\n\n### \\- Image Similarity Search with Deep Lake\n\n### Deep Learning Applications\n\nUsing Deep Lake for managing data while training Deep Learning models:\n\n### \\- Deep Learning Quickstart\n\n### \\- Tutorials for Training Models\n\n## \u2699\ufe0f Integrations\n\nDeep Lake offers integrations with other tools in order to streamline your\ndeep learning workflows. Current integrations include:\n\n  * **LLM Apps**\n    * Use Deep Lake as a vector store for LLM apps. Our integration combines the Langchain VectorStores API with Deep Lake datasets as the underlying data storage. The integration is a serverless vector store that can be deployed locally or in a cloud of your choice.\n\n## \ud83d\udcda Documentation\n\nGetting started guides, examples, tutorials, API reference, and other useful\ninformation can be found on our documentation page.\n\n## \ud83c\udf93 For Students and Educators\n\nDeep Lake users can access and visualize a variety of popular datasets through\na free integration with Deep Lake's App. Universities can get up to 1TB of\ndata storage and 100,000 monthly queries on the Tensor Database for free per\nmonth. Chat in on our website: to claim the access!",
        "node_275": "The value returned will be a multidimensional\narray of values rather than the raw image bytes.\n\n**Available formats:**\n\n  * png (default)\n  * apng\n  * jpg / jpeg\n  * tiff / tif\n  * jpeg2000 / jp2\n  * bmp\n  * nii\n  * nii.gz\n  * dcm\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`dtype` |  `DataType | str` |  The data type of the array elements to return |  `'uint8'`  \n`sample_compression` |  `str` |  The on-disk compression/format of the image |  `'png'`  \n  \nExamples:\n\n    \n    \n    ds.add_column(\"col1\", types.Image)\n    ds.add_column(\"col2\", types.Image(sample_compression=\"jpg\"))\n    \n    \n    \n    # Basic image storage\n    ds.add_column(\"images\", deeplake.types.Image())\n    \n    # JPEG compression\n    ds.add_column(\"images\", deeplake.types.Image(\n        sample_compression=\"jpeg\"\n    ))\n    \n    # With specific dtype\n    ds.add_column(\"images\", deeplake.types.Image(\n        dtype=\"uint8\"  # 8-bit RGB\n    ))\n    \n\n##  `` deeplake.types.Embedding \u00b6\n\n    \n    \n    Embedding(\n        size: int | None = None,\n        dtype: DataType | str = \"float32\",\n        index_type: (\n            EmbeddingIndexType | QuantizationType | None\n        ) = None,\n    ) -> Type\n    \n\nCreates a single-dimensional embedding of a given length.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`size` |  `int | None` |  int | None The size of the embedding |  `None`  \n`dtype` |  `DataType | str` |  DataType | str The datatype of the embedding. Defaults to float32 |  `'float32'`  \n`quantization` |  |  QuantizationType | None How to compress the embeddings in the index.",
        "node_722": "For each restaurant in `scraped_data`, we retrieve its name and images, create\nembeddings for the images, and convert them to `UInt8` arrays. Then, we append\nthe restaurant names, images, and embeddings to the dataset and save with\n`vector_search_images.commit()`.\n\n    \n    \n    import numpy as np\n    \n    for sd, rest_images in zip(scraped_data, restaurants_images):\n        restaurant_name = [sd[\"restaurant_name\"]] * len(rest_images)\n        embeddings = embedding_function_images(rest_images, model=model, transform=tform, batch_size=4)\n        vector_search_images.append({\"restaurant_name\": restaurant_name, \"image\": [np.array(fn).astype(np.uint8) for fn in rest_images], \"embedding\": embeddings})\n    \n    vector_search_images.commit()\n    \n\n### Search similar images\u00b6\n\nIf you want direct access to the images and the embeddings, you can copy the\nActiveloop dataset.\n\n    \n    \n    deeplake.copy(\"al://activeloop/restaurant_dataset_images_v4\", f\"al://{org_id}/{image_dataset_name}\")\n    vector_search_images = deeplake.open(f\"al://{org_id}/{image_dataset_name}\")\n    \n\nAlternatively, you can load the dataset you just created.\n\n    \n    \n    vector_search_images = deeplake.open(f\"al://{org_id}/{image_dataset_name}\")\n    vector_search_images\n    \n    \n    \n    query = \"https://www.moltofood.it/wp-content/uploads/2024/09/Hamburger.jpg\"\n    \n    image_query = requests.get(query)\n    image_query_pil = Image.open(BytesIO(image_query.content))\n    \n\n### Performing a similar image search based on a specific image\u00b6\n\n    \n    \n    print(image_query_pil)\n    \n\nOutput:\n\nWe generate an embedding for the query image, `image_query_pil`, by calling\n`embedding_function_images([image_query_pil])[0]`. This embedding is then\nconverted into a comma-separated string, `query_embedding_string`, for\ncompatibility in the query.The query, `tql`, retrieves entries from the\ndataset by calculating cosine similarity between `embedding` and\n`query_embedding_string`.",
        "node_506": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_51": "## \ud83e\udde0 Deep Lake Code Examples by Application\n\n### Vector Store Applications\n\nUsing Deep Lake as a Vector Store for building LLM applications:\n\n### \\- Vector Store Quickstart\n\n### \\- Vector Store Tutorials\n\n### \\- LangChain Integration\n\n### \\- LlamaIndex Integration\n\n### \\- Image Similarity Search with Deep Lake\n\n### Deep Learning Applications\n\nUsing Deep Lake for managing data while training Deep Learning models:\n\n### \\- Deep Learning Quickstart\n\n### \\- Tutorials for Training Models\n\n## \u2699\ufe0f Integrations\n\nDeep Lake offers integrations with other tools in order to streamline your\ndeep learning workflows. Current integrations include:\n\n  * **LLM Apps**\n    * Use Deep Lake as a vector store for LLM apps. Our integration combines the Langchain VectorStores API with Deep Lake datasets as the underlying data storage. The integration is a serverless vector store that can be deployed locally or in a cloud of your choice.\n\n## \ud83d\udcda Documentation\n\nGetting started guides, examples, tutorials, API reference, and other useful\ninformation can be found on our documentation page.\n\n## \ud83c\udf93 For Students and Educators\n\nDeep Lake users can access and visualize a variety of popular datasets through\na free integration with Deep Lake's App. Universities can get up to 1TB of\ndata storage and 100,000 monthly queries on the Tensor Database for free per\nmonth. Chat in on our website: to claim the access!",
        "node_314": "Changes are persisted immediately.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`key` |  `str` |  Metadata key to set |  _required_  \n`value` |  `Any` |  Value to store |  _required_  \n  \nExamples:\n\n    \n    \n    ds.metadata[\"train_split\"] = 0.8\n    ds.metadata[\"val_split\"] = 0.1\n    ds.metadata[\"test_split\"] = 0.1\n    \n\n#####  `` keys \u00b6\n\n    \n    \n    keys() -> list[str]\n    \n\nLists all available metadata keys.\n\nReturns:\n\nType | Description  \n---|---  \n`list[str]` |  list[str]: List of metadata key names  \n  \nExamples:\n\n    \n    \n    # Print all metadata\n    for key in metadata.keys():\n        print(f\"{key}: {metadata[key]}\")\n    \n    \n    \n    # Set dataset metadata\n    ds.metadata[\"description\"] = \"Training dataset\"\n    ds.metadata[\"version\"] = \"1.0\"\n    ds.metadata[\"params\"] = {\n        \"image_size\": 224,\n        \"mean\": [0.485, 0.456, 0.406],\n        \"std\": [0.229, 0.224, 0.225]\n    }\n    \n    # Read dataset metadata\n    description = ds.metadata[\"description\"]\n    params = ds.metadata[\"params\"]\n    \n    # List all metadata keys\n    for key in ds.metadata.keys():\n        print(f\"{key}: {ds.metadata[key]}\")\n    \n\n## Column Metadata\u00b6\n\n####  `` deeplake.ReadOnlyMetadata \u00b6\n\nRead-only access to dataset and column metadata for ML workflows.\n\nStores important information about datasets like: \\- Model parameters and\nhyperparameters \\- Preprocessing statistics (mean, std, etc.) \\- Data splits\nand fold definitions \\- Version and training information\n\nExamples:\n\nAccessing model metadata:\n\n    \n    \n    metadata = ds.metadata\n    model_name = metadata[\"model_name\"]\n    model_params = metadata[\"hyperparameters\"]\n    \n\nReading preprocessing stats:\n\n    \n    \n    mean = ds[\"images\"].metadata[\"mean\"]\n    std = ds[\"images\"].metadata[\"std\"]\n    \n\n#####  `` __contains__ \u00b6\n\n    \n    \n    __contains__(key: str) -> bool\n    \n\nChecks if the metadata contains the given key.",
        "node_469": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_623": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_9": "Skip to content\n\n## Navigation Menu\n\nToggle navigation\n\nSign in\n\n  * Product \n\n    * GitHub Copilot\n\nWrite better code with AI\n\n    * GitHub Advanced Security\n\nFind and fix vulnerabilities\n\n    * Actions\n\nAutomate any workflow\n\n    * Codespaces\n\nInstant dev environments\n\n    * Issues\n\nPlan and track work\n\n    * Code Review\n\nManage code changes\n\n    * Discussions\n\nCollaborate outside of code\n\n    * Code Search\n\nFind more,",
        "node_159": "For each restaurant in `scraped_data`, we retrieve its name and images, create\nembeddings for the images, and convert them to `UInt8` arrays. Then, we append\nthe restaurant names, images, and embeddings to the dataset and save with\n`vector_search_images.commit()`.\n\n    \n    \n    import numpy as np\n    \n    for sd, rest_images in zip(scraped_data, restaurants_images):\n        restaurant_name = [sd[\"restaurant_name\"]] * len(rest_images)\n        embeddings = embedding_function_images(rest_images, model=model, transform=tform, batch_size=4)\n        vector_search_images.append({\"restaurant_name\": restaurant_name, \"image\": [np.array(fn).astype(np.uint8) for fn in rest_images], \"embedding\": embeddings})\n    \n    vector_search_images.commit()\n    \n\n### Search similar images\u00b6\n\nIf you want direct access to the images and the embeddings, you can copy the\nActiveloop dataset.\n\n    \n    \n    deeplake.copy(\"al://activeloop/restaurant_dataset_images_v4\", f\"al://{org_id}/{image_dataset_name}\")\n    vector_search_images = deeplake.open(f\"al://{org_id}/{image_dataset_name}\")\n    \n\nAlternatively, you can load the dataset you just created.\n\n    \n    \n    vector_search_images = deeplake.open(f\"al://{org_id}/{image_dataset_name}\")\n    vector_search_images\n    \n    \n    \n    query = \"https://www.moltofood.it/wp-content/uploads/2024/09/Hamburger.jpg\"\n    \n    image_query = requests.get(query)\n    image_query_pil = Image.open(BytesIO(image_query.content))\n    \n\n### Performing a similar image search based on a specific image\u00b6\n\n    \n    \n    print(image_query_pil)\n    \n\nOutput:\n\nWe generate an embedding for the query image, `image_query_pil`, by calling\n`embedding_function_images([image_query_pil])[0]`. This embedding is then\nconverted into a comma-separated string, `query_embedding_string`, for\ncompatibility in the query.The query, `tql`, retrieves entries from the\ndataset by calculating cosine similarity between `embedding` and\n`query_embedding_string`.",
        "node_596": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_597": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_101": "For example, in Azure Machine Learning Studio, when creating a compute\ninstance, toggle `Assign Identity` and select the `Managed Identity` from Step\n1.\n\n## Step 3: Create a Deep Lake Workload Identity using the Azure Managed\nIdentity\u00b6\n\nNavigate to the `Permissions` tab for your organization in the Deep Lake App,\nlocate the `Workload Identities`, and select `Add`.\n\nSpecify a `Display Name`, `Client ID` (for the Managed Identity), and `Tenant\nID`. The `Client ID` can be found in the main page for the Managed Identity,\nand the `Tenant ID` can be found in `Tenant Properties` in Azure. Click `Add`.\n\n## Step 4: Run the workload\u00b6\n\nSpecify the environmental variables below in the Deep Lake client and run\nother Deep APIs as normal.\n\n    \n    \n    #### THIS IS THE CLIENT_ID FOR THE COMPUTE INSTANCE\n    #### NOT THE MANAGED IDENTITY \n    os.environ[\"AZURE_CLIENT_ID\"] = azure_client_id\n    \n    os.environ[\"ACTIVELOOP_AUTH_PROVIDER\"] = \"azure\"\n    \n\nSpecifying the `AZURE_CLIENT_ID` is not necessary in some environments because\nthe correct value may automatically be set.\n\nFor a compute instance in the Azure Machine Learning Studio, the Client ID can\nbe found in instance settings below:\n\nBack to top",
        "node_652": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_315": "Stores important information about datasets like: \\- Model parameters and\nhyperparameters \\- Preprocessing statistics (mean, std, etc.) \\- Data splits\nand fold definitions \\- Version and training information\n\nExamples:\n\nAccessing model metadata:\n\n    \n    \n    metadata = ds.metadata\n    model_name = metadata[\"model_name\"]\n    model_params = metadata[\"hyperparameters\"]\n    \n\nReading preprocessing stats:\n\n    \n    \n    mean = ds[\"images\"].metadata[\"mean\"]\n    std = ds[\"images\"].metadata[\"std\"]\n    \n\n#####  `` __contains__ \u00b6\n\n    \n    \n    __contains__(key: str) -> bool\n    \n\nChecks if the metadata contains the given key.\n\n#####  `` __getitem__ \u00b6\n\n    \n    \n    __getitem__(key: str) -> Any\n    \n\nGets metadata value for the given key.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`key` |  `str` |  Metadata key to retrieve |  _required_  \n  \nReturns:\n\nType | Description  \n---|---  \n`Any` |  The stored metadata value  \n  \nExamples:\n\n    \n    \n    mean = ds[\"images\"].metadata[\"mean\"]\n    std = ds[\"images\"].metadata[\"std\"]\n    \n\n#####  `` keys \u00b6\n\n    \n    \n    keys() -> list[str]\n    \n\nLists all available metadata keys.\n\nReturns:\n\nType | Description  \n---|---  \n`list[str]` |  list[str]: List of metadata key names  \n  \nExamples:\n\n    \n    \n    # Print all metadata\n    for key in metadata.keys():\n        print(f\"{key}: {metadata[key]}\")\n    \n    \n    \n    # Set column metadata\n    ds[\"images\"].metadata[\"mean\"] = [0.485, 0.456, 0.406]\n    ds[\"images\"].metadata[\"std\"] = [0.229, 0.224, 0.225]\n    ds[\"labels\"].metadata[\"class_names\"] = [\"cat\", \"dog\", \"bird\"]\n    \n    # Read column metadata\n    mean = ds[\"images\"].metadata[\"mean\"]\n    class_names = ds[\"labels\"].metadata[\"class_names\"]\n    ds.commit() # Commit the changes to the dataset\n    \n\nBack to top",
        "node_762": "ai\n\nactiveloop.ai\n\n### License\n\nApache-2.0 license\n\n8.5k stars  656 forks  Branches Tags Activity\n\nStar\n\nNotifications  You must be signed in to change notification settings\n\n  * Code\n  * Issues 49\n  * Pull requests 9\n  * Discussions\n  * Actions\n  * Projects 0\n  * Wiki\n  * Security\n  * Insights\n\nAdditional navigation options\n\n  * Code \n  * Issues \n  * Pull requests \n  * Discussions \n  * Actions \n  * Projects \n  * Wiki \n  * Security \n  * Insights \n\n# activeloopai/deeplake\n\nmain\n\nBranchesTags\n\nGo to file\n\nCode\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\n## History\n\n9,213 Commits  \n.github| .github|  |   \npython/deeplake| python/deeplake|  |   \n.gitignore| .gitignore|  |   \n.pre-commit-config.yaml| .pre-commit-config.yaml|  |   \nCONTRIBUTING.md| CONTRIBUTING.md|  |   \nLICENSE| LICENSE|  |   \nREADME.md| README.md|  |   \nSECURITY.md| SECURITY.md|  |   \nView all files  \n  \n## Repository files navigation\n\n  * README\n  * Apache-2.",
        "node_394": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_664": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nQuickstart Guide\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart  Quickstart  Table of contents \n      * Installation \n      * Creating a Dataset \n      * Adding Data \n      * Accessing Data \n      * Vector Search \n      * Data Versioning \n      * Async Operations \n      * Next Steps \n      * Support \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Installation \n  * Creating a Dataset \n  * Adding Data \n  * Accessing Data \n  * Vector Search \n  * Data Versioning \n  * Async Operations \n  * Next Steps \n  * Support \n\n# Quickstart Guide\u00b6\n\nGet started with Deep Lake by following these examples.",
        "node_588": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_642": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_404": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_277": "Parameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`index_type` |  `str | TextIndexEnumType | TextIndexType | None` |  str | TextIndexEnumType | TextIndexType | None How to index the data in the column for faster searching. Options are:\n\n  * :class:`deeplake.types.Inverted`\n  * :class:`deeplake.types.BM25`\n\nDefault is `None` meaning \"do not index\" |  `None`  \n  \nReturns:\n\nName | Type | Description  \n---|---|---  \n`Type` |  `Type` |  A new text data type.  \n  \nExamples:\n\nCreate text columns with different configurations:\n\n    \n    \n    ds.add_column(\"col1\", types.Text)\n    ds.add_column(\"col2\", \"text\")\n    ds.add_column(\"col3\", str)\n    ds.add_column(\"col4\", types.Text(index_type=types.Inverted))\n    ds.add_column(\"col5\", types.Text(index_type=types.BM25))\n    \n    \n    \n    # Basic text\n    ds.add_column(\"text\", deeplake.types.Text())\n    \n    # Text with BM25 index for semantic search\n    ds.add_column(\"text2\", deeplake.types.Text(\n        index_type=deeplake.types.BM25\n    ))\n    \n    # Text with inverted index for keyword search\n    ds.add_column(\"text3\", deeplake.types.Text(\n        index_type=deeplake.types.Inverted\n    ))\n    \n\n##  `` deeplake.types.Dict \u00b6\n\n    \n    \n    Dict() -> Type\n    \n\nCreates a type that supports storing arbitrary key/value pairs in each row.\n\nReturns:\n\nName | Type | Description  \n---|---|---  \n`Type` |  `Type` |  A new dictionary data type.  \nSee Also\n\n:func:`deeplake.types.Struct` for a type that supports defining allowed keys.",
        "node_565": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_92": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nEnabling CORS in Azure\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure  Enabling CORS in Azure  Table of contents \n            * Steps for enabling CORS in Azure \n            * Next Steps \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Steps for enabling CORS in Azure \n  * Next Steps \n\n# Enabling CORS in Azure\u00b6\n\nCross-Origin Resource Sharing (CORS) is typically enabled by default in Azure.\nIf that's not the case in your Azure account, please enable CORS in order to\nuse the Deep Lake app to visualize Deep Lake datasets stored in your own Azure\nstorage. CORS should be enabled in the storage account containing the Deep\nLake dataset and any linked data.\n\n## Steps for enabling CORS in Azure\u00b6\n\n1\\. Login to the Azure.\n\n2\\.",
        "node_274": "The type system includes basic numeric types as well as\nspecialized types optimized for common data formats like images, embeddings,\nand text.\n\nEach type can be specified either using the full type class or a string\nshorthand:\n\n    \n    \n    # Using type class\n    ds.add_column(\"col1\", deeplake.types.Float32())\n    \n    # Using string shorthand\n    ds.add_column(\"col2\", \"float32\")\n    \n\n#### Types determine:\u00b6\n\n  * How data is stored and compressed\n  * What operations are available\n  * How the data can be queried and indexed\n  * Integration with external libraries and frameworks\n\n## Numeric Types\u00b6\n\nAll basic numeric types:\n\n    \n    \n    import deeplake\n    \n    # Integers\n    ds.add_column(\"int8\", deeplake.types.Int8())      # -128 to 127\n    ds.add_column(\"int16\", deeplake.types.Int16())    # -32,768 to 32,767\n    ds.add_column(\"int32\", deeplake.types.Int32())    # -2^31 to 2^31-1\n    ds.add_column(\"int64\", deeplake.types.Int64())    # -2^63 to 2^63-1\n    \n    # Unsigned Integers\n    ds.add_column(\"uint8\", deeplake.types.UInt8())    # 0 to 255\n    ds.add_column(\"uint16\", deeplake.types.UInt16())  # 0 to 65,535\n    ds.add_column(\"uint32\", deeplake.types.UInt32())  # 0 to 2^32-1\n    ds.add_column(\"uint64\", deeplake.types.UInt64())  # 0 to 2^64-1\n    \n    # Floating Point\n    ds.add_column(\"float32\", deeplake.types.Float32())\n    ds.add_column(\"float64\", deeplake.types.Float64())\n    \n\n##  `` deeplake.types.Image \u00b6\n\n    \n    \n    Image(\n        dtype: DataType | str = \"uint8\",\n        sample_compression: str = \"png\",\n    ) -> Type\n    \n\nAn image of a given format. The value returned will be a multidimensional\narray of values rather than the raw image bytes.",
        "node_423": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_240": "See `deeplake.schema` such as deeplake.schemas.TextEmbeddings for common starting schemas. |  `None`  \n  \nExamples:\n\n    \n    \n    # Create a dataset in your local filesystem:\n    ds = deeplake.create(\"directory_path\")\n    ds.add_column(\"id\", types.Int32())\n    ds.add_column(\"url\", types.Text())\n    ds.add_column(\"embedding\", types.Embedding(768))\n    ds.commit()\n    ds.summary()\n    \n    \n    \n    # Create dataset in your app.activeloop.ai organization:\n    ds = deeplake.create(\"al://organization_id/dataset_name\")\n    \n    # Create a dataset stored in your cloud using specified credentials:\n    ds = deeplake.create(\"s3://mybucket/my_dataset\",\n        creds = {\"aws_access_key_id\": id, \"aws_secret_access_key\": key})\n    \n    # Create dataset stored in your cloud using app.activeloop.ai managed credentials.\n    ds = deeplake.create(\"s3://mybucket/my_dataset\",\n        creds = {\"creds_key\": \"managed_creds_key\"}, org_id = \"my_org_id\")\n    \n    ds = deeplake.create(\"azure://bucket/path/to/dataset\")\n    \n    ds = deeplake.create(\"gcs://bucket/path/to/dataset\")\n    \n    ds = deeplake.create(\"mem://in-memory\")\n    \n\nRaises:\n\nType | Description  \n---|---  \n`LogExistsError` |  if a dataset already exists at the given URL  \n  \n###  `` deeplake.open \u00b6\n\n    \n    \n    open(\n        url: str,\n        creds: dict[str, str] | None = None,\n        token: str | None = None,\n    ) -> Dataset\n    \n\nOpens an existing dataset, potenitally for modifying its content.\n\nSee deeplake.open_read_only for opening the dataset in read only mode\n\nTo create a new dataset, see deeplake.create\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`url` |  `str` |  The URL of the dataset.",
        "node_480": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_700": "'}, score=0.7132230191866898),\n     '2637': Document(id='2637', data={'restaurant_name': 'Mifen101 \u82b1\u6eaa\u7c73\u7c89\u738b', 'restaurant_review': 'Feel like I\u2019m back in China.'}, score=0.10997834807700335),\n     '11383': Document(id='11383', data={'restaurant_name': 'Ludwigs Biergarten Mountain View', 'restaurant_review': 'Beer is fresh tables are big feel like a proper beer garden'}, score=0.09158030054295993),\n     '2496': Document(id='2496', data={'restaurant_name': 'Seasons Noodles & Dumplings Garden', 'restaurant_review': 'Comfort food, excellent service! Feel like back to home.'}, score=0.04344738382536802),\n     '10788': Document(id='10788', data={'restaurant_name': 'Casa Lupe', 'restaurant_review': 'Run by a family that makes you feel like part of the family. Awesome food. I love their wet Chili Verde burritos'}, score=0.04177094836797888)}\n    \n\n### Fusion method\u00b6\n\nWe define weights for our hybrid search: `VECTOR_WEIGHT` and `LEXICAL_WEIGHT`\nare both set to `0.5`, giving equal importance to vector-based and BM25\nscores.\n\n  1. **Initialize Results Dictionary** : \n\n     * We create an empty dictionary, `results`, to store documents with their combined scores from both search methods.\n  2. **Combine Scores** : \n\n     * We iterate over the unique document IDs from `docs_vs` and `docs_bm25`.\n\n     * For each document: \n\n       * We add it to `results`, defaulting to the version available (vector or BM25).\n       * We calculate a weighted score: `vs_score` from vector results (if present in `docs_vs`) and `bm_score` from BM25 results (if present in `docs_bm25`).",
        "node_698": "**Apply Softmax to Scores** : \n\n     * We extract `score` values from `vs_results` and `bm25_results` and apply `softmax` to them, storing the results in `vss` and `bm25s`. This step scales both sets of scores for easy comparison.\n  2. **Create Document Dictionaries** : \n\n     * We create dictionaries `docs_vs` and `docs_bm25` to store documents from `vs_results` and `bm25_results`, respectively. For each result, we add the `restaurant_name` and `restaurant_review` along with the normalized score. Each document is identified by `row_id`.\n\nThis code standardizes scores and organizes results, allowing comparison\nacross both vector and BM25 search methods.\n\n    \n    \n    vs_score = vs_results[\"score\"]\n    bm_score = bm25_results[\"score\"]\n    \n    vss = softmax(vs_score)\n    bm25s = softmax(bm_score)\n    print(vss)\n    print(bm25s)\n    print(vs_results)\n    \n\nOutput:\n\n    \n    \n    [0.21224761685297047, 0.19800771415362647, 0.1970674552539808, 0.19663342673946818, 0.19604378699995426]\n    [0.7132230191866898, 0.10997834807700335, 0.09158030054295993, 0.04344738382536802, 0.04177094836797888]\n    Dataset(columns=(embedding,restaurant_name,restaurant_review,owner_answer,row_id,score), length=5)\n    \n    \n    \n    docs_vs = {}\n    docs_bm25 = {}\n    for el, score in zip(vs_results, vss):\n        docs_vs[str(el[\"row_id\"])] = Document(id=str(el[\"row_id\"]), data={\"restaurant_name\": el[\"restaurant_name\"], \"restaurant_review\": el[\"restaurant_review\"]}, score=score)\n    \n    for el, score in zip(bm25_results, bm25s):\n        docs_bm25[str(el[\"row_id\"])] = Document(id=str(el[\"row_id\"]), data={\"restaurant_name\": el[\"restaurant_name\"], \"restaurant_review\": el[\"restaurant_review\"]}, score=score)\n    print(docs_vs)\n    print(docs_bm25)\n    \n\nOutput:\n\n    \n    \n    {'17502': Document(id='17502', data={'restaurant_name': \"St."
    },
    "relevant_docs": {
        "548cf161-c5c0-47f8-b1a0-0a9d4e671e50": [
            "node_323"
        ],
        "51d0ca50-c8cf-46cb-8766-0f2de56842d8": [
            "node_35"
        ],
        "4470ecb5-f093-4712-882a-2049668d4aac": [
            "node_725"
        ],
        "369e312c-8ece-4c92-a81d-0b9541b50db8": [
            "node_4"
        ],
        "5f510598-e49a-44e9-8f0a-907ee8cba780": [
            "node_201"
        ],
        "05bf8662-c342-4155-8e16-d2ac29419d19": [
            "node_580"
        ],
        "17205696-7940-48a6-84d3-f99fc18e445f": [
            "node_529"
        ],
        "7b3d35cc-b857-454a-888b-1114b205b64b": [
            "node_639"
        ],
        "aa3a6b46-b6fe-4971-b2c6-8dd39b77c58f": [
            "node_764"
        ],
        "7025dd5c-4152-4a85-b7e9-168d66ae235f": [
            "node_278"
        ],
        "9ecc8d12-4aa2-4e72-a2bf-08331422307d": [
            "node_646"
        ],
        "53075fb7-fbce-4684-9e0c-941c9f5bb866": [
            "node_606"
        ],
        "4a741322-b003-4c08-8ff4-35588179315e": [
            "node_713"
        ],
        "900981c7-e057-468a-870e-4f73280bea01": [
            "node_293"
        ],
        "b97e8ac1-c41c-4935-b6aa-922ae1ae6ed6": [
            "node_720"
        ],
        "cf6b3251-7b2e-4320-b7f4-8cad958f5197": [
            "node_524"
        ],
        "86a9b548-f701-4193-8ed4-c7682f57c642": [
            "node_672"
        ],
        "528576d4-513f-4e25-b20d-47a20097d3ed": [
            "node_509"
        ],
        "d5cb09c2-7b9b-4063-98f1-57b62c97b202": [
            "node_608"
        ],
        "9c30e118-5037-4e5c-ae5c-f95920b800e7": [
            "node_244"
        ],
        "b7fdb6c9-0ebf-4a68-a70d-a632e833fb8e": [
            "node_369"
        ],
        "bb917a44-5c8f-4a13-9d37-6ba99390e62a": [
            "node_85"
        ],
        "5ad1c81f-2a1e-4aad-87da-d73237be3620": [
            "node_40"
        ],
        "389e65a5-fb2e-46c4-8b75-f86656c42a79": [
            "node_575"
        ],
        "b82ad47e-8afb-41e8-b249-5d826b2bead5": [
            "node_459"
        ],
        "eedf68c4-2601-4798-b0da-1d0287705904": [
            "node_7"
        ],
        "eb190a76-8248-4e97-b7bb-4e58c7d0d7da": [
            "node_250"
        ],
        "75cfbddc-358a-4298-bf2f-c7649911a69f": [
            "node_328"
        ],
        "b1e5e50a-b0be-43f1-8d09-f28d0b201a20": [
            "node_727"
        ],
        "4635ef1c-dbbd-45bc-86fe-95f9572fea59": [
            "node_208"
        ],
        "2e624717-4c1f-4ccf-802e-6786a22602a9": [
            "node_618"
        ],
        "519434dc-33ec-4d0d-abc3-529dc590424b": [
            "node_157"
        ],
        "039e73fc-7d4e-4abc-aa07-a45738b7de98": [
            "node_747"
        ],
        "2bed4a78-4771-4382-8d11-6f227af3b0e8": [
            "node_429"
        ],
        "7a5450f9-9277-4e32-8c5f-0d7b35ca5197": [
            "node_430"
        ],
        "c0060d19-7900-464d-a440-d7346bc6cf77": [
            "node_411"
        ],
        "7ddeb6fd-16d9-4307-b2da-7b2ca4ecd812": [
            "node_638"
        ],
        "8fed457d-66a7-453c-a25b-0f1ce034dd8c": [
            "node_262"
        ],
        "884430b3-c9e5-46c8-b867-5db63fd0b5d9": [
            "node_688"
        ],
        "c00608bc-686f-46b0-8bfc-f608e73bc0c0": [
            "node_227"
        ],
        "03928690-c671-4483-bd1f-68922ce5a880": [
            "node_81"
        ],
        "4567d9bd-d459-4089-a960-987b4f006794": [
            "node_389"
        ],
        "6717cf82-84c7-4c11-bb6a-eebce8921b0e": [
            "node_32"
        ],
        "16f630eb-d88d-4e6f-8112-13fed3fbd0d6": [
            "node_390"
        ],
        "5f8c85dd-3817-4130-846e-b40b26d0a7af": [
            "node_421"
        ],
        "0bb4c13e-a799-494e-b5d7-06b52fab9464": [
            "node_125"
        ],
        "d6ba3c56-8de2-4cb4-89f9-5ddd6f6d8f42": [
            "node_485"
        ],
        "dffa15d9-aca4-4523-afb8-8b9224844d32": [
            "node_440"
        ],
        "2264f239-65ea-4bf0-a1f0-788eb4b38bd7": [
            "node_771"
        ],
        "840f5739-71e2-48d1-9a26-d7d819586369": [
            "node_216"
        ],
        "37e444e4-c1b5-45e0-a2ab-ff0484ffc276": [
            "node_442"
        ],
        "d6082df1-b545-4def-8f62-442dd02c431a": [
            "node_636"
        ],
        "144c8438-83fe-4baf-96f1-69c70ab54ee2": [
            "node_162"
        ],
        "ca6f2569-fe93-4e0d-a7cc-b2aff16e57a3": [
            "node_180"
        ],
        "d08f1531-f43c-4b5d-ae37-9c9756669338": [
            "node_292"
        ],
        "da43a813-35b1-4ef6-81d4-322346018039": [
            "node_766"
        ],
        "22ad02b5-6492-4847-83a8-c36881e2831e": [
            "node_53"
        ],
        "cfdbc698-dd7d-435e-8a36-45978ce7ae92": [
            "node_748"
        ],
        "0f8d3903-530e-471f-9506-cbdf19b20b55": [
            "node_41"
        ],
        "b796c72a-7489-4b88-b3aa-fc9758f4c049": [
            "node_152"
        ],
        "678d1745-09ab-4a17-8691-d5f2cc23d64a": [
            "node_743"
        ],
        "63c89274-1a10-4474-b9ea-005411f9be01": [
            "node_117"
        ],
        "8321bf8e-cdea-47e8-940a-051ecaf3b7d4": [
            "node_375"
        ],
        "69ed11b9-8c9e-45c5-9167-faa9228f7385": [
            "node_12"
        ],
        "5955feeb-2141-4c91-87c7-80332fbfcab8": [
            "node_587"
        ],
        "bf4458c3-4a36-4b7d-aaa5-f84482cc8877": [
            "node_673"
        ],
        "bb61708f-731b-49aa-8982-b73338bd2fb0": [
            "node_478"
        ],
        "c56e0b7c-0f18-4669-b6da-e63408423704": [
            "node_3"
        ],
        "a8455a4d-75c5-426b-a13a-a1869ed59332": [
            "node_122"
        ],
        "45db230d-b93a-4563-9274-b91446217944": [
            "node_397"
        ],
        "99311678-d015-4e67-b3c9-6571cf047afe": [
            "node_774"
        ],
        "6397b3eb-822f-4d85-86ce-b4aaffc23181": [
            "node_131"
        ],
        "a333f680-90a3-473f-8cfa-377c0b593986": [
            "node_318"
        ],
        "d9f762f0-9b2b-420b-a40b-a70a250867d3": [
            "node_383"
        ],
        "fa619f76-791d-4a31-8484-aa106cfcadb8": [
            "node_39"
        ],
        "09cdd057-6fc5-4d61-accf-a69ae2bec4f6": [
            "node_681"
        ],
        "c8fc8986-8d23-4b57-a9be-946a79fa5b3d": [
            "node_645"
        ],
        "9b829e2b-09da-4271-afc9-936fbb03623f": [
            "node_622"
        ],
        "5173a539-f60d-485a-9aa3-4b1c48b41df4": [
            "node_333"
        ],
        "c65c8076-fa32-4b4b-9090-db03728f2b7a": [
            "node_718"
        ],
        "24c33cee-5f78-4b06-813e-14cc812f0adc": [
            "node_709"
        ],
        "89e33d7c-c1c1-4920-a7ae-745a8bc0de56": [
            "node_219"
        ],
        "c2cf08a5-01d9-4682-9c5c-0a43d022dc69": [
            "node_461"
        ],
        "d640f74b-413e-4d86-bb4f-d517cce43fb6": [
            "node_368"
        ],
        "29b2d6c1-497a-4e26-a1a3-565fcf6b1f03": [
            "node_330"
        ],
        "e460b898-c6f2-45fb-90a8-fc82d7134076": [
            "node_788"
        ],
        "0dc871d5-2918-4916-bbc8-c25d0897b606": [
            "node_111"
        ],
        "cfbcb460-d14c-4469-aecb-2faeaba2c3ef": [
            "node_153"
        ],
        "49c36df2-d971-4e9a-8f89-46e401d1ec48": [
            "node_104"
        ],
        "ffecdacc-d265-4f23-91a4-bdfd14992e3b": [
            "node_130"
        ],
        "b5c0f043-3368-4124-b3ea-5db5f1288883": [
            "node_768"
        ],
        "36904a92-538a-42f5-a3df-a64431a7a17e": [
            "node_275"
        ],
        "d3692c77-6207-497b-a2c6-82e2e0e62f51": [
            "node_722"
        ],
        "634da38f-6d46-4cc8-ac3a-f9bd4463aafc": [
            "node_506"
        ],
        "eb9d9695-55f7-452b-9ae8-4ce8783f9d0d": [
            "node_51"
        ],
        "6114c8dd-0d8e-4091-a776-349eb7a9d5e3": [
            "node_314"
        ],
        "3ece9c39-d25a-4e32-841f-76ffecf7b80c": [
            "node_469"
        ],
        "bb80039c-be9a-4d4e-bb91-c9e5e309fab1": [
            "node_623"
        ],
        "9f78f2e3-c7bc-44d8-9a82-c7dfa9f8ca8b": [
            "node_9"
        ],
        "06f80e4c-c839-4ba1-abb4-e682998acb65": [
            "node_159"
        ],
        "0f867d6b-bab8-45b2-8d1b-6eb32577f018": [
            "node_596"
        ],
        "f1d0a124-7588-4dc4-91d8-6b5f2dd85392": [
            "node_597"
        ],
        "879ee449-c314-4b9f-a2b6-880381a6c718": [
            "node_101"
        ],
        "454edc7f-20dd-43ca-b920-6e7a14ec918a": [
            "node_652"
        ],
        "9f85cb16-0d5c-4a22-bac3-fc4346aabd59": [
            "node_315"
        ],
        "8d9ac316-0834-4ace-982e-3ad797f286d2": [
            "node_762"
        ],
        "d7a69ac6-8df3-4a5a-9e2a-07b5b9f52ead": [
            "node_394"
        ],
        "e0dec67e-1447-4385-8dec-746f975f001a": [
            "node_664"
        ],
        "1981027e-2b82-4152-ad29-0f21064cc7f9": [
            "node_588"
        ],
        "16954472-7d92-4db4-896f-bd7a8dc8d8d5": [
            "node_642"
        ],
        "e676ee97-61ff-4e23-9ec4-8dbedeed234f": [
            "node_404"
        ],
        "a59d87ec-4c20-488c-87d2-a7e23f763270": [
            "node_277"
        ],
        "d8884331-76e5-40d9-81c5-2782d560fabf": [
            "node_565"
        ],
        "a3a5c9b0-e992-4226-95ba-dc3851fa3cea": [
            "node_92"
        ],
        "5367a861-a1b0-4d91-a1e6-71c61878619a": [
            "node_274"
        ],
        "057df539-a3b6-4d74-a597-303d8404aabd": [
            "node_423"
        ],
        "67ca98e6-70a2-4ac5-8636-35a5ee68c432": [
            "node_240"
        ],
        "daa3a46a-e47f-4485-8514-fded793ad63a": [
            "node_480"
        ],
        "dfc174f7-a0b8-4d0d-86e4-c6cc0d0eec38": [
            "node_700"
        ],
        "953400aa-4a27-48f0-9e36-b46db02c03c2": [
            "node_698"
        ]
    },
    "mode": "text"
}