{
    "queries": {
        "7b8a3efc-586a-4ae4-b8f5-37d32ef0e9a1": "Here are five questions that can be asked based on the given context:",
        "d3329133-d58f-4fd6-8428-4433957d2123": "Here are five questions that cover various aspects of the Deep Lake database:",
        "9a660b28-5dbc-4743-b08e-7d9bad0204d8": "Here are five diverse questions based on the context information:",
        "328ce107-2ae6-4150-8251-cd430136c44e": "Here are five questions that can be asked based on the provided context:",
        "5ebb5419-5c6e-49e7-a040-6148a32d8585": "Here are some potential questions based on the provided context:",
        "915ab960-5eec-48bf-8f09-6909d6ca83d7": "Here are five questions that cover different aspects of the provided context information:",
        "64a72ae7-1168-4dbf-a073-7ad8655665a7": "Here are five potential questions that can be asked based on the given context information:",
        "df2cb9b3-216f-4778-b0fb-6b4925cb7fd4": "Here are 7 questions based on the given context information:",
        "34339025-c9e2-471a-b299-ebd139130473": "Based on the provided context information, here's a question that tests the student's understanding of Deeplake query syntax and operations:",
        "6fbe6469-1eb8-4636-b7f1-c058f8def23d": "Here are some potential questions that can be derived from the given context:",
        "d54d7704-ed33-41ab-9936-a1d80a26ba0f": "Here are five questions that can be asked on the Deep Lake: Multi-Modal AI Database based on the provided context information:",
        "49b457df-0824-4e29-9609-9acebb0aa147": "Here are five questions based on the provided context information:",
        "f661e35d-33f6-4c18-b47d-347fd4a335e5": "Here are five questions based on the context information:",
        "5bc7184f-c221-4301-a4ae-bbe0d6a2f377": "Here are five questions based on the context information:",
        "c6e5e12d-55b2-43d5-b1e7-cb49a7f31307": "Here are some potential questions based on the given context:",
        "2f424ee8-bb94-423d-b976-9f08e770a233": "Here are five questions based on the provided context information:",
        "e8ffaf99-9b97-4d22-87e3-e5d321731254": "Here's a question that covers various aspects of the Deeplake ReadOnlyDataset:",
        "2dbe503c-e399-4ec9-be7a-965ba6bd95bf": "Here are five questions based on the given context:",
        "c16a5d09-960a-47d2-ac06-fc12f144a5af": "Here are five questions based on the provided context information:",
        "aebd4d1b-021a-47c0-8b2b-971ea30c85ae": "Here are five questions based on the given context information:",
        "98991fcc-bb2e-4ad3-93ab-1aabbf066140": "Here are five questions that can be used for an upcoming quiz/examination based on the provided context information:",
        "0e6a6ae6-5499-444e-add0-88a8504e326d": "Here's a question that tests understanding of the dataset functionality:",
        "42db95a1-b46d-4767-a0c6-f9938df0628d": "Here are some potential questions that can be asked based on the provided context:",
        "15022be8-becd-4afb-b3c4-7d22c38a4139": "Here are some potential questions that can be generated based on the provided context:",
        "ba5fe3dd-a1fb-4575-ad64-beff7a89f8ec": "Here's a question that can be generated based on the given context:",
        "a602a0c3-ae5a-47a7-8a9f-40baa75378e0": "Here are some questions based on the context information:",
        "60f7d60d-370b-4bba-8e99-06af50c9ef12": "Here are five questions based on the context information:",
        "8000b42c-a065-4b9a-a2f1-f410d949c21f": "Here are some potential questions that can be derived from the given context:",
        "d9a9a78b-ed89-4532-b66c-44c97bc6da52": "Here are some questions that can be asked based on the provided context information:",
        "6e73435e-dbba-450b-a89b-7acb023df89e": "Here's a potential question:",
        "eb4839b8-220b-4ea9-b564-f80d5c3f9e26": "Here are some possible questions that can be asked based on the provided context information:",
        "db697bcd-987c-476b-8274-022a620a81bb": "Here are some potential questions based on the given context:",
        "1473bf81-c12f-4413-8508-4c4e2ab9a6cf": "Here's a question that covers the key features of Deep Lake:",
        "1015b99e-6673-4c5d-b203-aa590c90936c": "Here are five questions based on the given context information:",
        "3b5876d7-a8b8-479a-88f0-b5006d000b8b": "Here are five questions based on the provided context information:",
        "9034725b-73e1-4704-9e3b-5267970bec68": "Here are some potential questions that could be asked based on the provided context:",
        "dc7eeac7-5176-46b0-98e3-dcccda01757b": "Here are five questions based on the provided context information:",
        "7505e933-65c3-477c-ba92-f241b529a03b": "Here are 5 questions based on the provided context:",
        "9284b072-f884-4311-b693-3e3cd276b3a3": "Here are five questions based on the provided context:",
        "a61e8222-1b12-4158-923c-993c56295e40": "Here are some questions based on the context information:",
        "a8ddab24-0a6e-4cfa-8225-4f6f6375da6d": "Here are five questions based on the provided context information:",
        "0ca15ac0-cc91-4643-aac0-d38ff306350f": "Here are 5 questions based on the provided context information:",
        "07e65184-1fc6-49e2-80b2-83548e81f934": "Here's a potential question that can be asked based on the provided context:",
        "008b4397-063e-40a6-977f-10ebf7a7e8f3": "Here are five questions based on the provided context:",
        "1529f05c-8f00-4e3a-bbbd-fda29c9b1a4c": "Here are five questions based on the provided context information:",
        "2d9cdc22-517d-46d4-a6b3-2871bacbc1ad": "Based on the provided context information, here's a question that can be generated for the upcoming quiz/examination:",
        "20fbc071-69eb-4bfc-a1d1-40d866137c53": "Here are some potential questions that you could ask students based on the provided context:",
        "b904f6de-0bbc-464d-8f50-2bae1cef09b6": "Here are five questions based on the provided context information:",
        "90605e6b-91b2-4e4b-b46e-6e55be2baabc": "Here are five potential questions that can be asked based on the context information provided:",
        "46f17f13-ad87-4c1b-9791-1d08d03399b8": "Here are five questions based on the provided context information:",
        "c5d6a0a1-604b-46aa-a894-9abc9b40af9a": "Here are five questions based on the context information:",
        "203b7343-6ceb-4ef3-89f7-74872c975951": "Here are five questions based on the provided context information:",
        "785c58a2-34d5-4c4e-9819-ecea5c87ad95": "Based on the context information, here's a generated question:",
        "9e8ee018-115e-45a0-8631-e7a51d609640": "Here's a question that tests understanding of the Deeplake types:",
        "8158a3e7-5418-4123-9a91-409316e982bc": "Here are some potential questions based on the given context:",
        "1830e125-9862-415b-9a91-d86f7c1acc12": "Here's a question that covers various aspects of the context information:",
        "96cb3f78-6dc7-4294-95c2-799baa55dc24": "Here are some potential questions that can be asked based on the provided context:",
        "411e1e2b-e659-49b2-841a-10b63dbb9cd5": "Here's a question that covers a variety of topics:",
        "8640fb7f-040e-4c59-9489-8e1acff61eae": "Based on the context information, I can help you generate some questions that would be suitable for a quiz or examination. Here are five questions that cover different aspects of the text:",
        "5489ef74-f3ad-4f27-ab1d-548f2d442140": "Here are five questions that cover different aspects of the provided context:",
        "48365491-db28-44eb-bdeb-ca6f30536d6e": "Here's a question that can be asked to test understanding of the Deep Lake database:",
        "cd95ceb2-4b66-4dc8-907e-a523402c8c1d": "Here are five questions based on the provided context information:",
        "0c82fd92-4ec1-4bbc-a2c5-d8acd757db10": "Here are some potential questions that can be asked based on the given context:",
        "4c6093e2-67c5-4e2a-88b9-90afbf1574d5": "Here are five questions that can be used for a quiz/examination based on the provided context information:",
        "6ce50ec8-fd00-4877-a9a7-159eba0bf446": "Here are 5 questions based on the provided context information:",
        "0994b35c-324d-451e-9c1d-a50408599536": "Here are five diverse questions based on the provided context information:",
        "61b7ed59-96e0-4dfc-8bd3-fe22c6294251": "Here are some questions based on the provided context information:",
        "d4b6b2f8-70cc-4d7a-bafd-35ee239a8c56": "Here are five questions based on the context information:",
        "a5d51fde-59be-47bc-b852-c0e682936052": "Here are five questions based on the provided context information:",
        "42b94693-3b62-4322-a4b8-bdf757198bf5": "Based on the context information, here's a question that covers a diverse range of topics:",
        "6fd71488-3765-430d-8750-e51d1d1d925b": "Here are five questions that can be asked based on the provided context information:",
        "f898fda8-c9eb-4f8a-a13d-2e624b8d0dc7": "Here are some potential quiz questions based on the provided context:",
        "554d4ba2-7fb4-49df-99af-46323291079f": "Here's a question that tests the student's understanding of the context information:",
        "0ccb54d9-af81-4161-871b-a703ac23b195": "Here are five questions based on the provided context information:",
        "2771be83-7f77-468d-8c5d-107f3bd37615": "Here are some potential questions that can be derived from the provided context:",
        "f63b7593-f91b-446d-a052-a02cc2391ab7": "Here's a possible question that can be asked based on the given context:",
        "50b0cd84-d6cd-4d15-b585-93208f637c3c": "Here are five questions based on the provided context information:",
        "af93fc28-e866-4ad0-bce4-7a036b3a34ae": "Here's a question that covers various aspects of the context information:",
        "22ebd979-05a2-403e-8fa1-c23225fa1b1e": "Here are five potential questions based on the provided context:",
        "6e849a0e-b6e1-4f3f-8f94-21f3b8c92efc": "Here's a question based on the provided context:",
        "807b1ca0-b144-45d2-8ec1-ae54181eba52": "Here's a question that can be asked based on the given context:",
        "aeecc384-d0db-4adc-9111-4ce0f094cd95": "Here are five questions based on the provided context information:",
        "6940a10c-441d-4b3a-a447-fe4d2a94739a": "Here are five diverse questions based on the provided context information:",
        "5cc6f171-c63b-41f0-9dd6-1305267f2857": "Here are some potential questions that can be generated based on the provided context:",
        "e389d19f-d3bf-4fe5-8d64-54ce06290d05": "Here's a question that assesses the student's understanding of the provided context:",
        "6f201a9d-f203-4317-860d-57e8adb58af2": "Here are five questions based on the provided context information:",
        "711a9052-8063-43a4-8390-73f9b64c8eb6": "Here are five diverse questions that can be asked based on the provided context information:",
        "b07e9ca9-9639-403f-bfbc-889627d758f5": "Here are some potential questions that can be asked based on the provided context information:",
        "60f25c3b-865c-4a97-a597-5292033aaf89": "Here's a question that covers a diverse range of topics from the provided context:",
        "2fef89cd-fc94-489c-99e1-86715965baf6": "Here's a possible question that can be asked based on the provided context:",
        "1ba9ea02-5956-4df5-850f-eb3076d641a5": "Here are some potential questions that can be asked based on the provided context:",
        "fd7afb55-15b2-4b73-9ea4-74e199667994": "Based on the provided context, here's a potential question that can be generated:",
        "e8d8f54a-c3b7-47a0-8dc7-e7999ad0d912": "Here are some potential questions based on the context information:",
        "1089b003-926c-4a3c-954d-c8517fce1c0d": "Here are five questions based on the provided context information:",
        "4f564b38-0fe7-4342-868d-8e85b1bc04cd": "Here's a question setup for an upcoming quiz/examination based on the provided context information:",
        "fe60a9ad-d6ef-4aba-bc24-10da3d5e7864": "Here are five questions based on the provided context:",
        "67e4a5dd-d8b7-4298-9821-34bc64c4e097": "Here are some potential questions that can be asked based on the provided context information:",
        "b0091aa8-39f3-4ce5-9c7c-3a328125e7c5": "Here are five diverse questions based on the provided context information:",
        "49a7cec3-263e-4ea0-9b9b-7ee9365cd5cc": "Here are some potential questions that can be generated based on the provided context information:",
        "c7c8cd5e-bc01-421c-a540-ab4008045f80": "Here's a question that can be formed based on the given context:",
        "096ef874-6f31-4cba-8248-255ddd0a088f": "Here are five questions based on the provided context information:",
        "3bd14a0d-2295-4f66-a6fe-e9ceecbb58ec": "Here are five questions based on the provided context information:",
        "608cefa6-9203-45c1-b755-517b421e4dd1": "Here are five questions based on the provided context:",
        "05874434-34f6-4d99-8fa8-65eb85fc24be": "Here are five questions that can be asked based on the provided context:",
        "38436ba7-c7c3-4144-9cd3-e78a64f2c292": "Here are five potential questions based on the given context:",
        "e2d1be18-d49e-44b0-afad-09543c227297": "Based on the provided context information, here's a question that covers diversity and can be used for an examination:",
        "43387e42-b0ea-4a47-a11c-d24cc02eda96": "Here are five questions based on the context information:",
        "ec1a5f01-99bc-449a-9b60-ad8a02aa9a5d": "Here are five questions that can be generated from the given context:",
        "6a20718f-2d48-4f91-ac79-578a9a1b32b8": "Here's a potential question that can be asked based on the given context:",
        "c37da5ab-4c34-46cb-bb59-78f7c295e0fa": "Here are some potential questions that can be asked based on the given context information:",
        "7e8e0c55-a642-4a97-bd44-24bcb9e27aa8": "Here are five diverse questions based on the provided context information:",
        "6e6b6059-4500-47a3-8309-d276a4e72671": "Here are five questions based on the provided context:",
        "2bb8fd59-978c-423c-a99d-2c9e5f290ed4": "Here are five questions based on the context information:",
        "8144a67f-fb01-4df4-8a35-0af16d5f3192": "Here are some potential questions that could be asked based on the given context:",
        "d5c4e059-8ed9-482b-a86c-52bf0d7bf05d": "Here are five questions based on the provided context information:",
        "d9f87f6f-42ec-49a8-95bc-72f1204476cf": "Here are five potential questions that could be asked on the quiz based on the provided context:",
        "c82093ca-6f05-4018-a2c9-3b3f8ce51d8c": "Here are five questions based on the provided context information:",
        "87a863e5-6d6f-4ba7-ab3d-57eaef5f5c6a": "Here are some potential quiz questions based on the context information:",
        "442c1906-a4a2-4169-832b-6830657b1784": "Here are five questions that can be asked based on the provided context:",
        "e458496b-69ca-4505-b804-c89329258c27": "Here's a question that can be generated based on the context:",
        "646ce726-4a1a-4ad7-87ae-b20deef683cb": "Here are five questions that can be asked based on the provided context information:",
        "1a189cbf-85e0-4eed-8d70-72ffda3958ae": "Here are some potential questions that can be asked based on the provided context:",
        "d54f08a8-681c-4843-983d-9ae6fa5ae68a": "Here are five questions that can be asked based on the given context:",
        "66fb1368-5532-49c4-b32d-ed12c4ca9e8f": "Here are some potential questions that can be asked based on the provided context:",
        "ba09e090-a7c5-405a-991c-d9f8dd61d3da": "Here's a question that covers various aspects of the Deep Lake database:",
        "e2842bb7-2e3a-4320-a71a-17abd6e4753a": "Here are five questions that can be used for the upcoming quiz/examination based on the provided context information:",
        "e94fad44-3b49-4d9e-be64-420b1ebe4adc": "Here are five questions based on the provided context information:",
        "4ed20af8-66d2-42b5-8459-0a90daa27802": "Here are five questions based on the provided context information:",
        "3d0af99a-5102-4447-b7d7-8cfc3c62b3d9": "Here are five questions based on the provided context information:",
        "c1c758eb-b55f-4b5b-b508-7c1240c8cdff": "Here are some questions based on the context information:",
        "55381596-1c2d-4f25-aa26-b90b3ceffb5a": "Here's a possible question that can be formed based on the given context information:",
        "bac45f0d-7031-4d78-8c17-2a54e0b4f000": "Here are five questions based on the context information provided:",
        "a8f3f022-e16c-4aae-97e9-7997aaa9abc7": "Here are five questions that can be asked based on the given context:",
        "f65b10f5-bf0f-4218-ae90-255079ce83c3": "Here are some potential questions that can be asked based on the provided context:",
        "23142ac0-4a16-46d9-b6b5-ddcb73c7462b": "Here are some potential questions that can be asked based on the provided context information:",
        "ae8bf696-509d-4f31-845b-b6a32584bf0f": "Here are five questions based on the provided context:",
        "672bd33f-cb79-457d-a2f0-0d7e988f962e": "Here are five questions based on the context information:",
        "a71a33a6-8270-4d2b-8ce8-09e57806987e": "Here are five questions that can be derived from the given context information:",
        "707c31e4-608f-4a0e-ac2f-923044ce1508": "Here's a question that covers various aspects of the documentation:",
        "92c9e722-254d-4bdc-9f67-e1b5d7830fb0": "Here are some questions based on the given context:",
        "143ef2b2-68ab-4070-ba55-a12c76a7a2e0": "Here are five questions that can be asked based on the provided context:",
        "86097e99-6975-4444-83d6-94f7ca4cdf0b": "Here are five questions based on the provided context information:",
        "99de4feb-4184-445a-9dd0-dd341ec6d116": "Here are five questions based on the provided context information:",
        "26c3f436-45df-4fe0-a10b-e4d7b6180b79": "Here are some questions based on the given context:",
        "3f464795-4cdb-44ab-b5a3-ff2fcaa4fc53": "Here are five questions based on the context information:",
        "b2bef92f-884d-4e94-90b8-09222f6722af": "Here are five questions based on the provided context information:",
        "da587849-e4d4-4117-9dee-965d4d48a112": "Here's a generated question based on the context:",
        "27e0454c-8aba-4304-836c-cfbec2fd8cce": "Here are some potential questions based on the context information:",
        "9f0dce07-ba35-4b8a-acc6-c462c391197c": "Here are some potential questions that can be asked based on the context information:",
        "f883e5c1-b1ce-4ae8-bba8-ad66c4a22106": "Here's a possible question that can be generated based on the given context:",
        "15f8f5aa-e7a1-44db-ab0c-300493df28b4": "Here are five questions that can be asked based on the given context:",
        "2cf7ccb0-83a4-4b6d-8333-79fa5d3c48ed": "Here are five questions based on the provided context:",
        "0f2e235a-972a-4cc2-8d14-46e5fb2a2b5c": "Here are some possible questions that can be asked based on the given context information:",
        "096c4e99-1980-40bb-871f-a2d7b5441d13": "Here are some questions that can be asked based on the context information:",
        "3a0ff086-76c9-4bcb-93a7-1428c647770a": "Here's a question that covers a diverse range of topics from the given context:",
        "1381de24-4994-482a-8482-cc98826c4aa5": "Here are some potential questions that can be asked based on the given context:",
        "f39b2604-bcf9-47fd-ae4f-4dfa5349fb74": "Here are some potential questions based on the context information:",
        "c0fb563a-71b6-44b4-be75-245387066cca": "Here's a question that can be generated from the given context:",
        "b870f5d9-7415-4ae1-a1bb-cbd17adc6390": "Here are five questions based on the provided context information:",
        "23fa3883-7d1e-4047-b23d-66aeb73b607f": "Here's a question based on the context information:",
        "8f036f5a-1e2e-4073-acf8-ff49266dc94f": "Here are five potential questions that can be asked based on the given context:",
        "5d96d9a5-636f-47b6-a098-f590b6ef33d1": "Here are five questions based on the context information:",
        "2b3cdbb1-9942-4b2d-948e-98008a1d8363": "Here's a question that can be asked based on the provided context:",
        "d1aca9d9-585e-41f6-95aa-10e69daaa0bb": "Here are five questions based on the provided context:",
        "fabcf0ae-3c29-4f03-b4be-daaf77089e05": "Here are five questions based on the provided context information:",
        "1dc7c0bc-12f6-4e3c-ba59-bdbdedc2530d": "Here's a possible question that can be generated based on the provided context:",
        "c5f43a5f-5eac-459c-8377-e282f33b07b8": "Here are five questions based on the provided context information:",
        "6d19bdba-b684-4dbc-95ec-fce5adb1bc4b": "Here's a question that covers various aspects of the ColPali model:",
        "5e801d22-b7ca-43fc-ae9e-187d16114ff5": "Here are some questions based on the context information:",
        "fc2bc1d3-685f-40dc-8a2d-3ef483ca02e0": "Here's a question that can be formed based on the given text:",
        "d41570eb-eb80-46e6-9d7d-f66a402b716b": "Here's a possible question that could be asked based on the context information:",
        "7d49ab7f-73a1-4ba0-88b8-f0463229b209": "Here are some potential questions you could ask based on the provided context information:",
        "c94d5ea6-9e56-4420-94f5-eaf7d98653fb": "Here are five questions based on the provided context information:",
        "8b8b2015-f3c0-4110-9888-04dec4f14256": "Here are five questions based on the provided context information:",
        "4a71b35a-5616-45b3-89ff-9ec3ac5d77f8": "Here are some potential questions that can be derived from the context information:",
        "b08d2c8e-64a0-4043-81ea-8283ffc62897": "Here are five questions based on the context information:",
        "b7af0128-2bcf-4824-a205-54c0cb28d9c0": "Here are five questions based on the provided context:",
        "0e8956c5-6b05-45b9-94c1-7675dda99faf": "Here are five questions based on the given context:",
        "602d80cc-0b03-48bf-b6f1-61bf895e4c71": "Here are some questions based on the provided context information:",
        "dbfb339a-8d3c-422d-9735-5a889fb76a72": "Here are five questions based on the provided context information:",
        "f2151a1d-ee79-43db-bf46-41ad209a82eb": "Based on the provided context, here's a question that can be generated:",
        "17caab14-33d0-4954-9521-069e4ee87fe9": "Here are five questions based on the provided context:",
        "4b7c11f1-db1a-4f4f-83db-ead19db48fc9": "Here are five questions that can be used for an upcoming quiz/examination based on the provided context information:",
        "76d8e554-a1dd-45cb-81c0-b28c60dd6fbe": "Here are five questions that can be generated based on the provided context:",
        "1764d90b-95ae-4b78-bc61-331b485d0e53": "Here are five questions based on the provided context information:",
        "d27832f9-f2b6-44cb-8003-1eab725d9a8b": "Here's a question that can be asked to test understanding of the ColPali model:",
        "59a526ad-3daa-4a34-bf0e-0985be8ec738": "Here are five questions based on the context information:",
        "151f3a52-7ed6-42e0-9585-ae188b8c4faa": "Here are five questions that can be used for the quiz/examination based on the provided context:",
        "45de60a2-f5d0-4fd2-b1f8-ee0dab67a589": "Here are five diverse questions based on the provided context information:",
        "8b7e8d45-e808-4cb6-bd87-f73d6e81fd3b": "Here are five diverse questions based on the provided context information:",
        "d82a387f-7c3b-4fe7-acb2-6b43cdcc56a8": "Based on the given context, here's a possible question that can be generated:",
        "175f3a96-c3b0-4733-8136-391446ac5e47": "Based on the provided context, here's a question that can be generated:",
        "a40f45e9-21bb-45d1-8727-e7076343e4a5": "Here are five diverse questions based on the provided context information:",
        "f841497e-49d7-4aaf-87e2-703ed4cf3ebe": "Here's a question that covers a variety of topics from the given context:",
        "4bca71a9-2a8b-419e-8005-a0a3525b3bd9": "Here are five questions based on the provided context information:",
        "9009bee9-d14d-45c7-a163-a2f46b9c88b7": "Here are some questions based on the context information:",
        "6f3dcc07-dcf5-44af-a024-f19ab7a62453": "Here are some potential questions based on the provided context:",
        "f7e32ce4-1da0-4b92-81a6-2e88c9fb7f8e": "Here are five questions based on the context information:",
        "1525d828-b470-4df6-829b-dc429371581d": "Here are five diverse questions based on the provided context information:",
        "363c9944-cb72-4e62-a69b-dd59e4d5be9e": "Here's a question that can be asked based on the given context:",
        "100af6a7-03fb-41f4-ab57-823772205c2a": "Here are five questions that can be generated based on the provided context:",
        "8292fdb7-329d-499a-88d7-a37dfadb2b40": "Here's a question based on the context:",
        "c97f5148-3be9-43b9-b873-12652579337d": "Here are some potential questions based on the provided context:",
        "22a704ce-d766-4fd5-82d5-54e7ffb20ba1": "Here are some potential questions based on the context information:",
        "afc71ec7-389d-4701-a022-2a38ce41a9c1": "Here are five questions based on the provided context information:",
        "4a56b1e5-fc3d-4efe-8b43-f4c3a84aba9e": "Here are five questions that can be asked based on the given context:",
        "c4f58c10-7fe3-4596-b94f-b3d58cc1da60": "Based on the provided context, here's a question that can be generated:",
        "bff1c2f6-bd0d-454b-b291-b6dbfe29cab3": "Here are five potential questions based on the given context:",
        "f7c49b02-0793-4777-856f-30a312dcc1d3": "Here are five diverse questions based on the provided context information:",
        "52695421-7042-469c-a028-51bb8c6780d2": "Here are five questions based on the context information:",
        "e19e8dd2-088c-4b39-a0ac-beafb78a14eb": "Here are some potential questions that can be generated based on the context information:",
        "72a102ef-480a-4947-8a79-320bef748b03": "Here are five questions that can be asked based on the context information:",
        "fcc3681b-2340-4441-9491-dd84d5726626": "Here are five questions that can be asked based on the given context:",
        "ee4c7fc9-009e-4a00-b7d9-2a7ca0a4f995": "Here are five questions based on the context information:",
        "9a8d1f6e-fc97-4f30-acb2-d78fcce89b73": "Based on the provided context, here's a question that can be generated:",
        "d07235fe-4d8c-4c6e-9cca-7d02d50baebb": "Here are five questions based on the provided context information:",
        "7829f183-cf30-4bab-b70a-d56b5c06e19d": "Here are five questions that can be asked based on the provided context information:",
        "27a2d622-a98d-4705-890c-1fde49cf0877": "Here are some potential quiz questions based on the provided context information:",
        "f7cde762-76a5-4744-a18c-fcf5e6a4e6d3": "Here's a question that covers multiple aspects of the context:",
        "095bb659-9c92-4fbb-81af-5bd2770a0006": "Here are five questions that can be generated based on the provided context:",
        "8ae7c066-1e8e-4cf4-8759-52cc467b46f0": "Here are some potential questions based on the provided context:",
        "7680a7f4-dc5e-488d-9d23-ff0e570d3a60": "Here are some questions based on the provided context information:",
        "1e9c4b0c-7510-4f41-910d-7dee83dde03e": "Here's a question based on the context information:",
        "4e2534f2-c5da-4503-8a8d-e849958e9bb0": "Based on the context information, here's a possible question that can be asked:",
        "2243a3f7-1f4b-4fa4-945d-8963b79c906c": "Here are some questions that can be generated based on the provided context:",
        "caff1f62-d690-4b06-8ab8-4859e3607fb9": "Here are some potential questions based on the context information:",
        "b42ba1ea-faf0-4659-973c-0d8f3e364f72": "Here are five potential questions that can be asked based on the given context information:",
        "e9788fe3-4969-4bac-96d0-9b3b0fb46e2e": "Here are five questions that can be asked based on the provided context:",
        "c2bde0ac-9319-49aa-857b-df0a90589523": "Here are five questions based on the provided context information:",
        "75a7d22f-6a5a-4f4f-bda7-63bf4ffc5c66": "Here are some potential questions based on the provided context:",
        "ae663efd-9c2a-4f83-aec7-ed6200fe20ab": "Here are some potential questions that can be asked based on the provided context information:",
        "721f007b-0827-4d7d-8f28-d6f4187ff6e3": "Here are some potential questions that can be asked based on the given context:",
        "0a849e78-4e9f-491b-a757-113e5132171d": "Here are some potential questions that can be generated based on the provided context information:",
        "e24aa1d6-8a36-48b6-91ae-c9686e3b5f4a": "Here are five questions that can be used for an upcoming quiz/examination based on the provided context:",
        "77057bd8-6bb3-4755-8ac6-1a13755a2b7f": "Here's a question that can be asked based on the context information:",
        "b93eabcc-a058-4d07-b325-56a384a8e19f": "Here are five questions that you could ask your students based on the context information:",
        "a34fcefb-4ba6-43db-8804-6514e71533be": "Here are some potential questions based on the given context:",
        "4347f00e-f843-496e-a56e-32f2c71ae6d8": "Here are five questions based on the context information:",
        "1510b271-bdd8-4fd3-b10e-b81230671707": "Here's a question that covers different aspects of the documentation:",
        "489ff7a2-7a9b-4814-9e4e-c45dc78e11d6": "Here are five questions that cover different aspects of the provided context information:",
        "d4ad3b6a-2959-4778-b7c2-b215fdb0b74a": "Here are five questions based on the provided context:",
        "bccab39c-b77b-4ad8-a042-20471786ef88": "Here are five questions that can be generated based on the provided context information:",
        "b2b6381a-7a34-46b2-99e2-be78e9593000": "Here are some potential questions based on the provided context:",
        "04f6f868-29a5-495c-9315-dae249148e40": "Here are five questions that can be asked based on the provided context:",
        "536e15e3-8223-4f06-a925-d2c31183a7c0": "Here are five questions based on the provided context information:",
        "202854ec-86df-49fe-9b24-e5f51bd14fd8": "Here are five questions based on the context information:",
        "3a319d2c-4952-4f61-a49d-10c37b5ad9a8": "Here are five questions based on the provided context:",
        "36b73d2a-7ea1-42b3-85c7-01915a8845a1": "Here are five questions based on the provided context information:",
        "501c1ad9-6593-4350-9ca9-4b1f2750ad24": "Here are five potential questions that can be asked based on the provided context information:",
        "c789a0f7-67b0-4f4b-9185-37c88ec8c117": "Here are five questions based on the given context:",
        "7034607b-719d-47d7-85ea-03b7e893fc69": "Here are some potential questions that can be generated based on the provided context information:",
        "fbabff75-2cc5-410e-893f-7d0143c2152b": "Based on the context information, here's how you can generate questions using only the provided query and context.",
        "405d3779-990a-4b42-ab11-553058c1bdf9": "Here are five questions based on the provided context information:",
        "f59ec565-394f-4e6d-acb1-935ac1041700": "Here's a possible question that can be generated based on the given context:",
        "0028e2ca-cd3a-49a2-89a2-4bcc2ee65711": "Here are five different question options based on the provided context information:",
        "9c28cb17-de67-4067-9827-4f9e10d9b27e": "Here's a possible question that can be generated based on the provided context:",
        "1ed8b85e-9265-4346-872e-18f10473dbec": "Here are some potential questions that can be generated based on the provided context:",
        "821f6235-c87f-47ea-870f-216154e7cdad": "Here are some potential questions that could be asked based on the given context information:",
        "408c2365-508e-4389-8402-d61e4bab9f43": "Here are five questions based on the provided context information:",
        "90e77d24-bdcb-4516-9bc9-48b7be9c37d2": "Here are five questions based on the given context information:",
        "9d8fc254-de74-4a42-bb06-e41a60044fb0": "Based on the context information, here's a possible question that can be generated:",
        "6f623aa1-64cd-4746-a402-3fdeecaadcd3": "Here's how you can implement the `generate_VQA` function according to the given specifications:",
        "caea8ab7-e1b9-45f2-b785-4f701ac66388": "Here are some potential quiz questions based on the provided context:",
        "733ff232-4b3a-4123-afe1-5c404ca4ab85": "Here are five questions based on the context information:",
        "f8fe5e0f-f30d-4fa2-a881-389245d85592": "Here's a question that tests the understanding of the Deep Lake documentation:",
        "31d31c1c-4c8a-433b-86bf-f226cf74151a": "Here are five questions that can be formed based on the provided context:",
        "fd69d20c-ebbc-48e2-ad96-4903e1bc5ef8": "Here are some potential questions that can be asked based on the provided context:",
        "603e0ed8-476a-48b8-8d05-30c14cbefae9": "Here are five questions that can be generated based on the provided context information:",
        "0fbbfd91-6af4-491e-b9a6-b4264e1d3798": "Here's a question that can be asked based on the given context:",
        "38402f33-a582-46bb-a84d-29d2e14518d0": "Here are five diverse questions based on the context information:",
        "f290ce16-2013-47f4-932a-c1a0e8987d84": "Here are five potential questions that can be asked based on the context information:",
        "4c0d16d7-8004-441f-bc5e-35db271d768f": "Here are five questions based on the given context information:",
        "85546611-a298-4de6-a36c-00347876029e": "Here's a potential question that can be asked based on the given context:",
        "45b870c1-1a57-47de-abea-dfcd07cc7774": "Here are five potential questions that cover different aspects of the document:",
        "66c6e72a-0f2c-47cd-a1cd-6d4034790331": "Here's a potential question that could be asked based on the given context information:",
        "eb9c9d69-94f1-48d0-9ee5-d95f1bff4adb": "Here are some potential questions based on the provided context:",
        "80a7917e-24e8-43fb-a7e4-07eaae2459f1": "Here are five questions based on the provided context:",
        "75a9eec5-3e85-40fa-a63d-b854a55536a7": "Here are some potential quiz questions based on the provided context:",
        "8916f31e-d5a4-4391-a62d-f71296056593": "Here are five questions based on the provided context information:",
        "501aa46e-7289-4882-a441-12bb1b0cdbca": "Here are some questions based on the given context:",
        "88295cde-c862-42ec-a6e8-48d23d2f103a": "Here are five diverse questions based on the provided context:",
        "ff831781-0363-41f0-9a48-dfdc933e2278": "Here's a question that covers a diverse range of topics from the given context:",
        "95c0321e-c72e-4258-a7e0-3ba00e1a3bdf": "Here's how you can use the function to generate a question based on the given query:",
        "02a41428-e5f2-4cea-ad1b-7f4378530d09": "Here's a question that can be generated based on the context information:",
        "b6548a5c-1de0-4919-9e03-be81858e04d4": "Here are some potential questions based on the provided context:",
        "96d2e64f-ed7b-4f7a-ba32-dbdc98a5d29f": "Here are five questions based on the provided context:",
        "c440a139-30ba-4052-851f-d35a15240689": "Based on the provided documents, here's a question that covers diversity in nature:",
        "8372f0e1-1882-424f-bd91-2504196da41b": "Here are five questions based on the provided context:",
        "63e54eb1-59c1-4250-beb5-b3082874f9ea": "Here are five potential questions based on the provided context information:",
        "46a067f2-86c4-4334-a3e8-a355ed624118": "Here are five questions based on the provided context information:",
        "2e0e53af-c6da-4e87-831b-ff0447dead2d": "Here are five questions based on the provided context information:",
        "b74515f3-7d64-46e8-aee5-4458a87c8629": "Here are five potential questions based on the provided context:",
        "3691566f-2474-4213-a820-04edd615f460": "Here are five diverse questions based on the context information:",
        "2328f33a-4a6e-44aa-b55e-0aa1ecbdc168": "Here's a question that can be asked based on the provided context:",
        "9954c15f-a5b2-478a-a22b-d0b8ebc687a9": "Here's a possible question that can be formed based on the given context:",
        "a6bacf5a-32fa-4587-a959-083fa2e6451f": "Here are five questions based on the context information:",
        "117bc711-d219-4ad3-8639-82333b2448a4": "Here are five questions based on the context information:",
        "11ac046d-2169-4280-bc96-8bf595d13371": "Here are five questions based on the provided context:",
        "0a6d482f-d166-4b29-9363-69a4b48f61a9": "Here's a question that covers multiple aspects of the documentation:",
        "2dddc65b-669b-49b7-ace4-72ade5b587a5": "Here are five questions that can be asked based on the given context information:",
        "61410a75-02b3-458d-aac9-ab6ef0084fc4": "Here are five questions that cover diverse topics from the given context information:",
        "bbc6b839-1982-4855-974a-3466ff92c21b": "Here are five questions based on the context information:",
        "0b1b3f1e-f6d5-4502-84f4-3dd60907aa80": "Here are five questions that can be used for an upcoming quiz/examination based on the provided context:",
        "bf383667-54fa-468b-9556-75d6c4251027": "Here's a question that covers a diverse range of topics from the provided context:",
        "7a7e76c6-27f7-4312-a706-6125f0df172c": "Here are some potential questions that could be asked based on the provided context:",
        "489afe5c-34bb-426c-807b-15a93984c089": "Here are five questions based on the provided context information:",
        "30e28d31-b37c-49cb-a79a-8b31112e2a9b": "Here are some potential questions based on the provided context:",
        "23dd1ca6-9e59-4008-8c68-fd2bd5f311ad": "Here are some potential questions based on the provided context information:",
        "38dd61f2-3aaa-4d6f-bc0f-73ed4a33c173": "Based on the context information, here's a question that can be generated:",
        "c8a26e75-56ff-41a9-baa2-ca76e58ae89f": "Here's a possible question that can be asked based on the provided context:",
        "ee49b661-68da-42f3-8817-35677954994c": "Here are five questions based on the context information:",
        "6fcc61b0-b86f-4915-bbbb-0ff39939ed33": "Here's a question that covers different aspects of the documentation:",
        "d010ba14-1509-42eb-9475-0c8cde258194": "Here are some potential questions that can be asked based on the given context:",
        "349bb2cc-9301-4028-85ed-15d0eabf2213": "Here are five potential questions that could be asked based on the given context information:",
        "5c4d6304-25af-496c-992a-e3d7d8cd5986": "Here are five questions based on the provided context information:",
        "5dd7de3b-5675-49f5-a58c-667e7d83af21": "Here are five questions based on the context information:",
        "71702e39-3845-4305-94c3-9ae929d35d53": "Here are five questions that can be generated from the context information:",
        "5544a4b5-5214-409a-99dd-893090d75bc1": "Here are five questions based on the provided context:",
        "b7bd123a-e0be-45c0-9d8a-9607417a6627": "Here are five questions based on the given context information:",
        "2763f044-c6a1-41f2-a619-d8e9a7ecfadc": "Here are some questions that can be asked based on the provided context information:",
        "f8ebdd42-d381-42f1-8cbe-1cbb5148ce50": "Here are five potential questions based on the provided context:",
        "3db278d2-332f-42a7-8d52-97849a100db5": "Here are some potential questions that can be generated based on the given context:",
        "5621f5f1-1955-474c-ab26-80d3dca4b030": "Here's a question that tests understanding of the given context:",
        "f7d5927f-ec2f-4a77-b31f-280e00606398": "Here are 5 diverse questions based on the context information:",
        "d28ca814-47b0-4269-8d67-8d24307ac181": "Here's a question based on the context information:",
        "9b30cd4a-8adf-4d5b-8194-93efe88fb4cf": "Here's a question based on the context information:",
        "474fdb5d-bb71-4553-bd72-102bfeb8beed": "Here are five questions based on the provided context:",
        "43dfb027-3b4f-4452-906c-43f900375965": "Here are five questions based on the provided context information:",
        "142df164-1261-42b7-9c92-21e9db8cea2c": "Here are five diverse questions based on the context information:",
        "821ee8c2-b785-40d5-8a27-f35d16bc644b": "Here are five potential questions that can be generated based on the provided context:",
        "272c1b52-1faa-4b8a-8c9e-b73b9e953c5e": "Here are five questions based on the provided context information:",
        "86dd27f1-f575-452e-a81a-449aa851c291": "Here are five questions based on the provided context information:",
        "6fc7accf-bccf-4c7f-959d-ac3a48003a9f": "Here are five questions based on the context information:",
        "4f8ec1c8-4266-428b-8618-7cd771a3a4e6": "Here are five questions based on the provided context:",
        "4bfb2868-7fbc-4744-aa76-6f33fb42afad": "Here are five questions based on the provided context information:",
        "4e08043a-bf2d-499f-9506-05fc9c75421b": "Here are some potential questions that can be asked based on the given context:",
        "1d699e5c-c5e9-4317-96be-f11c39041aa6": "Here are five questions based on the context information:",
        "3e5cbd3d-fd69-4f75-aced-2d769d8a6551": "Here are some potential questions that can be asked based on the provided context:",
        "cba49c3d-e90f-45bb-9352-471000bf748b": "Here are five potential questions that can be asked based on the provided context information:",
        "ada89592-ddf1-4725-8e99-5fdb99bbcf59": "Here are five questions based on the provided context information:",
        "eb9ceba5-12f8-4eb1-952b-b0dfa475d39e": "Here are five questions based on the context information:",
        "dee83bb3-6c1e-493f-ad62-c1b52469dd8d": "Here are five questions based on the context information:",
        "6bca4ecf-0d0b-46ba-9592-f7ab13594e35": "Here are five questions that can be asked based on the provided context:",
        "56d94236-a737-49e2-be2d-4ed832e396e2": "Here are five questions based on the provided context:",
        "7bbd2855-3c1e-4213-b148-21fb8709e324": "Here are some potential questions based on the provided context information:",
        "ab636cd2-d97c-488a-8bae-521fb6670de9": "Here are some questions that can be generated based on the given context information:",
        "141c96ec-4cd9-4582-b9dc-9376b1b0e858": "Here's a possible question:",
        "c90d7446-652d-4db3-ae53-3e734c7d7058": "Here's a question that tests understanding of the context:",
        "a7bc20c0-de85-4734-a1a5-236da4473165": "Here are five questions that can be formed from the given context:",
        "1d00e05d-d467-4180-bc07-06af6ae9a3b2": "Here's a question that can be asked based on the given context:",
        "4c6ccf84-5f36-4f8e-a8e6-e5cf4b13e163": "Here are some potential questions you could ask based on the provided context:",
        "bf981461-c2fe-42fb-8cf3-c7f7948ff2f6": "Here are five diverse questions based on the provided context information:",
        "c1e04431-37b3-4a08-802d-7db862050481": "Here's a question that tests understanding of the given code:",
        "ff2ee5c1-6ec1-4944-b317-316168aba33e": "Based on the provided context, here's a potential question that covers a variety of aspects:",
        "4353b30a-b35d-4b13-8388-580079876ccd": "Here are five potential questions based on the given context:",
        "2b8e97f9-6254-45d5-baab-35cc7154deff": "Here are some question options for your upcoming quiz/examination based on the provided context information:",
        "de127110-e84b-4862-9f78-313e933afb9e": "Here are 5 questions based on the provided context:",
        "a83b36df-3b5b-4ac6-b0a4-d48a9a854453": "Here are five questions based on the provided context information:",
        "07db0c84-74da-4a4f-ad8d-e89d9fb06444": "Here are five diverse questions based on the given context:",
        "5d726516-e522-4c3c-94d0-03d047898fad": "Here are five potential questions that can be asked based on the provided context information:",
        "87ac07d3-3aa8-4282-ad57-ac116126d484": "Here's a possible question that can be generated based on the context information:",
        "d1a4dc2e-4377-4782-9f18-45408106aa1d": "Here are five questions based on the provided context information:",
        "843bd722-9bc8-4e4d-886e-79daf009e64e": "Here are five questions based on the context information:",
        "023b50d2-fe4a-4287-a26f-533bb97b168c": "Here's a question that covers various aspects of the text:",
        "d77c8dfa-d5d3-4bb3-b5c4-1fdf03aa63f4": "Here are five questions that can be generated based on the provided context:",
        "60df6eb2-851f-4764-ab14-f0cf22c1e307": "Here are five questions based on the provided context:",
        "453a1fa1-06b3-4f5c-ae25-178765d4098f": "Here's a question based on the context information:",
        "157d84b5-a9b8-4a01-ad63-01c4c4cc9b96": "Here are some potential questions that could be asked based on the given context information:",
        "cc087a00-dc37-43e2-a587-443629ca53e8": "Based on the given context information, here's a possible question that can be generated:",
        "aaccffc4-6c57-4f74-aa49-4cef8977fa47": "Here are five questions based on the provided context information:",
        "daac09ce-7f60-4355-9cbb-ae9b51ec5e62": "Here are five questions based on the provided context information:",
        "3d957d2f-5954-4afe-a336-21d73059a7b4": "Here are five questions based on the provided context:",
        "317168ea-6565-4e26-b3bb-b68f572164d7": "Here are five questions based on the provided context information:",
        "fdf9e079-76ba-4e02-8581-be910fb7f0ee": "Here are some potential questions that could be asked on the quiz/examination based on the provided context:",
        "660e33d6-1165-4e64-93ab-b77fc0c9a824": "Here's a question that can be asked based on the context information:",
        "5c5045e9-ba06-4c95-acb8-88c0d7076977": "Here are five questions based on the context information:",
        "ccdad3c3-6616-41e8-bfb1-b06d13a2774f": "Here are some potential questions based on the context information:",
        "988fd7cd-b0d0-4709-9312-1005e8401760": "Here are five questions that cover different aspects of the provided context information:",
        "dfe8f3e9-722f-48a8-9e50-d722551f1092": "Here are five questions based on the context information:",
        "0a82e564-6a6b-44a6-8404-81669d657422": "Here's a question that covers various aspects of the context information:",
        "d0c3b659-2576-4b58-8993-d778db068d8e": "Here are some potential questions based on the given context:",
        "4963477f-b6fd-47bd-a81d-2571334dd57d": "Based on the context information, here's a generated question:",
        "6c8fee5f-3229-4b50-8324-30a72e1a3f26": "Here are five questions based on the given context information:",
        "931e3f07-24a8-41cb-b120-f162d8004946": "Here's a question that covers various aspects of the text:",
        "b3092744-0dbb-42cf-8c55-05d5109c8d74": "Here are some questions based on the provided context:",
        "1e344f4f-12c0-47f9-bca9-b36654fb8678": "Here's a potential question based on the provided context:",
        "e21882ee-65ab-4c72-bddb-d91a43473d9f": "Here are some potential questions that can be asked based on the given context:",
        "2475ff94-7594-4984-af71-e888b944c784": "Here are some potential questions that can be asked based on the given context information:",
        "2ae00bc1-0bd4-4c5a-bf8e-12949242e2bb": "Here are some potential questions based on the given context information:",
        "f64fff76-91cc-4434-9687-25ba0d88537d": "Here are five questions that can be asked based on the context information:",
        "4cea187e-0103-4b99-a19a-b30042a890c2": "Here are some questions based on the provided context:",
        "d4a4c1a5-e088-4345-afcc-d3ea8a5873ca": "Here are five questions based on the provided context information:",
        "1a7b9607-ac7f-45e0-b668-28a8a92cd992": "Here are some potential questions that can be asked based on the provided context:",
        "6d9ae381-bfad-41e2-a4f1-02951e6f29e9": "Here are some potential questions that can be asked based on the provided context:",
        "86b729c1-f62f-4e2c-846f-7612979df0fd": "Here's a question that can be derived from the given context:",
        "f313e035-0e3c-45bb-9751-0da3a6bb20b1": "Here are some potential questions based on the provided context:",
        "5132719e-a445-4e66-9dcb-7f5673780074": "Here are five potential questions that can be asked based on the provided context information:",
        "0b2bfe9f-a14e-4b94-a6f2-76a982a3626b": "Here are five questions that can be asked based on the provided context information:",
        "fabfbb41-0afe-4e9e-9627-126a7590c171": "Here are five questions based on the given context information:",
        "e46f900a-0aca-480d-b243-f31507b017d3": "Here are five questions that can be asked based on the provided context information:",
        "3d3c323e-f235-4b6c-b20e-37c1665a2bdc": "Here's a question that covers multiple aspects of the document:",
        "f8d2188b-bc18-45ba-9576-6f8ac017a33d": "Here are some potential questions based on the provided context information:",
        "b1019398-c335-4729-88c3-76ae770adede": "Here's a question based on the provided context:",
        "763a3cf8-7e5a-4bf5-871b-f47a42cc7bff": "Here are five questions based on the provided context information:",
        "3b06ad53-a086-4b39-971d-52ae51f34f41": "Here are five diverse questions based on the provided context information:",
        "7ed94873-74d4-4d55-802c-6a3f898f3984": "Here are five potential questions that can be derived from the provided context information:",
        "245102a9-4365-4200-bca0-1b2b13c3718a": "Here are 5 questions based on the provided context information:",
        "c08a0406-db6e-4fba-951e-ac9931ab798c": "Here are some potential questions based on the provided context:",
        "823c65ce-19c5-4ee5-81aa-96117dfd98f0": "Here are some potential questions that can be asked based on the provided context information:",
        "a87fe19a-6aec-41f2-9e97-d10d0fca5fed": "Here are some potential quiz questions based on the provided context:",
        "8558fe16-2d48-4bfe-a7b7-403d1b2e24f5": "Here's a generated question based on the provided context:",
        "59267e35-8670-4d3b-a6d3-81a90712d5e0": "Here are some questions that can be generated based on the given context information:",
        "c62284f6-7233-4305-bbc3-ff14b13fb209": "Here are five questions based on the context information:",
        "7b1438ed-43be-4a56-a23d-9baafe0c55c7": "Here are five questions based on the provided context:",
        "0314b4d6-b81c-4505-a1bf-45cbd921e96b": "Here are five questions based on the given context:",
        "d9430fb0-6f8d-4146-b8cd-3f9db29ab2af": "Here's a question that can be asked based on the context information:",
        "776020e8-dc1f-4814-83f3-b0e7a4801840": "Here are five potential questions based on the provided context information:",
        "d940f088-5fe3-4727-a92b-4a3366ac09a7": "Here are some potential questions based on the context information:",
        "4dbfeacc-37ca-49ad-80ac-7a572f16cdd6": "Based on the provided context information, here's a question that can be asked:",
        "0b67b601-6e1f-4b38-a963-17d9c57f7a04": "Based on the given context, I'd like to create a question that assesses understanding and critical thinking skills.",
        "79e5b999-0e35-47f9-aa6e-8a4426ed7611": "Based on the context, I can generate questions that would be suitable for an upcoming quiz/examination based on the table-based question answering task. Here are five diverse questions:",
        "ea27271b-69db-4956-860f-5b3a529deebe": "Here are some potential questions that can be generated based on the provided context:",
        "df9981f9-e9d1-4ad1-86b4-5402e6815cfe": "Here's a question that can be asked in the context of setting up a dataset with BM25 index:",
        "c87e839f-bbba-4110-a250-cb1ab09e949b": "Here are five questions based on the provided context information, covering different aspects of the topic:",
        "d435a10a-b99f-403f-9e0a-fc2c5141668e": "Here are some potential questions based on the provided context:",
        "9972e665-a441-4fa6-9b50-00c18f01215f": "Here are five questions based on the provided context information:",
        "203e710e-3857-40dd-b7d6-a4e62e25ed5a": "Here's a question that can be generated based on the context information:",
        "2e2dc69a-5d85-479d-9117-e405c5c5d54d": "Here are five questions that can be asked based on the provided context information:",
        "093dd74c-7a30-42da-8167-d2e146f23b94": "Here are five questions based on the provided context information:",
        "a2022dc6-7d2f-499d-acb4-8672c8422989": "Here are five questions based on the provided context information:",
        "54ea2c71-440b-4de4-a9da-4926170f19d7": "Here are five questions based on the provided context information:",
        "f35af8a0-b5c4-4a12-b871-60368b993e32": "Here are five questions based on the provided context information:",
        "62b5f3b7-be31-46ac-a2be-78d313264b75": "Here are five diverse questions based on the provided context information:",
        "ffb7b5a1-4d71-44e5-bc4e-59957957ff18": "Here are some potential questions that can be derived from the given context:",
        "6d7db650-23b1-4269-98ef-6dc3925aa8c7": "Here are some potential questions that can be asked based on the given context:",
        "f9b323f2-c738-4159-909a-08691ccb0a13": "Here's a possible question that can be asked based on the given context:",
        "698c7a35-274a-4dce-969a-8721e95afe83": "Here's a question that can be generated based on the given context:",
        "2cebcdfa-b8e0-4349-86b8-68554a245a55": "Here are five diverse questions based on the provided context information:",
        "818c7367-3215-474b-b2bb-8e36f24c555e": "Here's a question that can be asked to test understanding of async/await and non-blocking checks.",
        "be5155f5-fb34-4e6b-83c6-0c8d18ba7da8": "Here are some potential quiz questions based on the provided context:",
        "4d9b79e8-b682-47a3-bf13-268db9d3e66c": "Here are five diverse questions based on the provided context:",
        "3d5b3f79-23d0-4025-940a-1eacd5a12512": "Here are five questions based on the provided context information:",
        "0d50b93b-ebb5-4efe-88a4-006c23f68991": "Based on the context information, here are five potential questions that could cover diverse aspects of the text:",
        "3d5ba4a5-df4b-40d7-82d5-7f9b482b8ed8": "Here are five questions based on the provided context information:",
        "c7e0b77c-0495-49ff-b3d9-d62ca752cddd": "Here are five questions based on the context information:",
        "eb7f1cd1-a5c3-4974-92d2-7def6fa42763": "Here are some potential questions that can be asked based on the given context:",
        "ca3a9bd3-d42e-4e53-879e-8841861c8cf0": "Here are five questions that can be asked based on the provided context:",
        "2094a3de-efa8-4a54-b429-19e45cd3d035": "Here are five questions that can be derived from the provided context information:",
        "b52c3f64-9b01-4f02-87a8-d8c1c1abb558": "Here are some potential questions that can be formed based on the given context:",
        "79321473-bd14-4b6d-85f2-0c1f8f5981b4": "Here are five diverse questions based on the provided context:",
        "ffd891c8-86a9-4498-87be-610891becbed": "Here are some potential questions that can be asked based on the context information:",
        "260deea8-40e8-4990-ad10-9ba036fcac9d": "Here are five questions based on the provided context information:",
        "15d16beb-728c-46ad-97af-0975ba4038a0": "Here's a question that can be asked based on the given context:",
        "b291ca08-91f0-4908-97e6-3c49cd94f79d": "Here are some questions based on the provided context information:",
        "cf6c58ff-07e6-44af-825e-606665d47cac": "Here's a potential question that can be generated based on the provided context:",
        "82e7614f-499a-45af-9f20-529cc29c7f5c": "Here are five questions based on the context information:",
        "51bee377-a3af-4588-9b81-be364e227a1b": "Here are five questions based on the context information:",
        "1d375278-a498-4e82-857b-bf7282a9e49b": "Here are five questions that can be asked based on the given context:",
        "17da4412-7d93-4677-971a-2ab7f82475fd": "Here are some potential questions based on the provided context information:",
        "bd2f66a9-bc61-46e2-a9ba-41346ee6fec2": "Here are five questions based on the context information:",
        "8295cc69-4c87-41ce-b4d4-590714242aed": "Here are five diverse questions based on the provided context information:",
        "d7828361-e1e4-4ffb-9a1a-0b0f6918100a": "Here's a question that covers multiple aspects of the context information:",
        "1921b802-dc3a-4910-8d17-e13fbfd30dd4": "Here are five diverse questions based on the given context information:",
        "c3eda931-423e-4cad-8ba9-fb80a288120b": "Here are five questions that cover various aspects of the context information:",
        "f751aa91-a960-4e80-a7a9-19869a5c7694": "Here are some potential questions that can be asked based on the provided context:",
        "69953bcf-62ee-42bc-a3d8-892898de1072": "Here are five questions based on the provided context information:",
        "e3244299-254b-47f1-9a13-a17412434d00": "Here's a question that can be asked based on the given context:",
        "513b4ce3-020d-4adc-bdb6-b02db3a4bb18": "Here are five questions based on the provided context information:",
        "954f980f-e59f-42cf-8eba-c5846f45597a": "Here are five questions that can be asked based on the provided context:",
        "1b8b6cbe-b8d8-4e8d-8be1-ce61016ddb1c": "Here are some possible questions that can be generated based on the provided context:",
        "be34069e-748f-4742-b06a-96d3a5f46f13": "Here are five questions based on the context information:",
        "f310e93f-686a-4db8-939b-fe064341d510": "Here are some questions based on the provided context:",
        "3722f3dc-3558-4773-b5e5-b5c7679915b7": "Here's a question that covers various aspects of the document:",
        "7ee9d7d2-f43a-47f0-8d04-089dcd6c831a": "Here are five questions based on the provided context:",
        "48f45117-f77d-4611-806a-567063f18558": "Here's a question that covers a diverse range of topics from the given context:",
        "5c6e2b0e-e4e5-439c-992a-067b0199f3f2": "Here are five questions based on the provided context:",
        "6ea9dc6b-6f91-41a4-a23c-2d00e5485c37": "Based on the context information, here's a question that can be generated:",
        "83279bae-431f-42a7-a7e8-353ae987fb0e": "Here are five questions that can be asked based on the provided context information:",
        "091b6ca6-9799-4cac-bf4e-5b7160faa9e8": "Here are five questions based on the provided context information:",
        "f1c1feef-e840-4bf5-b775-e8bb059293d9": "Here are some potential quiz questions based on the given context:"
    },
    "corpus": {
        "node_366": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_592": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_412": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_632": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_154": "1. **Set Device** :\n\n     * We define `device` to use GPU if available, otherwise defaulting to CPU, ensuring compatibility across hardware.\n  2. **Load CLIP Model** :\n\n     * We load the CLIP model (`ViT-B/32`) with its associated preprocessing steps using `clip.load()`. This model is optimized for multi-modal tasks and is set to run on the specified `device`.\n\nThis setup allows us to efficiently process images for embedding, supporting\nmulti-modal applications like image-text similarity.\n\nThe following image illustrates the `CLIP` (Contrastive Language-Image\nPretraining) model's structure, which aligns text and images in a shared\nembedding space, enabling cross-modal understanding.\n\n    \n    \n    import torch\n    import clip\n    \n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n    \n\n### Create the embedding function for images\u00b6\n\nTo prepare images for embedding generation, we define a transformation\npipeline and a function to process images in batches.\n\n  1. **Define Transformations (`tform`)**:\n\n     * The transformation pipeline includes:\n       * **Resize** : Scales images to 224x224 pixels.\n       * **ToTensor** : Converts images to tensor format.\n       * **Lambda** : Ensures grayscale images are replicated across three channels to match the RGB format.\n       * **Normalize** : Standardizes pixel values based on common RGB means and standard deviations.\n  2. **Define`embedding_function_images`**:\n\n     * This function generates embeddings for a list of image.\n     * If `images` is a single filename, it's converted to a list.\n     * **Batch Processing** : Images are processed in batches (default size 4), with transformations applied to each image. The batch is then loaded to the device.\n     * **Embedding Creation** : The model encodes each batch into embeddings, stored in the `embeddings` list, which is returned as a single list.",
        "node_214": "* Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM  Segmentation with MM  Table of contents \n        * Integration Interface \n          * Example Configuration with Deep Lake \n        * Prerequisites \n        * Setup \n        * Configuration \n        * Training \n        * Deep Lake Integration Benefits \n        * Monitoring Training \n        * Inference \n          * Key Integration Parameters \n        * Common Issues and Solutions \n          * Custom Loss Functions \n          * Multiple Optimization Strategies \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Integration Interface \n    * Example Configuration with Deep Lake \n  * Prerequisites \n  * Setup \n  * Configuration \n  * Training \n  * Deep Lake Integration Benefits \n  * Monitoring Training \n  * Inference \n    * Key Integration Parameters \n  * Common Issues and Solutions \n    * Custom Loss Functions \n    * Multiple Optimization Strategies \n\n# Semantic Segmentation with Deep",
        "node_362": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_110": "* Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Load the Data from Deep Lake \n  * 1) Create the Dataset and Use an Inverted Index for Filtering \n    * Extract the data \n    * Add the data to the dataset \n    * Search for the restaurant using a specific word \n    * Show the results \n  * 2) Create the Dataset and use BM25 to Retrieve the Data \n    * Add data to the dataset \n    * Search for the restaurant using a specific sentence \n    * Show the results \n  * 3) Create the Dataset and use Vector Similarity Search \n    * Create the dataset and add the columns \n    * Search for the restaurant using a specific sentence \n  * 4) Explore Results with Hybrid Search \n    * Search for the correct restaurant using a specific sentence \n    * Show the scores \n    * Normalize the score \n    * Fusion method \n    * Show the results \n    * Let's run a search on a multiple dataset \n    * Comparison of Sync vs Async Query Performance \n  * 5) Integrating Image Embeddings for Multi-Modal Search \n    * Create the embedding function for images \n    * Create a new dataset to save the images \n    * Convert the URLs into images \n    * Search similar images \n    * Performing a similar image search based on a specific image \n    * Show similar images and the their respective restaurants \n  * 6) ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT \n  * 7) Discover Restaurants Using ColPali and the Late Interaction Mechanism \n    * Download",
        "node_291": "query(f\"\"\"\n        SELECT *\n        FROM \"s3://bucket/embeddings\"\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Text Search\u00b6\n\nText search using BM25 or keyword matching:\n\n    \n    \n    # Semantic search using BM25\n    results = deeplake.query(\"\"\"\n        SELECT *\n        FROM \"s3://bucket/documents\"\n        ORDER BY BM25_SIMILARITY(text, 'search query') DESC\n        LIMIT 10\n    \"\"\")\n    \n    # Keyword search using CONTAINS\n    results = deeplake.query(\"\"\"\n        SELECT *\n        FROM \"s3://bucket/metadata\"\n        WHERE CONTAINS(keywords, 'specific term')\n    \"\"\")\n    \n\n## Array Operations\u00b6\n\nOperate on multidimensional arrays:\n\n    \n    \n    # Select specific array dimensions\n    results = deeplake.query(\"\"\"\n        SELECT features[:, 0:10]\n        FROM \"s3://bucket/features\"\n    \"\"\")\n    \n    # Filter by array values\n    results = deeplake.query(\"\"\"\n        SELECT *\n        FROM \"s3://bucket/features\"\n        WHERE features[0] > 0.5\n    \"\"\")\n    \n    # Aggregate array operations\n    results = deeplake.query(\"\"\"\n        SELECT AVG(features, axis=0)\n        FROM \"s3://bucket/features\"\n    \"\"\")\n    \n\n## Joining Datasets\u00b6\n\nJoin data across different datasets and across different clouds:\n\n    \n    \n    # Join datasets from different storage\n    results = deeplake.query(\"\"\"\n        SELECT i.image, i.embedding, m.labels, m.metadata\n        FROM \"s3://bucket1/images\" AS i\n        JOIN \"s3://bucket2/metadata\" AS m \n        ON i.id = m.image_id\n        WHERE m.verified = true\n    \"\"\")\n    \n    # Complex join with filtering\n    results = deeplake.query(\"\"\"\n        SELECT \n            i.image,\n            e.embedding,\n            l.label\n        FROM \"s3://bucket1/images\" AS i\n        JOIN \"gcs://bucket2/embeddings\" AS e ON i.id = e.",
        "node_64": "We do not host or distribute these datasets, vouch for their quality or\nfairness, or claim that you have a license to use the datasets. It is your\nresponsibility to determine whether you have permission to use the datasets\nunder their license.\n\nIf you're a dataset owner and do not want your dataset to be included in this\nlibrary, please get in touch through a GitHub issue. Thank you for your\ncontribution to the ML community!\n\n**Usage Tracking**\n\nBy default, we collect usage data using Bugout (here's the code that does it).\nIt does not collect user data other than anonymized IP address data, and it\nonly logs the Deep Lake library's own actions. This helps our team understand\nhow the tool is used and how to build features that matter to you! After you\nregister with Activeloop, data is no longer anonymous.",
        "node_500": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_184": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nUsing Deep Lake as a Vector Store in LangChain\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore  VectorStore  Table of contents \n      * How to Use Deep Lake as a Vector Store in LangChain \n      * Downloading and Preprocessing the Data \n      * A note on chunking text files \n      * Creating the Deep Lake Vector Store \n      * Use the Vector Store in a Q&A App \n      * Accessing the Low Level Deep Lake API (Advanced) \n      * SelfQueryRetriever with Deep Lake \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM",
        "node_371": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_350": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_119": "Next, we create a new dataset with the\nspecified name and location in Deep Lake storage.\n\nWe then add two columns to the dataset: `restaurant_name` and\n`restaurant_review`. Both columns use a BM25 index, which optimizes them for\nrelevance-based searches, enhancing the ability to rank results based on how\nwell they match search terms.\n\nFinally, we use `ds_bm25.commit()` to save these changes to the dataset and\n`ds_bm25.summary()` to display an overview of the dataset's structure and\ncontents.\n\nIf you don't have a token yet, you can sign up and then log in on the official\nActiveloop website, then click the `Create API token` button to obtain a new\nAPI token. Here, under `Select organization`, you can also find your\norganization ID(s).\n\n    \n    \n    import os, getpass\n    os.environ[\"ACTIVELOOP_TOKEN\"] = getpass.getpass(\"Activeloop API token: \")\n    \n    \n    \n    org_id = \"<your_org_id>\" \n    dataset_name_bm25 = \"bm25_test\"\n    \n    ds_bm25 = deeplake.create(f\"al://{org_id}/{dataset_name_bm25}\")\n    \n    \n    \n    # Add columns to the dataset\n    ds_bm25.add_column(\"restaurant_name\", types.Text(index_type=types.BM25))\n    ds_bm25.add_column(\"restaurant_review\", types.Text(index_type=types.BM25))\n    ds_bm25.add_column(\"owner_answer\", types.Text(index_type=types.BM25))\n    ds_bm25.commit()\n    ds_bm25.summary()\n    \n\nOutput:\n\n    \n    \n    Dataset(columns=(restaurant_name,restaurant_review,owner_answer), length=0)\n    +-----------------+-----------------+\n    |     column      |      type       |\n    +-----------------+-----------------+\n    | restaurant_name |text (bm25 Index)|\n    +-----------------+-----------------+\n    |restaurant_review|text (bm25 Index)|\n    +-----------------+-----------------+\n    |  owner_answer   |text (bm25 Index)|\n    +-----------------+-----------------+\n    \n\n### Add data to the dataset\u00b6\n\nWe add data to the `ds_bm25` dataset by appending the two columns, filled with\nvalues from the lists we previously created.",
        "node_428": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_300": "It is returned by the deeplake.ReadOnlyDataset.branches property.\n\n#####  `` __getitem__ \u00b6\n\n    \n    \n    __getitem__(name: str) -> BranchView\n    \n\nReturn a branch by name or id\n\n#####  `` __len__ \u00b6\n\n    \n    \n    __len__() -> int\n    \n\nThe number of branches in the dataset\n\n#####  `` names \u00b6\n\n    \n    \n    names() -> list[str]\n    \n\nReturn a list of branch names\n\n    \n    \n    # Access read-only branches\n    branches_view = ds.branches\n    \n    # List branch names\n    for name in branches_view.names():\n        print(f\"Found branch: {name}\")\n    \n    # Get specific branch\n    branch_view = branches_view[\"B1\"]\n    \n\n## Tagging\u00b6\n\n### Tag\u00b6\n\n####  `` deeplake.Tag \u00b6\n\nDescribes a tag within the dataset.\n\nTags are created using deeplake.Dataset.tag.\n\n#####  `` delete \u00b6\n\n    \n    \n    delete() -> None\n    \n\nDeletes the tag from the dataset\n\n#####  `` id `property` \u00b6\n\n    \n    \n    id: str\n    \n\nThe unique identifier of the tag\n\n#####  `` name `property` \u00b6\n\n    \n    \n    name: str\n    \n\nThe name of the tag\n\n#####  `` open \u00b6\n\n    \n    \n    open() -> DatasetView\n    \n\nFetches the dataset corresponding to the tag\n\n#####  `` open_async \u00b6\n\n    \n    \n    open_async() -> Future\n    \n\nAsynchronously fetches the dataset corresponding to the tag and returns a\nFuture object.\n\n#####  `` rename \u00b6\n\n    \n    \n    rename(new_name: str) -> None\n    \n\nRenames the tag within the dataset\n\n#####  `` version `property` \u00b6\n\n    \n    \n    version: str\n    \n\nThe version that has been tagged\n\n    \n    \n    # Create tag\n    ds.tag(\"v1.0\")\n    \n    # Access tagged version\n    tag = ds.tags[\"v1.0\"]\n    print(f\"Tag: {tag.name}\")\n    print(f\"Version: {tag.version}\")\n    \n    # Open dataset at tag\n    tagged_ds = tag.open()\n    \n    # Rename tag\n    tag.rename(\"v1.0.0\")\n    \n    # Delete tag\n    tag.delete()\n    \n\n### Tags\u00b6\n\n####  `` deeplake.Tags \u00b6\n\nProvides access to the tags within a dataset.",
        "node_550": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_427": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_42": "Skip to content\n\n## Navigation Menu\n\nToggle navigation\n\nSign in\n\n  * Product \n\n    * GitHub Copilot\n\nWrite better code with AI\n\n    * GitHub Advanced Security\n\nFind and fix vulnerabilities\n\n    * Actions\n\nAutomate any workflow\n\n    * Codespaces\n\nInstant dev environments\n\n    * Issues\n\nPlan and track work\n\n    * Code Review\n\nManage code changes\n\n    * Discussions\n\nCollaborate outside of code\n\n    * Code Search\n\nFind more, search less\n\nExplore\n\n    * All features \n    * Documentation \n    * GitHub Skills \n    * Blog \n\n  * Solutions \n\nBy company size\n\n    * Enterprises \n    * Small and medium teams \n    * Startups \n    * Nonprofits \n\nBy use case\n\n    * DevSecOps \n    * DevOps \n    * CI/CD \n    * View all use cases \n\nBy industry\n\n    * Healthcare \n    * Financial services \n    * Manufacturing \n    * Government \n    * View all industries \n\nView all solutions\n\n  * Resources \n\nTopics\n\n    * AI \n    * DevOps \n    * Security \n    * Software Development \n    * View all \n\nExplore\n\n    * Learning Pathways \n    * Events & Webinars \n    * Ebooks & Whitepapers \n    * Customer Stories \n    * Partners \n    * Executive Insights \n\n  * Open Source \n\n    * GitHub Sponsors\n\nFund open source developers\n\n    * The ReadME Project\n\nGitHub community articles\n\nRepositories\n\n    * Topics \n    * Trending \n    * Collections \n\n  * Enterprise \n\n    * Enterprise platform\n\nAI-powered developer platform\n\nAvailable add-ons\n\n    * GitHub Advanced Security\n\nEnterprise-grade security features\n\n    * Copilot for business\n\nEnterprise-grade AI features\n\n    * Premium Support\n\nEnterprise-grade 24/7 support\n\n  * Pricing\n\nSearch or jump to.",
        "node_75": "you should\nregister and login in the Deep Lake App.\n\n### Authentication in Programmatic Interfaces\u00b6\n\nYou can create an API token in the Deep Lake App and authenticate in\nprogramatic interfaces using 2 options:\n\n#### Environmental Variable\u00b6\n\nSet the environmental variable `ACTIVELOOP_TOKEN` to your API token. In\nPython, this can be done using:\n\n`os.environ['ACTIVELOOP_TOKEN'] = <your_token>`\n\n#### Pass the Token to Individual Methods\u00b6\n\nYou can pass your API token to individual methods that require authentication\nsuch as:\n\n`ds = deeplake.open('al://org_name/dataset_name', token = <your_token>)`\n\n## Next Steps\u00b6\n\nNow that you have a local dataset, you can learn more about:\n\n  * Activeloop Managed Credentials\n\nBack to top",
        "node_246": "Note\n\nNo data is copied.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`src` |  `DatasetView` |  The dataset to copy the structure from. |  _required_  \n`dest` |  `str` |  The URL to create the new dataset at. creds (dict, str, optional): The string `ENV` or a dictionary containing credentials used to access the dataset at the path.\n\n  * If 'aws_access_key_id', 'aws_secret_access_key', 'aws_session_token' are present, these take precedence over credentials present in the environment or in credentials file. Currently only works with s3 paths.\n  * It supports 'aws_access_key_id', 'aws_secret_access_key', 'aws_session_token', 'endpoint_url', 'aws_region', 'profile_name' as keys.\n  * To use credentials managed in your Activeloop organization, use they key 'creds_key': 'managed_key_name'. This requires the org_id dataset argument to be set.\n  * If nothing is given is, credentials are fetched from the environment variables. This is also the case when creds is not passed for cloud datasets\n\n|  _required_  \n`token` |  `str` |  Activeloop token, used for fetching credentials to the dataset at path if it is a Deep Lake dataset. This is optional, tokens are normally autogenerated. |  `None`  \n  \nExamples:\n\n    \n    \n    ds = deeplake.like(src=\"az://bucket/existing/to/dataset\",\n       dest=\"s3://bucket/new/dataset\")\n    \n\n## Dataset Class\u00b6\n\nThe main class providing full read-write access.\n\n###  `` deeplake.Dataset \u00b6\n\nBases: `DatasetView`\n\nDatasets are the primary data structure used in DeepLake. They are used to\nstore and manage data for searching, training, evaluation.\n\nUnlike deeplake.ReadOnlyDataset, instances of `Dataset` can be modified.",
        "node_271": "Examples:\n\n    \n    \n    # Async batch load\n    future = column.get_async(slice(0, 32))\n    batch = future.result()\n    \n    # Using with async/await\n    async def load_batch():\n        batch = await column.get_async(slice(0, 32))\n        return batch\n    \n\n####  `` metadata `property` \u00b6\n\n    \n    \n    metadata: ReadOnlyMetadata\n    \n\nAccess the column's metadata. Useful for storing statistics, preprocessing\nparameters, or other information about the column data.\n\nReturns:\n\nName | Type | Description  \n---|---|---  \n`ReadOnlyMetadata` |  `ReadOnlyMetadata` |  A ReadOnlyMetadata object for reading metadata.  \n  \nExamples:\n\n    \n    \n    # Access preprocessing parameters\n    mean = column.metadata[\"mean\"]\n    std = column.metadata[\"std\"]\n    \n    # Check available metadata\n    for key in column.metadata.keys():\n        print(f\"{key}: {column.metadata[key]}\")\n    \n\n####  `` name `property` \u00b6\n\n    \n    \n    name: str\n    \n\nGet the name of the column.\n\nReturns:\n\nName | Type | Description  \n---|---|---  \n`str` |  `str` |  The column name.",
        "node_789": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_549": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_223": "* **`deeplake_credentials`** : (Optional) Required for private, nonlocal datasets.\n    * **`deeplake_tag_id`** : (Optional) Specifies a dataset commit for reproducibility.\n    * **`deeplake_query`** : (Optional) Used to load datasets based on a query.\n    * **`deeplake_tensors`** : Maps MMSegmentation tensors to Deep Lake tensors:\n      * `\"img\"`: Image tensor.\n      * `\"gt_semantic_seg\"`: Semantic segmentation tensor.\n\n## Common Issues and Solutions\u00b6\n\n  1. Memory Issues:\n\n     * Reduce `samples_per_gpu` in config\n     * Decrease image size in pipeline\n     * Use smaller batch sizes\n  2. Performance Issues:\n\n     * Increase `num_workers` in `deeplake_dataloader`\n     * Enable distributed training\n     * Use proper GPU settings\n  3. Mask Format Issues:\n\n     * Verify mask format in dataset\n     * Check normalization settings\n     * Ensure proper padding configuration\n\n### Custom Loss Functions\u00b6\n\n    \n    \n    # Add custom loss function to decode head\n    config['model']['decode_head']['loss_decode'] = dict(\n        type='CrossEntropyLoss',\n        use_sigmoid=False,\n        loss_weight=1.0,\n        class_weight=[1.0] * 171  # Class weights for imbalanced datasets\n    )\n    \n\n### Multiple Optimization Strategies\u00b6\n\n    \n    \n    # Different learning rates for backbone and heads\n    config['optimizer'] = dict(\n        type='AdamW',\n        lr=0.0001,\n        paramwise_cfg=dict(\n            custom_keys={\n                'backbone': dict(lr_mult=0.1),\n                'decode_head': dict(lr_mult=1.0),\n                'auxiliary_head': dict(lr_mult=1.0)\n            }\n        )\n    )\n    \n\nBack to top",
        "node_344": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_258": "####  `` version `property` \u00b6\n\n    \n    \n    version: str\n    \n\nThe currently checked out version of the dataset\n\n####  `` history `property` \u00b6\n\n    \n    \n    history: History\n    \n\nThe history of the overall dataset configuration.\n\n####  `` schema `property` \u00b6\n\n    \n    \n    schema: SchemaView\n    \n\nThe schema of the dataset.\n\n## DatasetView Class\u00b6\n\nLightweight view returned by queries. Provides read-only access to query\nresults.\n\n###  `` deeplake.DatasetView \u00b6\n\nA DatasetView is a dataset-like structure. It has a defined schema and\ncontains data which can be queried.\n\n####  `` query \u00b6\n\n    \n    \n    query(query: str) -> DatasetView\n    \n\nExecutes the given TQL query against the dataset and return the results as a\ndeeplake.DatasetView.\n\nExamples:\n\n    \n    \n    result = ds.query(\"select * where category == 'active'\")\n    for row in result:\n        print(\"Id is: \", row[\"id\"])\n    \n\n####  `` query_async \u00b6\n\n    \n    \n    query_async(query: str) -> Future\n    \n\nAsynchronously executes the given TQL query against the dataset and return a\nfuture that will resolve into deeplake.DatasetView.\n\nExamples:\n\n    \n    \n    future = ds.query_async(\"select * where category == 'active'\")\n    result = future.result()\n    for row in result:\n        print(\"Id is: \", row[\"id\"])\n    \n    async def query_and_process():\n        # or use the Future in an await expression\n        future = ds.query_async(\"select * where category == 'active'\")\n        result = await future\n        for row in result:\n            print(\"Id is: \", row[\"id\"])\n    \n\n####  `` tag \u00b6\n\n    \n    \n    tag(name: str | None = None) -> Tag\n    \n\nSaves the current view as a tag to its source dataset and returns the tag.\n\n####  `` batches \u00b6\n\n    \n    \n    batches(\n        batch_size: int, drop_last: bool = False\n    ) -> Iterable\n    \n\nThe batches can be used to more efficiently stream large amounts of data from\na DeepLake dataset, such as to the DataLoader then to the training framework.",
        "node_118": "This approach provides a robust and scalable infrastructure that can grow\nalongside our projects, minimizing the need for frequent hardware upgrades and\nensuring efficient data management.\n\n## 2) Create the Dataset and use BM25 to Retrieve the Data\u00b6\n\nOur advanced `\"Index-On-The-Lake\"` technology enables sub-second query\nperformance directly from object storage, such as `S3`, using minimal compute\npower and memory resources. Achieve up to `10x greater cost efficiency`\ncompared to in-memory databases and `2x faster performance` than other object\nstorage solutions, all without requiring additional disk-based caching.\n\nWith Deep Lake, you benefit from rapid streaming columnar access to train deep\nlearning models directly, while also executing sub-second indexed queries for\nretrieval-augmented generation.\n\nIn this stage, the system uses BM25 for a straightforward lexical search. This\napproach is efficient for retrieving documents based on exact or partial\nkeyword matches.\n\nWe start by importing deeplake and setting up an organization ID `org_id` and\ndataset name `dataset_name_bm25`. Next, we create a new dataset with the\nspecified name and location in Deep Lake storage.\n\nWe then add two columns to the dataset: `restaurant_name` and\n`restaurant_review`. Both columns use a BM25 index, which optimizes them for\nrelevance-based searches, enhancing the ability to rank results based on how\nwell they match search terms.\n\nFinally, we use `ds_bm25.commit()` to save these changes to the dataset and\n`ds_bm25.summary()` to display an overview of the dataset's structure and\ncontents.\n\nIf you don't have a token yet, you can sign up and then log in on the official\nActiveloop website, then click the `Create API token` button to obtain a new\nAPI token. Here, under `Select organization`, you can also find your\norganization ID(s).",
        "node_192": "metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n        ),\n        Document(\n            page_content=\"Toys come alive and have a blast doing so\",\n            metadata={\"year\": 1995, \"genre\": \"animated\"},\n        ),\n        Document(\n            page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n            metadata={\n                \"year\": 1979,\n                \"rating\": 9.9,\n                \"director\": \"Andrei Tarkovsky\",\n                \"genre\": \"science fiction\",\n                \"rating\": 9.9,\n            },\n        ),\n    ]\n    \n\nSince this feature uses Deep Lake's Tensor Query Language under the hood, the\nVector Store must be stored in or connected to Deep Lake, which requires\nregistration with Activeloop:\n\n    \n    \n    org_id = <YOUR_ORG_ID>\n    dataset_path = f\"al://{org_id}/self_query\"\n    \n    vectorstore = DeeplakeVectorStore.from_documents(\n        docs, embeddings, dataset_path = dataset_path, overwrite = True,\n    )\n    \n\nNext, let's instantiate our retriever by providing information about the\nmetadata fields that our documents support and a short description of the\ndocument contents.",
        "node_629": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_787": "* Product\n  * Pricing\n  * Support\n  * Download Slack\n  * Create a new workspace\n  * Find your workspace\n  * Sign in\n  * Menu\n\n  * Product\n  * Pricing\n  * Support\n  * Download the Slack app\n\n  * Sign in\n  * Create a new workspace\n\n# We're very sorry, but your browser is not supported!\n\nPlease upgrade to a supported browser, or try one of our apps.\n\n## Desktop Apps\n\n### Mac\n\nSee system requirements\n\nv4.43.51\n\n### Windows\n\nSee system requirements\n\nv4.43.51\n\n### Linux\n\nSee system requirements\n\nv4.41.105\n\n## Mobile Apps\n\n### iOS\n\n### Android\n\nDon't see the platform you're looking for? Let us know.\n\n  * Using Slack\n  * Product\n  * Enterprise\n  * Pricing\n  * Support\n  * Slack Guides\n  * Slack Marketplace\n  * API\n\n  * Slack \n  * Jobs\n  * Customers\n  * Developers\n  * Events\n  * Blog\n\n  * Legal\n  * Privacy\n  * Security\n  * Terms of Service\n  * Policies\n\n  * Handy Links\n  * Download desktop app\n  * Download mobile app\n  * Brand Guidelines\n  * Slack at Work\n  * Status\n\n  * Contact Us\n  *   *",
        "node_401": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_496": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_358": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_561": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_46": "213 Commits  \n.github| .github|  |   \npython/deeplake| python/deeplake|  |   \n.gitignore| .gitignore|  |   \n.pre-commit-config.yaml| .pre-commit-config.yaml|  |   \nCONTRIBUTING.md| CONTRIBUTING.md|  |   \nLICENSE| LICENSE|  |   \nREADME.md| README.md|  |   \nSECURITY.md| SECURITY.md|  |   \nView all files  \n  \n## Repository files navigation\n\n  * README\n  * Apache-2.0 license\n  * Security\n\n  \n\n# Deep Lake: Database for AI\n\n###  **Docs** \u2022 **Get Started** \u2022 **API Reference** \u2022 **LangChain & VectorDBs\nCourse** \u2022 **Blog** \u2022 **Whitepaper** \u2022 **Slack** \u2022 **Twitter**\n\n## What is Deep Lake?\n\nDeep Lake is a Database for AI powered by a storage format optimized for deep-\nlearning applications. Deep Lake can be used for:\n\n  1. Storing and searching data plus vectors while building LLM applications\n  2. Managing datasets while training deep learning models\n\nDeep Lake simplifies the deployment of enterprise-grade LLM-based products by\noffering storage for all data types (embeddings, audio, text, videos, images,\ndicom, pdfs, annotations, and more), querying and vector search, data\nstreaming while training models at scale, data versioning and lineage, and\nintegrations with popular tools such as LangChain, LlamaIndex, Weights &\nBiases, and many more.",
        "node_527": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_602": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_567": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_105": "**If you already have a service account, skip to Step 2**\n\n2\\. Navigate to `IAM & Admin` -> `Service Accounts` -> `CREATE SERVICE\nACCOUNT`\n\n3\\. Enter the `service account id`, and optional `name` and `description`.\nMake sure to copy the email address and and click on `CREATE AND CONTINUE`.\n\n4\\. Click `CONTINUE` without entering any information.\n\n5\\. Enter `[email protected]` in the `Service account users role` and click\n`DONE`.\n\n#### Step 2: Grant Access to the bucket using a Service Account Principal\u00b6\n\n1\\. Navigate to `Cloud Storage` and `Buckets`.\n\n2\\. Select `Edit Access` for the bucket you want to connect to Activeloop.\n\n3\\. Select `Add Principal`.\n\n4\\. Enter the `Service Account Email`, select the role as `Storage Object\nAdmin`, and click `Save`. If the bucket is encrypted with customer managed KMS\nkey, then `Cloud KMS CryptoKey Encrypter/Decrypter` should be added in the\n`Role` field as well.\n\n#### Step 3: Enter the Service Account Email (Step 2) into the Activeloop App\u00b6\n\nSee the first video in the managed credentials overview\n\n## Next Steps\u00b6\n\n  * Enabling CORS in GCP\n\nBack to top",
        "node_555": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_345": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_339": "pull(\n        url=\"s3://source/dataset\",\n        creds={\"aws_access_key_id\": \"key\", \"aws_secret_access_key\": \"secret\"}\n    )\n    \n    # Pull changes asynchronously\n    async def pull_async():\n        await replica_ds.pull_async(\"s3://source/dataset\") \n    \n\n### Push Changes\u00b6\n\nPush local changes to another dataset location:\n\n    \n    \n    # Make changes to dataset\n    ds.append({\"images\": new_images})\n    ds.commit()\n    \n    # Push changes to replica\n    ds.push(\n        url=\"s3://replica/dataset\",\n        creds={\"aws_access_key_id\": \"key\", \"aws_secret_access_key\": \"secret\"}\n    )\n    \n    # Push changes asynchronously  \n    async def push_async():\n        await ds.push_async(\"s3://replica/dataset\")\n    \n\n## Synchronization Example\u00b6\n\n    \n    \n    # Initial dataset creation\n    source_ds = deeplake.create(\"s3://bucket/source\")\n    source_ds.add_column(\"images\", deeplake.types.Image())\n    source_ds.commit()\n    \n    # Create replica\n    deeplake.copy(\n        src=\"s3://bucket/source\",\n        dst=\"gcs://bucket/replica\"\n    )\n    replica_ds = deeplake.open(\"gcs://bucket/replica\")\n    \n    # Add data to source\n    source_ds.append({\"images\": batch1})\n    source_ds.commit()\n    \n    # Sync replica with source\n    replica_ds.pull(\"s3://bucket/source\")\n    \n    # Add data to replica  \n    replica_ds.append({\"images\": batch2})\n    replica_ds.commit()\n    \n    # Push replica changes back to source\n    replica_ds.push(\"s3://bucket/source\")\n    \n\n## Summary\u00b6\n\n  * Copying the dataset preserves all data, metadata, and version history\n  * Push/pull synchronizes only the changes between datasets\n  * Copy/sync works across different storage providers - s3, gcs, azure, local, etc.\n\nBack to top",
        "node_715": "The asynchronous query time is calculated as the difference between `end_async` and `start_async`, and is printed.\n\nThe code executes two queries both synchronously and asynchronously, measuring\nthe execution time for each method. In the synchronous part, the queries are\nexecuted one after the other, and the execution time is recorded. In the\nasynchronous part, the queries are run concurrently using `asyncio.gather()`\nto parallelize the asynchronous calls, and the execution time is also\nmeasured. The \"speed factor\" is then calculated by comparing the execution\ntimes, showing how much faster the asynchronous execution is compared to the\nsynchronous one. Using `asyncio.gather()` allows the asynchronous queries to\nrun in parallel, reducing the overall execution time.\n\n    \n    \n    import time\n    import asyncio\n    import nest_asyncio\n    \n    nest_asyncio.apply()\n    \n    async def run_async_queries():\n        # Use asyncio.gather to run queries concurrently\n        ds_async_results, ds_bm25_async_results = await asyncio.gather(\n            vector_search.query_async(tql_vs),\n            ds_bm25.query_async(tql_bm25)\n        )\n        return ds_async_results, ds_bm25_async_results\n    \n    # Measure synchronous execution time\n    start_sync = time.time()\n    ds_sync_results = vector_search.query(tql_vs)\n    ds_bm25_sync_results = ds_bm25.query(tql_bm25)\n    end_sync = time.time()\n    print(f\"Sync query time: {end_sync - start_sync}\")\n    \n    # Measure asynchronous execution time\n    start_async = time.time()\n    # Run the async queries concurrently using asyncio.gather\n    ds_async_results, ds_bm25_async_results = asyncio.run(run_async_queries())\n    end_async = time.time()\n    print(f\"Async query time: {end_async - start_async}\")\n    \n    sync_time = end_sync - start_sync\n    async_time = end_async - start_async\n    \n    # Calculate speed factor\n    speed_factor = sync_time / async_time\n    \n    # Print the result\n    print(f\"The async query is {speed_factor:.2f} times faster than the sync query.\")",
        "node_129": "Restaurant name: Chaat Bhavan Mountain View\n    Review: Great place with good food\n    Owner Answer: Thank you for your positive feedback! We're thrilled to hear that you had a great experience at our restaurant and enjoyed our delicious food. Your satisfaction is our priority, and we can't wait to welcome you back for another wonderful dining experience.\n    \n    Thanks,\n    Team Chaat Bhavan\n    Restaurant name: Chaat Bhavan Mountain View\n    Review: Good food.\n    Owner Answer: Thank you for your 4-star rating! We're glad to hear that you had a positive experience at our restaurant. Your feedback is valuable to us, and we appreciate your support. If there's anything specific we can improve upon to earn that extra star next time, please let us know. We look forward to serving you again soon.\n    \n    Thanks,\n    Team Chaat Bhavan\n    \n\n## 4) Explore Results with Hybrid Search\u00b6\n\nIn the stage, the system enhances its search capabilities by combining BM25\nwith Approximate Nearest Neighbors (ANN) for a hybrid search. This approach\nblends lexical search with semantic search, improving relevance by considering\nboth keywords and semantic meaning. The introduction of a Large Language Model\n(LLM) allows the system to generate text-based answers, delivering direct\nresponses instead of simply listing relevant documents.\n\nWe open the `vector_search` dataset to perform a hybrid search. First, we\ndefine a query `\"Let's grab a drink\"` and generate its embedding using\n`embedding_function(query)[0]`. We then convert this embedding into a comma-\nseparated string `embedding_string`, preparing it for use in combined text and\nvector-based searches.\n\n    \n    \n    vector_search = deeplake.open(f\"al://{org_id}/{dataset_name_vs}\")\n    \n\n### Search for the correct restaurant using a specific sentence\u00b6\n\n    \n    \n    query = \"I feel like a drink\"\n    embed_query = embedding_function(query)[0]\n    embedding_string = \",\".join(str(c) for c in embed_query)\n    \n\nWe create two queries:\n\n  1.",
        "node_573": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_778": "A full comparison\narticle can be found here.\n\n**Deep Lake vs HuggingFace** Deep Lake and HuggingFace offer access to popular\ndatasets, but Deep Lake primarily focuses on computer vision, whereas\nHuggingFace focuses on natural language processing. HuggingFace Transforms and\nother computational tools for NLP are not analogous to features offered by\nDeep Lake.  **Deep Lake vs WebDatasets** Deep Lake and WebDatasets both offer\nrapid data streaming across networks. They have nearly identical steaming\nspeeds because the underlying network requests and data structures are very\nsimilar. However, Deep Lake offers superior random access and shuffling, its\nsimple API is in python instead of command-line, and Deep Lake enables simple\nindexing and modification of the dataset without having to recreate it.\n**Deep Lake vs Zarr** Deep Lake and Zarr both offer storage of data as chunked\narrays. However, Deep Lake is primarily designed for returning data as arrays\nusing a simple API, rather than actually storing raw arrays (even though\nthat's also possible). Deep Lake stores data in use-case-optimized formats,\nsuch as jpeg or png for images, or mp4 for video, and the user treats the data\nas if it's an array, because Deep Lake handles all the data processing in\nbetween. Deep Lake offers more flexibility for storing arrays with dynamic\nshape (ragged tensors), and it provides several features that are not naively\navailable in Zarr such as version control, data streaming, and connecting data\nto ML Frameworks.",
        "node_641": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_106": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nAdvancing Search Capabilities: From Lexical to Multi-Modal with Deep Lake\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG  RAG  Table of contents \n      * Load the Data from Deep Lake \n      * 1) Create the Dataset and Use an Inverted Index for Filtering \n        * Extract the data \n        * Add the data to the dataset \n        * Search for the restaurant using a specific word \n        * Show the results \n      * 2) Create the Dataset and use BM25 to Retrieve the Data \n        * Add data to the dataset \n        * Search for the restaurant using a specific sentence \n        * Show the results",
        "node_451": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_116": "You can find the official documentation\nhere.\n\n    \n    \n    word = 'burritos'\n    view = ds.query(f\"\"\"\n        SELECT * \n        WHERE CONTAINS(restaurant_review, '{word}')\n        LIMIT 4\n    \"\"\")\n    print(view)\n    \n\nOutput:\n\n    \n    \n    Dataset(columns=(restaurant_name,restaurant_review,owner_answer), length=4)\n    \n\n### Show the results\u00b6\n\n    \n    \n    for row in view:\n        print(f\"Restaurant name: {row['restaurant_name']} \\nReview: {row['restaurant_review']}\")\n    \n\nOutput:\n\n    \n    \n    Restaurant name: Los Amigos\n    Review: Best Burritos i have ever tried!!!!! Wolderful!!!\n    Restaurant name: Los Amigos\n    Review: Really good breakfast burrito, and just burritos in general\n    Restaurant name: Los Amigos\n    Review: Ordered two of their veggie burritos, nothing crazy just added extra cheese and sour cream. They even repeated the order back to me and everything was fine, then when I picked the burritos up and got home they put zucchini and squash in it.. like what??\n    Restaurant name: Los Amigos\n    Review: Don't make my mistake and over order. The portions are monstrous. The wet burritos are as big as a football.\n    \n\nAI data retrieval systems today face 3 challenges: `limited modalities`, `lack\nof accuracy`, and `high costs at scale`. Deep Lake 4.0 fixes this by enabling\ntrue multi-modality, enhancing accuracy, and reducing query costs by 2x with\nindex-on-the-lake technology.\n\nConsider a scenario where we store all our data locally on a computer.\nInitially, this may be adequate, but as the volume of data grows, managing it\nbecomes increasingly challenging. The computer's storage becomes limited, data\naccess slows, and sharing information with others is less efficient.\n\nTo address these challenges, we can transition our data storage to the cloud\nusing Deep Lake. Designed specifically for handling large-scale datasets and\nAI workloads, Deep Lake enables up to 10 times faster data access.",
        "node_149": "information = [f'Review: {el[\"restaurant_review\"]}, Restaurant name: {el[\"restaurant_name\"]}' for el in view_vs]\n    result = generate_question(query, information)\n    print(result)\n    \n\nOutput:\n\n    \n    \n    \"If you're feeling like a drink, consider visiting Taqueria La Espuela, which is known for its refreshing horchata. Alternatively, you might enjoy Chaat Bhavan Mountain View, a great place with good food and a lively atmosphere.\"\n    \n\n### Let's run a search on a multiple dataset\u00b6\n\nIn this approach, we perform the hybrid search across two separate datasets:\n`vector_search` for vector-based search results and `ds_bm25` for BM25-based\ntext search results. This allows us to independently query and retrieve scores\nfrom each dataset, then combine them using the same fusion method as before.\n\n    \n    \n    ds_bm25 = deeplake.open(f\"al://{org_id}/{dataset_name_bm25}\")\n    vs_results = vector_search.query(tql_vs)\n    bm25_results = ds_bm25.query(tql_bm25)\n    \n    \n    \n    vs_score = vs_results[\"score\"]\n    bm_score = bm25_results[\"score\"]\n    \n    vss = softmax(vs_score)\n    bm25s = softmax(bm_score)\n    \n    \n    \n    docs_vs = {}\n    docs_bm25 = {}\n    for el, score in zip(vs_results, vss):\n        docs_vs[str(el[\"row_id\"])] = Document(id=str(el[\"row_id\"]), data={\"restaurant_name\": el[\"restaurant_name\"], \"restaurant_review\": el[\"restaurant_review\"]}, score=score)\n    \n    for el, score in zip(bm25_results, bm25s):\n        docs_bm25[str(el[\"row_id\"])] = Document(id=str(el[\"row_id\"]), data={\"restaurant_name\": el[\"restaurant_name\"], \"restaurant_review\": el[\"restaurant_review\"]}, score=score)\n    \n    \n    \n    results = fusion(docs_vs, docs_bm25)\n    \n    \n    \n    for v in sorted_documents.values():\n        print(f\"Restaurant name: {v.data['restaurant_name']} \\nReview: {v.data['restaurant_review']}\")\n    \n\nOutput:\n\n    \n    \n    Restaurant name: Olympus Caffe & Bakery\n    Review: I like the garden to sit down with friends and have a drink.",
        "node_283": "Parameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`dtype` |  `DataType | str` |  The datatype of values (default float32) |  `'float32'`  \n`format` |  `str | None` |  The bounding box format. Possible values: `ccwh`, `ltwh`, `ltrb`, `unknown` |  `None`  \n`bbox_type` |  `str | None` |  The pixel type. Possible values: `pixel`, `fractional` |  `None`  \n  \nExamples:\n\n    \n    \n    ds.add_column(\"col1\", types.BoundingBox())\n    ds.add_column(\"col2\", types.BoundingBox(format=\"ltwh\"))\n    \n    \n    \n    # Basic bounding boxes\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    \n    # With specific format\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox(\n        format=\"ltwh\"  # left, top, width, height\n    ))\n    \n\n##  `` deeplake.types.Point \u00b6\n\n    \n    \n    Point() -> Type\n    \n\nPoint datatype for storing 2D points with ability to visualize them.\n\nExamples:\n\n    \n    \n    ds.add_column(\"col1\", types.Point())\n    ds.append([{\"col1\": [[1.0, 2.0], [0.0, 1.0]]}])\n    \n\n##  `` deeplake.types.Medical \u00b6\n\n    \n    \n    Medical(compression: str) -> Type\n    \n\nMedical datatype for storing medical images.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`compression` |  `str` |  How to compress each row's value. Possible values: dcm, nii, nii.gz |  _required_  \n  \nExamples:\n\n    \n    \n    ds.add_column(\"col1\", types.Medical(compression=\"dcm\"))\n    \n    with open(\"path/to/dicom/file.dcm\", \"rb\") as f:\n        bytes_data = f.read()\n        ds.append([{\"col1\": bytes_data}])\n    \n\n##  `` deeplake.types.Struct \u00b6\n\n    \n    \n    Struct(fields: dict[str, DataType | str]) -> DataType\n    \n\nDefines a custom datatype with specified keys.",
        "node_538": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_144": "Feel like back to home.'}, score=0.02172369191268401),\n     '10788': Document(id='10788', data={'restaurant_name': 'Casa Lupe', 'restaurant_review': 'Run by a family that makes you feel like part of the family. Awesome food. I love their wet Chili Verde burritos'}, score=0.02088547418398944)}\n    \n\n### Show the results\u00b6\n\nWe will output a list of restaurants in order of relevance, showing each name\nand review based on the hybrid search results.\n\n    \n    \n    for v in sorted_documents.values():\n        print(f\"Restaurant name: {v.data['restaurant_name']} \\nReview: {v.data['restaurant_review']}\")\n    \n\nOutput:\n\n    \n    \n    Restaurant name: Olympus Caffe & Bakery\n    Review: I like the garden to sit down with friends and have a drink.\n    Restaurant name: St. Stephen's Green\n    Review: Nice place for a drink\n    Restaurant name: St. Stephen's Green\n    Review: Good drinks, good food\n    Restaurant name: Eureka! Mountain View\n    Review: Good drinks and burgers\n    Restaurant name: St. Stephen's Green\n    Review: Good drinks an easy going bartenders\n    Restaurant name: Scratch\n    Review: Just had drinks. They were good!\n    Restaurant name: Mifen101 \u82b1\u6eaa\u7c73\u7c89\u738b\n    Review: Feel like I\u2019m back in China.\n    Restaurant name: Ludwigs Biergarten Mountain View\n    Review: Beer is fresh tables are big feel like a proper beer garden\n    Restaurant name: Seasons Noodles & Dumplings Garden\n    Review: Comfort food, excellent service! Feel like back to home.\n    Restaurant name: Casa Lupe\n    Review: Run by a family that makes you feel like part of the family. Awesome food.",
        "node_637": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_295": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nVersion Control\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control  Version Control  Table of contents \n      * Version \n      * History \n      * Branching \n        * Branch \n        * Branches \n        * BranchView \n        * BranchesView \n      * Tagging \n        * Tag \n        * Tags \n        * TagView \n        * TagsView \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Version \n  * History \n  * Branching \n    * Branch \n    * Branches \n    * BranchView \n    * BranchesView \n  * Tagging \n    * Tag \n    * Tags \n    * TagView \n    * TagsView \n\n# Version Control\u00b6\n\n## Version\u00b6\n\n####  `` deeplake.Version \u00b6\n\nAn atomic change within deeplake.Dataset's history\n\n#####  `` client_timestamp `property` \u00b6\n\n    \n    \n    client_timestamp: datetime\n    \n\nWhen the version was created, according to the writer's local clock.",
        "node_711": "}}\n    \n        \"\"\"\n    \n        user_prompt = f\"Here is a question from a user: {question}\\n\\nHere are the top relevant information about restaurants {information}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},\n            ],\n            response_format={\"type\": \"json_object\"},\n        )\n    \n        try:\n            response = response.choices[0].message.content\n            response = json.loads(response)\n            questions = response[\"answer\"]\n            return questions\n        except:\n            return False\n    \n\nThis function takes a restaurant-related question and retrieves the best\nresponse based on the given context. It completes the RAG process by combining\nrelevant information and LLM-generated content into a concise answer.\n\n    \n    \n    information = [f'Review: {el[\"restaurant_review\"]}, Restaurant name: {el[\"restaurant_name\"]}' for el in view_vs]\n    result = generate_question(query, information)\n    print(result)\n    \n\nOutput:\n\n    \n    \n    \"If you're feeling like a drink, consider visiting Taqueria La Espuela, which is known for its refreshing horchata. Alternatively, you might enjoy Chaat Bhavan Mountain View, a great place with good food and a lively atmosphere.\"\n    \n\n### Let's run a search on a multiple dataset\u00b6\n\nIn this approach, we perform the hybrid search across two separate datasets:\n`vector_search` for vector-based search results and `ds_bm25` for BM25-based\ntext search results. This allows us to independently query and retrieve scores\nfrom each dataset, then combine them using the same fusion method as before.",
        "node_749": "ds.summary()\n    \n\nds.summary()\n\n    \n    \n    Dataset(columns=(images,labels,boxes), length=6471)\n    +------+--------------------------------------------+\n    |column|                    type                    |\n    +------+--------------------------------------------+\n    |images| array(dtype=uint8, shape=[None,None,None]) |\n    +------+--------------------------------------------+\n    |labels|     array(dtype=uint32, shape=[None])      |\n    +------+--------------------------------------------+\n    |boxes |array(dtype=float32, shape=[None,None,None])|\n    +------+--------------------------------------------+\n    \n    \n\n## Reading Data\u00b6\n\nDeep Lake does not download any data in advance. Data is fetched lazily from\nlong-term storage based on row numbers in the dataset:\n\nIn [5]:\n\nCopied!\n\n    \n    \n    image = ds[\"images\"][0] # Fetch the first image and return a numpy array\n    labels = ds[\"labels\"][0] # Fetch the labels in the first image\n    boxes = ds[\"boxes\"][0] # Fetch the bounding boxes for the first image\n    \n    img_list = ds[\"labels\"][0:100] # Fetch 100 labels and store them as a list of numpy arrays\n    \n\nimage = ds[\"images\"][0] # Fetch the first image and return a numpy array\nlabels = ds[\"labels\"][0] # Fetch the labels in the first image boxes =\nds[\"boxes\"][0] # Fetch the bounding boxes for the first image img_list =\nds[\"labels\"][0:100] # Fetch 100 labels and store them as a list of numpy\narrays\n\n## Visualizing Datasets\u00b6\n\nThe dataset above can be visualized in the Deep Lake App\n\n## Creating Your Own Datasets\u00b6\n\nLet's follow along with the example below to create our first dataset. First,\ndownload and unzip the small classification dataset below called the _animals\ndataset_.\n\nIn [57]:\n\nCopied!",
        "node_569": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_439": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_513": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_380": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_757": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_640": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_498": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_731": "By contrast, after myocardial infarction, susceptible dogs showed significant decrease in all measures of heart rate variability. Before myocardial infarction, no differences were seen between susceptible and resistant dogs. However, 30 days after infarction, epidemiologic analysis of the coefficient of variance showed high sensitivity and specificity (88% and 80%, respectively), predicting susceptibility. Therefore, results of analysis of 30 min of beat to beat heart period at rest 30 days after myocardial infarction are highly predictive for increased risk of sudden death. \\n5\\tMultiple organ failure: inflammatory priming and activation sequences promote autologous tissue injury. Systemic inflammation promotes multiple organ failure through the induction of diffuse microvascular leak. Inflammatory cells such as neutrophil\n    Text: Risk for sudden death was assessed 1 month after myocardial infarction by a protocol in which exercise and myocardial ischemia were combined; dogs that developed ventricular fibrillation were classified at high risk for sudden death (susceptible) and the survivors were considered low risk (resistant). In resistant dogs, myocardial infarction did not affect any measure of heart rate variability: 1) mean RR interval, 2) standard deviation of the mean RR interval, and 3) the coefficient of variance (standard deviation/RR interval). By contrast, after myocardial infarction, susceptible dogs showed significant decrease in all measures of heart rate variability. Before myocardial infarction, no differences were seen between susceptible and resistant dogs. However, 30 days after infarction, epidemiologic analysis of the coefficient of variance showed high sensitivity and specificity (88% and 80%, respectively), predicting susceptibility. Therefore, results of analysis of 30 min of beat to beat heart period at rest 30 days after myocardial infarction are highly predictive for increased risk of sudden death. \\n5\\tMultiple organ failure: inflammatory priming and activation sequences promote autologous tissue injury.\n    Text: However, no paired studies have been reported to examine heart rate variability before and after myocardial infarction.",
        "node_536": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_730": "1. **Query Embedding** : The query is embedded with `ckpt.queryFromText` and converted into a format compatible with TQL queries.\n\n    \n    \n    query_vectors = ckpt.queryFromText([\"What were the key risk factors for the development of posthemorrhagic/postoperative epilepsy in the study?\"])[0]\n    query_vectors = query_vectors.tolist()\n    \n\n  1. **TQL Query Construction** : The `maxsim` function compares the query embedding to dataset embeddings, ranking results by similarity and limiting them to the top `n_res` matches.\n\n  2. **Query Execution** : `medical_dataset.query` retrieves the most relevant entries based on semantic similarity.\n\n    \n    \n    n_res = 3\n    q_substrs = [f\"ARRAY[{','.join(str(x) for x in sq)}]\" for sq in query_vectors]\n    q_str = f\"ARRAY[{','.join(q_substrs)}]\"\n    \n    # Construct a formatted TQL query\n    tql_colbert = f\"\"\"\n        SELECT *, maxsim(embedding, {q_str}) as score\n        ORDER BY maxsim(embedding, {q_str}) DESC \n        LIMIT {n_res}\n    \"\"\"\n    \n    # Execute the query and append the results\n    results = medical_dataset.query(tql_colbert)\n    \n\nHere are the results:\n\n    \n    \n    for res in results:\n        print(f\"Text: {res['text']}\")\n    \n\nOutput:\n\n    \n    \n    Text: In resistant dogs, myocardial infarction did not affect any measure of heart rate variability: 1) mean RR interval, 2) standard deviation of the mean RR interval, and 3) the coefficient of variance (standard deviation/RR interval). By contrast, after myocardial infarction, susceptible dogs showed significant decrease in all measures of heart rate variability. Before myocardial infarction, no differences were seen between susceptible and resistant dogs. However, 30 days after infarction, epidemiologic analysis of the coefficient of variance showed high sensitivity and specificity (88% and 80%, respectively), predicting susceptibility.",
        "node_342": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_697": "**Apply Softmax to Scores** : \n\n     * We extract `score` values from `vs_results` and `bm25_results` and apply `softmax` to them, storing the results in `vss` and `bm25s`. This step scales both sets of scores for easy comparison.\n  2. **Create Document Dictionaries** : \n\n     * We create dictionaries `docs_vs` and `docs_bm25` to store documents from `vs_results` and `bm25_results`, respectively. For each result, we add the `restaurant_name` and `restaurant_review` along with the normalized score. Each document is identified by `row_id`.\n\nThis code standardizes scores and organizes results, allowing comparison\nacross both vector and BM25 search methods.",
        "node_780": "## Community\n\nJoin our **Slack community** to learn more about unstructured dataset\nmanagement using Deep Lake and to get help from the Activeloop team and other\nusers.\n\nWe'd love your feedback by completing our 3-minute **survey**.\n\nAs always, thanks to our amazing contributors!\n\nMade with contributors-img.\n\nPlease read CONTRIBUTING.md to get started with making contributions to Deep\nLake.\n\n## README Badge\n\nUsing Deep Lake? Add a README badge to let everyone know:\n\n    \n    \n    [![deeplake](https://img.shields.io/badge/powered%20by-Deep%20Lake%20-ff5a1f.svg)](https://github.com/activeloopai/deeplake)\n\n## Disclaimers\n\n**Dataset Licenses**\n\nDeep Lake users may have access to a variety of publicly available datasets.\nWe do not host or distribute these datasets, vouch for their quality or\nfairness, or claim that you have a license to use the datasets. It is your\nresponsibility to determine whether you have permission to use the datasets\nunder their license.\n\nIf you're a dataset owner and do not want your dataset to be included in this\nlibrary, please get in touch through a GitHub issue. Thank you for your\ncontribution to the ML community!\n\n**Usage Tracking**\n\nBy default, we collect usage data using Bugout (here's the code that does it).\nIt does not collect user data other than anonymized IP address data, and it\nonly logs the Deep Lake library's own actions. This helps our team understand\nhow the tool is used and how to build features that matter to you!",
        "node_403": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_654": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_657": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_0": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_699": "Stephen's Green\", 'restaurant_review': 'Nice place for a drink'}, score=0.21224761685297047),\n     '17444': Document(id='17444', data={'restaurant_name': \"St. Stephen's Green\", 'restaurant_review': 'Good drinks, good food'}, score=0.19800771415362647),\n     '4022': Document(id='4022', data={'restaurant_name': 'Eureka! Mountain View', 'restaurant_review': 'Good drinks and burgers'}, score=0.1970674552539808),\n     '17426': Document(id='17426', data={'restaurant_name': \"St. Stephen's Green\", 'restaurant_review': 'Good drinks an easy going bartenders'}, score=0.19663342673946818),\n     '5136': Document(id='5136', data={'restaurant_name': 'Scratch', 'restaurant_review': 'Just had drinks. They were good!'}, score=0.19604378699995426)}\n    \n    {'3518': Document(id='3518', data={'restaurant_name': 'Olympus Caffe & Bakery', 'restaurant_review': 'I like the garden to sit down with friends and have a drink.'}, score=0.7132230191866898),\n     '2637': Document(id='2637', data={'restaurant_name': 'Mifen101 \u82b1\u6eaa\u7c73\u7c89\u738b', 'restaurant_review': 'Feel like I\u2019m back in China.'}, score=0.10997834807700335),\n     '11383': Document(id='11383', data={'restaurant_name': 'Ludwigs Biergarten Mountain View', 'restaurant_review': 'Beer is fresh tables are big feel like a proper beer garden'}, score=0.09158030054295993),\n     '2496': Document(id='2496', data={'restaurant_name': 'Seasons Noodles & Dumplings Garden', 'restaurant_review': 'Comfort food, excellent service! Feel like back to home.",
        "node_205": "While this method is straightforward, it can become a\nbottleneck when working with large datasets with multiple tensors.\n\n## Asynchronous Data Fetching\u00b6\n\nThe asynchronous fetching method utilizes asyncio and threading to load data\nin parallel. This significantly improves loading times, especially for large\ndatasets with multiple tensors.\n\n    \n    \n    import deeplake\n    \n    import asyncio\n    from threading import Thread, Lock\n    from multiprocessing import Queue\n    \n    lock = Lock()\n    index = -1\n    def generate_data(ds: deeplake.Dataset):\n        total_count = len(ds)\n        global index\n        while True:\n            idx = 0\n            with lock:\n                index = (index + 1) % (total_count - 1)\n                idx = index\n            yield ds[idx]\n    \n    class AsyncImageDataset(torch.utils.data.IterableDataset):\n        def __init__(self, deeplake_ds: deeplake.Dataset, transform: Callable = None, max_queue_size: int = 1024):\n            self.ds = deeplake_ds\n            self.transform = transform\n            self.worked_started = False\n            self.data_generator = generate_data(self.ds)\n            self.q = Queue(maxsize=max_queue_size)\n    \n        async def run_async(self):\n            for item in self.data_generator:\n                data = await asyncio.gather(\n                    item.get_async(\"images\"),\n                    item.get_async(\"masks\")\n                )\n                self.q.put(data)\n    \n        def start_worker(self):\n            loop = asyncio.new_event_loop()\n    \n            for _ in range(128):\n                loop.create_task(self.run_async())\n    \n            def loop_in_thread(loop):\n                asyncio.set_event_loop(loop)\n                loop.run_forever()\n    \n            self.loop_thread = Thread(target=loop_in_thread, args=(loop,), daemon=True)\n            self.loop_thread.start()\n    \n            self.worked_started = True\n    \n        def __iter__(self):\n            while True:\n                if not self.worked_started:\n                    self.start_worker()\n    \n                # wait until some data is filled\n                while self.q.empty():\n                    pass\n    \n                image, mask = self.q.get()\n                if self.transform is not None:\n                    image, mask = self.transform((image, mask))\n    \n                yield image, mask\n    \n\nThe `AsyncImageDataset` utilizes Python\u2019s `asyncio` library to fetch images\nand masks concurrently from `deeplake.Dataset`, minimizing data loading times.",
        "node_294": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.",
        "node_171": "MaxSim allows the system to compare different types of\ndata, like text and images, and find the most relevant matches. This helps\nretrieve results that are contextually accurate and meaningful, making it\nespecially useful for complex applications, like scientific and medical\nresearch, where a deep understanding of the content is essential.\n\nRecent advancements in Visual Language Models (VLMs), as highlighted in the\nColPali paper, demonstrate that VLMs can achieve recall rates on document\nretrieval benchmarks comparable to those of traditional OCR pipelines. End-to-\nend learning approaches are positioned to surpass OCR-based methods\nsignificantly. However, representing documents as a `bag of embeddings`\ndemands 30 times more storage than single embeddings. Deep Lake's format,\nwhich inherently supports n-dimensional arrays, enables this storage-intensive\napproach, and the 4.0 query engine introduces MaxSim operations.\n\nWith Deep Lake 4.0's 10x increase in storage efficiency, we can allocate some\nof these savings to store PDFs as 'bags of embeddings' processed at high\nspeeds. While this approach requires 30 times more storage than single\nembeddings, it allows us to capture richer document representations, bypassing\nOCR-based, manual feature engineering pipelines. This trade-off facilitates\nseamless integration within VLM/LLM frameworks, leading to more accurate and\ngenuinely multimodal responses.\n\nUnlike CLIP, which primarily focuses on aligning visual and text\nrepresentations, ColPali leverages advanced Vision Language Model (VLM)\ncapabilities to deeply understand both textual and visual content. This allows\nColPali to capture rich document structures\u2014like tables, figures, and\nlayouts\u2014directly from images without needing extensive preprocessing steps\nlike OCR or document segmentation. ColPali also utilizes a late interaction\nmechanism, which significantly improves retrieval accuracy by enabling more\ndetailed matching between query elements and document content. These features\nmake ColPali faster, more accurate, and especially effective for visually rich\ndocument retrieval, surpassing CLIP's capabilities in these areas\u200b.",
        "node_254": "Parameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`url` |  `str` |  The URL of the destination dataset |  _required_  \n`creds` |  `dict[str, str] | None` |  Optional credentials needed to connect to the dataset |  `None`  \n`token` |  `str | None` |  Optional deeplake token |  `None`  \n  \n####  `` pull \u00b6\n\n    \n    \n    pull(\n        url: str,\n        creds: dict[str, str] | None = None,\n        token: str | None = None,\n    ) -> None\n    \n\nPulls any new history from the dataset at the passed url into this dataset.\n\nSimilar to deeplake.Dataset.push but the other direction.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`url` |  `str` |  The URL of the destination dataset |  _required_  \n`creds` |  `dict[str, str] | None` |  Optional credentials needed to connect to the dataset |  `None`  \n`token` |  `str | None` |  Optional deeplake token |  `None`  \n  \n####  `` query \u00b6\n\n    \n    \n    query(query: str) -> DatasetView\n    \n\nExecutes the given TQL query against the dataset and return the results as a\ndeeplake.DatasetView.\n\nExamples:\n\n    \n    \n    result = ds.query(\"select * where category == 'active'\")\n    for row in result:\n        print(\"Id is: \", row[\"id\"])\n    \n\n####  `` query_async \u00b6\n\n    \n    \n    query_async(query: str) -> Future\n    \n\nAsynchronously executes the given TQL query against the dataset and return a\nfuture that will resolve into deeplake.DatasetView.",
        "node_572": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_502": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_135": "**Apply Softmax to Scores** : \n\n     * We extract `score` values from `vs_results` and `bm25_results` and apply `softmax` to them, storing the results in `vss` and `bm25s`. This step scales both sets of scores for easy comparison.\n  2. **Create Document Dictionaries** : \n\n     * We create dictionaries `docs_vs` and `docs_bm25` to store documents from `vs_results` and `bm25_results`, respectively. For each result, we add the `restaurant_name` and `restaurant_review` along with the normalized score. Each document is identified by `row_id`.\n\nThis code standardizes scores and organizes results, allowing comparison\nacross both vector and BM25 search methods.\n\n    \n    \n    vs_score = vs_results[\"score\"]\n    bm_score = bm25_results[\"score\"]\n    \n    vss = softmax(vs_score)\n    bm25s = softmax(bm_score)\n    print(vss)\n    print(bm25s)\n    print(vs_results)\n    \n\nOutput:\n\n    \n    \n    [0.21224761685297047, 0.19800771415362647, 0.1970674552539808, 0.19663342673946818, 0.19604378699995426]\n    [0.7132230191866898, 0.10997834807700335, 0.09158030054295993, 0.04344738382536802, 0.04177094836797888]\n    Dataset(columns=(embedding,restaurant_name,restaurant_review,owner_answer,row_id,score), length=5)\n    \n    \n    \n    docs_vs = {}\n    docs_bm25 = {}\n    for el, score in zip(vs_results, vss):\n        docs_vs[str(el[\"row_id\"])] = Document(id=str(el[\"row_id\"]), data={\"restaurant_name\": el[\"restaurant_name\"], \"restaurant_review\": el[\"restaurant_review\"]}, score=score)\n    \n    for el, score in zip(bm25_results, bm25s):\n        docs_bm25[str(el[\"row_id\"])] = Document(id=str(el[\"row_id\"]), data={\"restaurant_name\": el[\"restaurant_name\"], \"restaurant_review\": el[\"restaurant_review\"]}, score=score)\n    print(docs_vs)\n    print(docs_bm25)\n    \n\nOutput:\n\n    \n    \n    {'17502': Document(id='17502', data={'restaurant_name': \"St.",
        "node_319": "Some data types\ncan be stored with multiple deeplake types. For example you can store images\nas `deeplake.types.Array(dimensions=3)` or `deeplake.types.Image()`. For this\ncase, it is recommended to use `deeplake.types.Image()` as it allows to store\nimages in compressed format and stores the data more efficiently.\n\nIn general if there is a specific type for your data prefer to use that\ninstead of generic arrays.\n\n  * Use `deeplake.types.Image()` for images. This allows efficient image compression and decompression.\n  * Use `deeplake.types.Text()` for text data. This allows efficient text search and indexing.\n  * Use `deeplake.types.Embedding()` for embeddings. This allows efficient vector similarity search with query.\n\n### Prefer appending data in batches\u00b6\n\nThere are two ways to append data to the dataset.\n\n  1. Append data row by row. In this case you have row as a dictionary. You can combine rows into list of dictionaries and append them to the dataset.\n    \n        ds.append([{\"column1\": value1, \"column2\": value2}, {\"column1\": value3, \"column2\": value4}])\n    \n\n  2. Append data in batches. In this case you have single dictionary with list of values for each column.\n    \n        ds.append({\"column1\": [value1, value3], \"column2\": [value2, value4]})\n    \n\nThe second method is more efficient as the Deep Lake columnar format is\nhandling the list for column more efficiently. It can bring to significant\nperformance boost.\n\n### Avoid decompressing images when adding them to the dataset\u00b6\n\nDeep Lake Image type supports multiple image formats including JPEG, PNG, and\nTIFF. When adding images to the dataset, no need to decompress images into\n`numpy.array` or `PIL.Image` objects. Instead you can pass raw bytes and Deep\nLake will automatically decompress the images when reading them.",
        "node_425": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_347": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_74": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nUser Authentication\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication  Authentication  Table of contents \n      * How to Register and Authenticate in Deep Lake \n        * Registration and Login \n        * Authentication in Programmatic Interfaces \n      * Next Steps \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * How to Register and Authenticate in Deep Lake \n    * Registration and Login \n    * Authentication in Programmatic Interfaces \n  * Next Steps \n\n# User Authentication\u00b6\n\n## How to Register and Authenticate in Deep Lake\u00b6\n\n### Registration and Login\u00b6\n\nIn order to use Deep Lake features that require authentication (Activeloop\nstorage, connecting your cloud dataset to the Deep Lake UI, etc.) you should\nregister and login in the Deep Lake App.",
        "node_553": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_677": "**Initialize Lists** : `restaurant_name`, `restaurant_review` and `owner_answer` are initialized to store respective data for each restaurant.\n\n  2. **Populate Lists** : For each entry (`el`) in `scraped_data`, the code appends:\n\n     * `el['restaurant_name']` to `restaurant_name`\n     * `el['restaurant_review']` to `restaurant_review`\n     * `el['owner_answer']` to `owner_answer`\n\nAfter running, each list holds a specific field from all restaurants, ready\nfor further processing.\n\n    \n    \n    restaurant_name = []\n    restaurant_review = []\n    owner_answer = []\n    images = []\n    for el in scraped_data:\n        restaurant_name.append(el['restaurant_name'])\n        restaurant_review.append(el['restaurant_review'])\n        owner_answer.append(el['owner_answer'])\n    \n\n### Add the data to the dataset\u00b6\n\nWe add the collected restaurant names and reviews to the dataset `ds`. Using\n`ds.append()`, we insert two columns: `\"restaurant_name\"` and\n`\"restaurant_review\"`, populated with the values from our lists\n`restaurant_name` and `restaurant_review`. After appending the data,\n`ds.commit()` saves the changes permanently to the dataset, ensuring all new\nentries are stored and ready for further processing.\n\n    \n    \n    ds.append({\n        \"restaurant_name\": restaurant_name,\n        \"restaurant_review\": restaurant_review,\n        \"owner_answer\": owner_answer\n    })\n    ds.commit()\n    print(ds)\n    \n\nOutput:\n\n    \n    \n    Dataset(columns=(restaurant_name,restaurant_review,owner_answer), length=18625)\n    \n\n### Search for the restaurant using a specific word\u00b6\n\nWe define a search query to find any entries in the dataset `ds` where the\nword `\"tapas\"` appears in the `restaurant_review` column. The command\n`ds.query()` runs a TQL query with `SELECT *`, which retrieves all entries\nthat match the condition `CONTAINS(restaurant_review, '{word}')`. This search\nfilters the dataset to show only records containing the specified word\n(`tapas`) in their reviews.",
        "node_174": "These two modules generate multi-dimensional embeddings representing both visual and textual aspects of the document.\n\n     * The embeddings are stored in a pre-indexed format, making them ready for fast retrieval during the online phase.\n\n  2. **Online Query Processing** : \n\n     * On the right side, in the **online** section, user queries (such as \"What are ViTs?\") are processed through the **language model** to create a query embedding.\n\n     * ColPali uses a **late interaction mechanism** , where each part of the query embedding is compared with document embeddings through a **MaxSim** operation to find the most similar regions in the document's visual and textual content.\n\n  3. **Similarity Scoring** : \n\n     * ColPali calculates a **similarity score** based on the MaxSim results, which identifies the most relevant documents or document sections matching the query.\n\n     * This approach allows ColPali to capture fine-grained matches, even within complex document structures. The ColPali model improves on traditional document retrieval methods by incorporating both vision and language models, making it effective for **visually rich documents** (such as those with tables, images, or infographics). Additionally, its **late interaction** mechanism enables fast and accurate retrieval, optimizing the model for low-latency performance even in large-scale applications\u200b.\n\n    \n    \n    import torch\n    from PIL import Image\n    \n    from colpali_engine.models import ColPali, ColPaliProcessor\n    \n    model_name = \"vidore/colpali-v1.2\"\n    \n    model = ColPali.from_pretrained(\n        model_name,\n        torch_dtype=torch.bfloat16,\n        device_map=\"cuda:0\",  # or \"mps\" if on Apple Silicon\n    ).eval()\n    \n    processor = ColPaliProcessor.from_pretrained(model_name)\n    \n\nWe load the **FigQA** dataset using `deeplake`, specifically retrieving the\n`\"train\"` split of the `\"FigQA\"` subset within the `\"futurehouse/lab-bench\"`\ndataset.",
        "node_407": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_615": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_302": "#####  `` rename \u00b6\n\n    \n    \n    rename(new_name: str) -> None\n    \n\nRenames the tag within the dataset\n\n#####  `` version `property` \u00b6\n\n    \n    \n    version: str\n    \n\nThe version that has been tagged\n\n    \n    \n    # Create tag\n    ds.tag(\"v1.0\")\n    \n    # Access tagged version\n    tag = ds.tags[\"v1.0\"]\n    print(f\"Tag: {tag.name}\")\n    print(f\"Version: {tag.version}\")\n    \n    # Open dataset at tag\n    tagged_ds = tag.open()\n    \n    # Rename tag\n    tag.rename(\"v1.0.0\")\n    \n    # Delete tag\n    tag.delete()\n    \n\n### Tags\u00b6\n\n####  `` deeplake.Tags \u00b6\n\nProvides access to the tags within a dataset.\n\nIt is returned by the deeplake.Dataset.tags property.\n\n#####  `` __getitem__ \u00b6\n\n    \n    \n    __getitem__(name: str) -> Tag\n    \n\nReturn a tag by name\n\n#####  `` __len__ \u00b6\n\n    \n    \n    __len__() -> int\n    \n\nThe total number of tags in the dataset\n\n#####  `` names \u00b6\n\n    \n    \n    names() -> list[str]\n    \n\nReturn a list of tag names\n\n    \n    \n    # Create tag\n    ds.tag(\"v1.0\")  # Tag current version\n    specific_version = ds.version\n    ds.tag(\"v2.0\", version=specific_version)  # Tag specific version\n    \n    # List all tags\n    for name in ds.tags.names():\n        tag = ds.tags[name]\n        print(f\"Tag {tag.name} points to version {tag.version}\")\n    \n    # Check number of tags\n    num_tags = len(ds.tags)\n    \n    # Access specific tag\n    tag = ds.tags[\"v1.0\"]\n    \n    # Common operations with tags\n    latest_ds = ds.tags[\"v2.0\"].open()  # Open dataset at tag\n    stable_ds = ds.tags[\"v1.0\"].open_async()  # Async open\n    \n    # Error handling\n    try:\n        tag = ds.tags[\"non_existent\"]\n    except deeplake.TagNotFoundError:\n        print(\"Tag not found\")\n    \n\n### TagView\u00b6\n\n####  `` deeplake.TagView \u00b6\n\nDescribes a read-only tag within the dataset.",
        "node_50": "**100+ most-popular image, video,\nand audio datasets available in seconds** Deep Lake community has uploaded\n100+ image, video and audio datasets like MNIST, COCO, ImageNet, CIFAR, GTZAN\nand others.  **Instant Visualization Support in theDeep Lake App** Deep Lake\ndatasets are instantly visualized with bounding boxes, masks, annotations,\netc. in Deep Lake Visualizer (see below).\n\n## \ud83d\ude80 How to install Deep Lake\n\nDeep Lake can be installed using pip:\n\n    \n    \n    pip install deeplake\n\n### To access all of Deep Lake's features, please register in the Deep Lake\nApp.\n\n## \ud83e\udde0 Deep Lake Code Examples by Application\n\n### Vector Store Applications\n\nUsing Deep Lake as a Vector Store for building LLM applications:\n\n### \\- Vector Store Quickstart\n\n### \\- Vector Store Tutorials\n\n### \\- LangChain Integration\n\n### \\- LlamaIndex Integration\n\n### \\- Image Similarity Search with Deep Lake\n\n### Deep Learning Applications\n\nUsing Deep Lake for managing data while training Deep Learning models:\n\n### \\- Deep Learning Quickstart\n\n### \\- Tutorials for Training Models\n\n## \u2699\ufe0f Integrations\n\nDeep Lake offers integrations with other tools in order to streamline your\ndeep learning workflows. Current integrations include:\n\n  * **LLM Apps**\n    * Use Deep Lake as a vector store for LLM apps. Our integration combines the Langchain VectorStores API with Deep Lake datasets as the underlying data storage. The integration is a serverless vector store that can be deployed locally or in a cloud of your choice.",
        "node_433": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_528": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_753": "class_names = [item for item in os.listdir(dataset_folder) if\nos.path.isdir(os.path.join(dataset_folder, item))] files_list = [] for\ndirpath, dirnames, filenames in os.walk(dataset_folder): for filename in\nfilenames: files_list.append(os.path.join(dirpath, filename))\n\nNext, let's **create the dataset columns and upload data**.\n\nIn [ ]:\n\nCopied!\n\n    \n    \n    ds.add_column('images', dtype = types.Image(sample_compression = \"jpg\"))\n    ds.add_column('labels', dtype = types.Array( dtype = types.UInt32(), dimensions=1))\n    \n    # Iterate through the files and append to Deep Lake dataset\n    for file in files_list:\n        label_text = os.path.basename(os.path.dirname(file))\n        label_num = class_names.index(label_text)\n    \n        #Append data to the tensors\n        ds.append({'images': [open(file, \"rb\").read()], 'labels': [label_num]})\n    \n\nds.add_column('images', dtype = types.Image(sample_compression = \"jpg\"))\nds.add_column('labels', dtype = types.Array( dtype = types.UInt32(),\ndimensions=1)) # Iterate through the files and append to Deep Lake dataset for\nfile in files_list: label_text = os.path.basename(os.path.dirname(file))\nlabel_num = class_names.index(label_text) #Append data to the tensors\nds.append({'images': [open(file, \"rb\").read()], 'labels': [label_num]})\n\nIn [68]:\n\nCopied!\n\n    \n    \n    ds.summary()\n    \n\nds.summary()\n\n    \n    \n    Dataset(columns=(images,labels), length=0)\n    +------+------------------------------------------+\n    |column|                   type                   |\n    +------+------------------------------------------+\n    |images|array(dtype=uint8, shape=[None,None,None])|\n    +------+------------------------------------------+\n    |labels|    array(dtype=uint32, shape=[None])     |\n    +------+------------------------------------------+\n    \n    \n\nNotebook\n\nBack to top",
        "node_393": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_185": "* Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore  VectorStore  Table of contents \n      * How to Use Deep Lake as a Vector Store in LangChain \n      * Downloading and Preprocessing the Data \n      * A note on chunking text files \n      * Creating the Deep Lake Vector Store \n      * Use the Vector Store in a Q&A App \n      * Accessing the Low Level Deep Lake API (Advanced) \n      * SelfQueryRetriever with Deep Lake \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * How to Use Deep Lake as a Vector Store in LangChain \n  * Downloading and Preprocessing the Data \n  * A note on chunking text files \n  * Creating the Deep Lake Vector Store \n  * Use the Vector Store in a Q&A App \n  * Accessing the Low Level Deep Lake API (Advanced) \n  * SelfQueryRetriever with Deep Lake \n\n# Using Deep Lake as a Vector Store in LangChain\u00b6\n\n## How to Use Deep Lake as a Vector Store in LangChain\u00b6\n\nDeep Lake can be used as a VectorStore in LangChain for building Apps",
        "node_564": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_47": "Deep Lake is a Database for AI powered by a storage format optimized for deep-\nlearning applications. Deep Lake can be used for:\n\n  1. Storing and searching data plus vectors while building LLM applications\n  2. Managing datasets while training deep learning models\n\nDeep Lake simplifies the deployment of enterprise-grade LLM-based products by\noffering storage for all data types (embeddings, audio, text, videos, images,\ndicom, pdfs, annotations, and more), querying and vector search, data\nstreaming while training models at scale, data versioning and lineage, and\nintegrations with popular tools such as LangChain, LlamaIndex, Weights &\nBiases, and many more. Deep Lake works with data of any size, it is\nserverless, and it enables you to store all of your data in your own cloud and\nin one place. Deep Lake is used by Intel, Bayer Radiology, Matterport, ZERO\nSystems, Red Cross, Yale, & Oxford.\n\n### Deep Lake includes the following features:\n\n**Multi-Cloud Support (S3, GCP, Azure)** Use one API to upload, download, and\nstream datasets to/from S3, Azure, GCP, Activeloop cloud, local storage, or\nin-memory storage. Compatible with any S3-compatible storage such as MinIO.\n**Native Compression with Lazy NumPy-like Indexing** Store images, audio, and\nvideos in their native compression. Slice, index, iterate, and interact with\nyour data like a collection of NumPy arrays in your system's memory.",
        "node_386": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.",
        "node_683": "After appending, `ds_bm25.commit()` saves the changes, ensuring the new data\nis permanently stored in the dataset. Finally, `ds_bm25.summary()` provides a\nsummary of the dataset's updated structure and contents, allowing us to verify\nthat the data was added successfully.\n\n    \n    \n    ds_bm25.append({\n        \"restaurant_name\": restaurant_name,\n        \"restaurant_review\": restaurant_review,\n        \"owner_answer\": owner_answer\n    })\n    ds_bm25.commit()\n    ds_bm25.summary()\n    \n\nOutput:\n\n    \n    \n    Dataset(columns=(restaurant_name,restaurant_review,owner_answer), length=18625)\n    +-----------------+-----------------+\n    |     column      |      type       |\n    +-----------------+-----------------+\n    | restaurant_name |text (bm25 Index)|\n    +-----------------+-----------------+\n    |restaurant_review|text (bm25 Index)|\n    +-----------------+-----------------+\n    |  owner_answer   |text (bm25 Index)|\n    +-----------------+-----------------+\n    \n\n### Search for the restaurant using a specific sentence\u00b6\n\nWe define a query, `\"I want burritos\"`, to find relevant restaurant reviews in\nthe dataset. Using `ds_bm25.query()`, we search and rank entries in\n`restaurant_review` based on **BM25 similarity** to the query. The code orders\nresults by how well they match the query (`BM25_SIMILARITY`), from highest to\nlowest relevance, and limits the output to the top 10 results. The final list\nof results is stored in `view_bm25`.",
        "node_143": "Mountain View', 'restaurant_review': 'Good drinks and burgers'}, score=0.0985337276269904),\n     '17426': Document(id='17426', data={'restaurant_name': \"St. Stephen's Green\", 'restaurant_review': 'Good drinks an easy going bartenders'}, score=0.09831671336973409),\n     '5136': Document(id='5136', data={'restaurant_name': 'Scratch', 'restaurant_review': 'Just had drinks. They were good!'}, score=0.09802189349997713),\n     '2637': Document(id='2637', data={'restaurant_name': 'Mifen101 \u82b1\u6eaa\u7c73\u7c89\u738b', 'restaurant_review': 'Feel like I\u2019m back in China.'}, score=0.054989174038501676),\n     '11383': Document(id='11383', data={'restaurant_name': 'Ludwigs Biergarten Mountain View', 'restaurant_review': 'Beer is fresh tables are big feel like a proper beer garden'}, score=0.045790150271479965),\n     '2496': Document(id='2496', data={'restaurant_name': 'Seasons Noodles & Dumplings Garden', 'restaurant_review': 'Comfort food, excellent service! Feel like back to home.'}, score=0.02172369191268401),\n     '10788': Document(id='10788', data={'restaurant_name': 'Casa Lupe', 'restaurant_review': 'Run by a family that makes you feel like part of the family. Awesome food. I love their wet Chili Verde burritos'}, score=0.02088547418398944)}\n    \n\n### Show the results\u00b6\n\nWe will output a list of restaurants in order of relevance, showing each name\nand review based on the hybrid search results.",
        "node_515": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_378": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_517": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_457": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_504": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_155": "**Define Transformations (`tform`)**:\n\n     * The transformation pipeline includes:\n       * **Resize** : Scales images to 224x224 pixels.\n       * **ToTensor** : Converts images to tensor format.\n       * **Lambda** : Ensures grayscale images are replicated across three channels to match the RGB format.\n       * **Normalize** : Standardizes pixel values based on common RGB means and standard deviations.\n  2. **Define`embedding_function_images`**:\n\n     * This function generates embeddings for a list of image.\n     * If `images` is a single filename, it's converted to a list.\n     * **Batch Processing** : Images are processed in batches (default size 4), with transformations applied to each image. The batch is then loaded to the device.\n     * **Embedding Creation** : The model encodes each batch into embeddings, stored in the `embeddings` list, which is returned as a single list.\n\nThis function supports efficient, batched embedding generation, useful for\nmulti-modal tasks like image-based search.\n\n    \n    \n    from torchvision import transforms\n    \n    tform = transforms.Compose([\n        transforms.Resize((224,224)), \n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: torch.cat([x, x, x], dim=0) if x.shape[0] == 1 else x),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ])\n    \n    def embedding_function_images(images, model = model, transform = tform, batch_size = 4):\n        \"\"\"Creates a list of embeddings based on a list of image. Images are processed in batches.\"\"\"",
        "node_396": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_557": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_44": "Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\n{{ message }}\n\nactiveloopai  / **deeplake ** Public\n\n  * Notifications  You must be signed in to change notification settings\n  * Fork 656\n  * Star  8.5k\n\nDatabase for AI. Store Vectors, Images, Texts, Videos, etc. Use with\nLLMs/LangChain. Store, query, version, & visualize any AI data. Stream data in\nreal-time to PyTorch/TensorFlow. https://activeloop.ai\n\nactiveloop.ai\n\n### License\n\nApache-2.0 license\n\n8.5k stars  656 forks  Branches Tags Activity\n\nStar\n\nNotifications  You must be signed in to change notification settings\n\n  * Code\n  * Issues 49\n  * Pull requests 9\n  * Discussions\n  * Actions\n  * Projects 0\n  * Wiki\n  * Security\n  * Insights\n\nAdditional navigation options\n\n  * Code \n  * Issues \n  * Pull requests \n  * Discussions \n  * Actions \n  * Projects \n  * Wiki \n  * Security \n  * Insights \n\n# activeloopai/deeplake\n\nmain\n\nBranchesTags\n\nGo to file\n\nCode\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\n## History\n\n9,213 Commits  \n.github| .",
        "node_690": "3. The query calculates the **cosine similarity** between the `embedding` column and `str_query`, which is the embedding of our query, `\"A restaurant that serves good burritos.\"`. This similarity score `score` measures how closely each entry matches our query.\n\n  4. **Order by Score and Limit Results** : \n\n  5. The query orders results by `score` in descending order, showing the most relevant matches first. We limit the results to the top 3 matches to focus on the best results.\n\n  6. **Execute Query** : \n\n  7. `vector_search.query(query_vs)` runs the query on the dataset, storing the output in `view_vs`, which contains the top 3 most similar entries based on cosine similarity. This approach helps us retrieve the most relevant records matching our query in `vector_search`.\n\n    \n    \n    query_vs = f\"\"\"\n        SELECT *, cosine_similarity(embedding, ARRAY[{str_query}]) as score\n        FROM (\n            SELECT *, ROW_NUMBER() AS row_id\n        )\n        ORDER BY cosine_similarity(embedding, ARRAY[{str_query}]) DESC \n    \n        LIMIT 3\n    \"\"\"\n    view_vs = vector_search.query(query_vs)\n    print(view_vs)\n    \n\nOutput:\n\n    \n    \n    Dataset(columns=(embedding,restaurant_name,restaurant_review,owner_answer,row_id,score), length=3)\n    \n    \n    \n    for row in view_vs:\n        print(f\"Restaurant name: {row['restaurant_name']} \\nReview: {row['restaurant_review']}\")\n    \n\nOutput:\n\n    \n    \n    Restaurant name: Cheztakos!!!\n    Review: Great burritos\n    Restaurant name: Los Amigos\n    Review: Nice place real good burritos.\n    Restaurant name: La Coste\u00f1a\n    Review: Awesome burritos\n    \n\nIf we want to filter for a specific owner answer, such as **Thank you** , we\nset `word = \"Thank you\"` to define the desired owner answer. Here, we're using\nan **inverted index** on the `owner_answer` column to efficiently filter\nresults based on this owner answer.",
        "node_360": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_419": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_435": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_305": "## Schema Templates\u00b6\n\nSchema templates are Python dictionaries that define the structure of the\ndataset. Each schema template is a dictionary with field names as keys and\nfield types as values.\n\n## Text Embeddings Schema\u00b6\n\n###  `` deeplake.schemas.TextEmbeddings \u00b6\n\n    \n    \n    TextEmbeddings(\n        embedding_size: int, quantize: bool = False\n    ) -> dict[str, DataType | str | Type]\n    \n\nA schema for storing embedded text from documents.\n\nThis schema includes the following fields: \\- id (uint64): Unique identifier\nfor each entry. \\- chunk_index (uint16): Position of the text chunk within the\ndocument. \\- document_id (uint64): Unique identifier for the document the\nembedding came from. \\- date_created (uint64): Timestamp when the document was\nread. \\- text_chunk (text): The text of the shard. \\- embedding\n(dtype=float32, size=embedding_size): The embedding of the text.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`embedding_size` |  `int` |  int Size of the embeddings. |  _required_  \n`quantize` |  `bool` |  bool, optional If true, quantize the embeddings to slightly decrease accuracy while greatly increasing query speed. Default is False.",
        "node_584": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_751": "# Unzip to './animals' folder\n    !tar -xvf ./animals.tar\n    \n\n# Unzip to './animals' folder !tar -xvf ./animals.tar\n\n    \n    \n    tar: Error opening archive: Unrecognized archive format\n    \n\nIn [ ]:\n\nCopied!\n\n    \n    \n    animals\n    - cats\n      - image_1.jpg\n      - image_2.jpg\n    - dogs\n      - image_3.jpg\n      - image_4.jpg\n    \n\nanimals \\- cats \\- image_1.jpg \\- image_2.jpg \\- dogs \\- image_3.jpg \\-\nimage_4.jpg\n\nNow that you have the data, you can **create a Deep Lake`Dataset`** and\ninitialize its tensors. Running the following code will create a Deep Lake\ndataset inside of the `./animals_dl` folder.\n\nIn [58]:\n\nCopied!\n\n    \n    \n    import deeplake\n    import numpy as np\n    import os\n    \n    ds = deeplake.create('./animals_dl') # Creates the dataset\n    \n\nimport deeplake import numpy as np import os ds =\ndeeplake.create('./animals_dl') # Creates the dataset\n\nNext, let's inspect the folder structure for the source dataset './animals' to\nfind the class names and the files that need to be uploaded to the Deep Lake\ndataset.\n\nIn [59]:\n\nCopied!\n\n    \n    \n    # Find the class_names and list of files that need to be uploaded\n    dataset_folder = '/Users/istranic/ActiveloopCode/Datasets/animals'\n    \n    # Find the subfolders, but filter additional files like DS_Store that are added on Mac machines.",
        "node_204": "The DataLoader\nsupports both sequential and asynchronous data fetching, with the asynchronous\napproach being optimized for performance and speed.\n\n## Dataset Structure\u00b6\n\nThe dataset comprises pairs of images and their respective masks. Each image\nis a high-resolution file, while each mask is a binary image indicating the\nregions of interest within the corresponding image.\n\n## Sequential data fetching\u00b6\n\nThis ImageDataset class is a custom implementation of a PyTorch dataset that\nuses `deeplake.Dataset` as a datasource.\n\n    \n    \n    class ImageDataset(torch.utils.data.Dataset):\n        def __init__(self, deeplake_ds: deeplake.Dataset, transform: Callable = None):\n            self.ds = deeplake_ds\n            self.transform = transform\n    \n        def __len__(self):\n            return len(self.ds)\n    \n        def __getitem__(self, item):\n            image =  self.ds[item][\"images\"]\n            mask = self.ds[item][\"masks\"]\n    \n            if self.transform is not None:\n                image, mask = self.transform((image, mask))\n    \n            return image, mask\n    \n\nIn the sequential fetching approach, data is loaded one item at a time in a\nsynchronous manner. While this method is straightforward, it can become a\nbottleneck when working with large datasets with multiple tensors.\n\n## Asynchronous Data Fetching\u00b6\n\nThe asynchronous fetching method utilizes asyncio and threading to load data\nin parallel. This significantly improves loading times, especially for large\ndatasets with multiple tensors.",
        "node_210": "deeplake_dataloader = {\"shuffle\": True, \"batch_size\": 4, 'num_workers': 8}\n        ),\n    \n        # Parameters as the same as for train\n        val=dict(\n            pipeline=test_pipeline,\n            deeplake_path=\"hub://activeloop/coco-val\",\n            deeplake_tensors = {\"img\": \"images\", \"gt_bboxes\": \"boxes\", \"gt_labels\": \"categories\"},\n            deeplake_dataloader = {\"shuffle\": False, \"batch_size\": 1, 'num_workers': 8}\n        ),\n    )\n    \n    \n    deeplake_metrics_format = \"COCO\"\n    \n    evaluation = dict(metric=[\"bbox\"], interval=1)\n    \n    load_from = \"checkpoints/yolov3_d53_mstrain-416_273e_coco-2b60fcd9.pth\"\n    \n    work_dir = \"./mmdet_outputs\"\n    \n    log_config = dict(interval=10)\n    \n    checkpoint_config = dict(interval=5000)\n    \n    seed = None\n    \n    device = \"cuda\"\n    \n    runner = dict(type='EpochBasedRunner', max_epochs=10)\n    \n\n## Training\u00b6\n\nNow we can start the training:\n\n    \n    \n    # Load config\n    cfg = Config.fromfile(config_path)\n    \n    # Build the detector\n    model = build_detector(cfg.model)\n    \n    # Create work directory\n    mmcv.mkdir_or_exist(os.path.abspath(cfg.work_dir))\n    \n    # Start training\n    from deeplake.integrations import mmdet as mmdet_deeplake\n    mmdet_deeplake.train_detector(\n        model, \n        cfg,\n        distributed=False,  # Set to True for multi-GPU training\n        validate=False      # Set to True if you have validation data\n    )\n    \n\n## Key Benefits of Using Deep Lake\u00b6\n\n  1. **Simple Data Loading** : Deep Lake automatically handles data streaming and batching, so you don't need to write custom data loaders.\n\n  2. **Efficient Storage** : Data is stored in an optimized format and loaded on-demand, saving disk space and memory.\n\n  3.",
        "node_5": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake? \n\n# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.",
        "node_109": "* 6) ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT \n      * 7) Discover Restaurants Using ColPali and the Late Interaction Mechanism \n        * Download the ColPali model \n        * Create a new dataset to store the ColPali embeddings \n        * Save the data in the dataset \n        * Chat with images \n        * Retrieve the most similar images \n        * VQA: Visual Question Answering \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Load the Data from Deep Lake \n  * 1) Create the Dataset and Use an Inverted Index for Filtering \n    * Extract the data \n    * Add the data to the dataset \n    * Search for the restaurant using a specific word \n    * Show the results \n  * 2) Create the Dataset and use BM25 to Retrieve the Data \n    * Add data to the dataset \n    * Search for the restaurant using a specific sentence \n    * Show the results \n  * 3) Create the Dataset and use Vector",
        "node_611": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_25": "Deep Lake datasets can be visualized and version\ncontrolled. Weaviate is limited to light metadata on top of the embeddings and\nhas no visualization. Deep Lake also has a performant dataloader for fine-\ntuning your Large Language Models.\n\n**Deep Lake vs DVC**\n\nDeep Lake and DVC offer dataset version control similar to git for data, but\ntheir methods for storing data differ significantly. Deep Lake converts and\nstores data as chunked compressed arrays, which enables rapid streaming to ML\nmodels, whereas DVC operates on top of data stored in less efficient\ntraditional file structures. The Deep Lake format makes dataset versioning\nsignificantly easier compared to traditional file structures by DVC when\ndatasets are composed of many files (i.e., many images). An additional\ndistinction is that DVC primarily uses a command-line interface, whereas Deep\nLake is a Python package. Lastly, Deep Lake offers an API to easily connect\ndatasets to ML frameworks and other common ML tools and enables instant\ndataset visualization through Activeloop's visualization tool.\n\n**Deep Lake vs MosaicML MDS format**\n\n  * **Data Storage Format:** Deep Lake operates on a columnar storage format, whereas MDS utilizes a row-wise storage approach. This fundamentally impacts how data is read, written, and organized in each system.\n  * **Compression:** Deep Lake offers a more flexible compression scheme, allowing control over both chunk-level and sample-level compression for each column or tensor. This feature eliminates the need for additional compressions like zstd, which would otherwise demand more CPU cycles for decompressing on top of formats like jpeg.",
        "node_15": "Deep Lake is a Database for AI powered by a storage format optimized for deep-\nlearning applications. Deep Lake can be used for:\n\n  1. Storing and searching data plus vectors while building LLM applications\n  2. Managing datasets while training deep learning models\n\nDeep Lake simplifies the deployment of enterprise-grade LLM-based products by\noffering storage for all data types (embeddings, audio, text, videos, images,\ndicom, pdfs, annotations, and more), querying and vector search, data\nstreaming while training models at scale, data versioning and lineage, and\nintegrations with popular tools such as LangChain, LlamaIndex, Weights &\nBiases, and many more. Deep Lake works with data of any size, it is\nserverless, and it enables you to store all of your data in your own cloud and\nin one place. Deep Lake is used by Intel, Bayer Radiology, Matterport, ZERO\nSystems, Red Cross, Yale, & Oxford.\n\n### Deep Lake includes the following features:\n\n**Multi-Cloud Support (S3, GCP, Azure)** Use one API to upload, download, and\nstream datasets to/from S3, Azure, GCP, Activeloop cloud, local storage, or\nin-memory storage. Compatible with any S3-compatible storage such as MinIO.\n**Native Compression with Lazy NumPy-like Indexing** Store images, audio, and\nvideos in their native compression. Slice, index, iterate, and interact with\nyour data like a collection of NumPy arrays in your system's memory.",
        "node_11": "# Search code, repositories, users, issues, pull requests.\n\nSearch\n\nClear\n\nSearch syntax tips\n\n#  Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\nInclude my email address so I can be contacted\n\nCancel  Submit feedback\n\n#  Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName\n\nQuery\n\nTo see all available qualifiers, see our documentation.\n\nCancel  Create saved search\n\nSign in\n\nSign up  Reseting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\n{{ message }}\n\nactiveloopai  / **deeplake ** Public\n\n  * Notifications  You must be signed in to change notification settings\n  * Fork 656\n  * Star  8.5k\n\nDatabase for AI. Store Vectors, Images, Texts, Videos, etc. Use with\nLLMs/LangChain. Store, query, version, & visualize any AI data. Stream data in\nreal-time to PyTorch/TensorFlow. https://activeloop.ai\n\nactiveloop.ai\n\n### License\n\nApache-2.0 license\n\n8.",
        "node_537": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_213": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nSemantic Segmentation with Deep Lake and MMSegmentation\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM  Segmentation with MM  Table of contents \n        * Integration Interface \n          * Example Configuration with Deep Lake \n        * Prerequisites \n        * Setup \n        * Configuration \n        * Training \n        * Deep Lake Integration Benefits \n        * Monitoring Training \n        * Inference \n          *",
        "node_224": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.",
        "node_601": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_381": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_99": "* Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Authenticating Using Workload Identities Instead of User Credentials \n  * Step 1: Define the workload identity in Azure \n  * Step 2: Attached the Azure Managed Identity to your workload \n  * Step 3: Create a Deep Lake Workload Identity using the Azure Managed Identity \n  * Step 4: Run the workload \n\n# Azure Workload Identities\u00b6\n\nHow to authenticate using workload identities instead of user credentials.\n\n## Authenticating Using Workload Identities Instead of User Credentials\u00b6\n\nWorkload identities enable you to define a cloud workload that will have\naccess to your Deep Lake organization without authenticating using Deep Lake\nuser tokens. This enables users to manage and define Deep Lake permissions for\njobs that many not be attributed to a specific user.\n\nSet up a Workload Identity using the following steps:\n\n  1. Define an Azure Managed Identity in your cloud\n  2. Attached the Azure Managed Identity to your workload\n  3. Create a Deep Lake Workload Identity using the Azure Managed Identity\n\n  4. Run the workload in Azure\n\n## Step 1: Define the workload identity in Azure\u00b6\n\n  1. Navigate to Managed Identities in Azure\n\n  2. Click `Create` a Managed Identity\n\n  3. Select the `Subscription` and `Resource Group` containing the workload, and give the Managed Identity a `Name`. Click `Review + Create`.",
        "node_22": "All computations run client-side,\nwhich enables users to support lightweight production apps in seconds. Unlike\nChromaDB, Deep Lake\u2019s data format can store raw data such as images, videos,\nand text, in addition to embeddings. ChromaDB is limited to light metadata on\ntop of the embeddings and has no visualization. Deep Lake datasets can be\nvisualized and version controlled. Deep Lake also has a performant dataloader\nfor fine-tuning your Large Language Models.\n\n**Deep Lake vs Pinecone**\n\nBoth Deep Lake and Pinecone enable users to store and search vectors\n(embeddings) and offer integrations with LangChain and LlamaIndex. However,\nthey are architecturally very different. Pinecone is a fully-managed Vector\nDatabase that is optimized for highly demanding applications requiring a\nsearch for billions of vectors. Deep Lake is serverless. All computations run\nclient-side, which enables users to get started in seconds. Unlike Pinecone,\nDeep Lake\u2019s data format can store raw data such as images, videos, and text,\nin addition to embeddings. Deep Lake datasets can be visualized and version\ncontrolled. Pinecone is limited to light metadata on top of the embeddings and\nhas no visualization. Deep Lake also has a performant dataloader for fine-\ntuning your Large Language Models.\n\n**Deep Lake vs Weaviate**\n\nBoth Deep Lake and Weaviate enable users to store and search vectors\n(embeddings) and offer integrations with LangChain and LlamaIndex. However,\nthey are architecturally very different.",
        "node_79": "## Overview\u00b6\n\n**Deep Lake datasets can be stored locally, or on several cloud storage\nproviders including Deep Lake Storage, AWS S3, Microsoft Azure, and Google\nCloud Storage.**\n\nDatasets are accessed by choosing the correct prefix for the dataset `path`\nthat is passed to methods such as deeplake.open(path), and\ndeeplake.create(path).\n\nThe path prefixes are:\n\nStorage Location | Path | Notes  \n---|---|---  \nIn Memory | mem://dataset_id |   \nLocal | file://local_path |   \nDeep Lake Storage | al://org_id/dataset_name |   \nAWS S3 | s3://bucket_name/dataset_name | Dataset can be connected to Deep Lake via Managed Credentials  \nMicrosoft Azure (Gen2 DataLake Only) | az://account_name/container_name/dataset_name | Dataset can be connected to Deep Lake via Managed Credentials  \nGoogle Cloud | gcs://bucket_name/dataset_name | Dataset can be connected to Deep Lake via Managed Credentials  \n  \nTip\n\nConnecting Deep Lake datasets stored in your own cloud via Deep Lake Managed\nCredentials is required for accessing enterprise features, and it\nsignificantly simplifies dataset access.\n\n## Authentication for each cloud storage provider\u00b6\n\n### Activeloop Storage and Managed Datasets\u00b6\n\nIn order to access datasets stored in Deep Lake, or datasets in other clouds\nthat are managed by Activeloop, users must register and authenticate using the\nsteps in the link below in User Authentication\n\n### AWS S3\u00b6\n\nAuthentication with AWS S3 has 4 options:\n\n  1. Use Deep Lake on a machine in the AWS ecosystem that has access to the relevant S3 bucket via AWS IAM, in which case there is no need to pass credentials in order to access datasets in that bucket.\n\n  2. Configure AWS through the cli using `aws configure`. This creates a credentials file on your machine that is automatically access by Deep Lake during authentication.\n\n  3. Save the `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_SESSION_TOKEN` (optional) in environmental variables of the same name, which are loaded as default credentials if no other credentials are specified.\n\n  4.",
        "node_252": "If no version\nis given, the current version will be picked up.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`name` |  `str` |  The name of the branch |  _required_  \n`version` |  `str | None` |  The version of the dataset |  `None`  \n  \n####  `` branches `property` \u00b6\n\n    \n    \n    branches: Branches\n    \n\nThe collection of deeplake.Branchs within the dataset\n\n####  `` merge \u00b6\n\n    \n    \n    merge(branch_name: str, version: str | None = None) -> None\n    \n\nMerge the given branch into the current branch. If no version is given, the\ncurrent version will be picked up.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`name` |  |  The name of the branch |  _required_  \n`version` |  `str | None` |  The version of the dataset |  `None`  \n  \nExamples:\n\n    \n    \n    ds = deeplake.create(\"mem://merge_branch\")\n    ds.add_column(\"c1\", deeplake.types.Int64())\n    ds.append({\"c1\": [1, 2, 3]})\n    ds.commit()\n    \n    b = ds.branch(\"Branch1\")\n    branch_ds = b.open()\n    branch_ds.append({\"c1\": [4, 5, 6]})\n    branch_ds.commit()\n    \n    ds.merge(\"Branch1\")\n    print(len(ds))\n    \n\n####  `` tag \u00b6\n\n    \n    \n    tag(name: str, version: str | None = None) -> Tag\n    \n\nTags a version of the dataset. If no version is given, the current version is\ntagged.",
        "node_82": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.",
        "node_402": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_388": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_456": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_777": "A key difference between Deep Lake and\nTFDS is that Deep Lake datasets are designed for streaming from the cloud,\nwhereas TFDS must be downloaded locally prior to use. As a result, with Deep\nLake, one can import datasets directly from TensorFlow Datasets and stream\nthem either to PyTorch or TensorFlow. In addition to providing access to\npopular publicly available datasets, Deep Lake also offers powerful tools for\ncreating custom datasets, storing them on a variety of cloud storage\nproviders, and collaborating with others via simple API. TFDS is primarily\nfocused on giving the public easy access to commonly available datasets, and\nmanagement of custom datasets is not the primary focus. A full comparison\narticle can be found here.\n\n**Deep Lake vs HuggingFace** Deep Lake and HuggingFace offer access to popular\ndatasets, but Deep Lake primarily focuses on computer vision, whereas\nHuggingFace focuses on natural language processing. HuggingFace Transforms and\nother computational tools for NLP are not analogous to features offered by\nDeep Lake.  **Deep Lake vs WebDatasets** Deep Lake and WebDatasets both offer\nrapid data streaming across networks. They have nearly identical steaming\nspeeds because the underlying network requests and data structures are very\nsimilar. However, Deep Lake offers superior random access and shuffling, its\nsimple API is in python instead of command-line, and Deep Lake enables simple\nindexing and modification of the dataset without having to recreate it.\n**Deep Lake vs Zarr** Deep Lake and Zarr both offer storage of data as chunked\narrays.",
        "node_55": "However,\nthey are architecturally very different. Pinecone is a fully-managed Vector\nDatabase that is optimized for highly demanding applications requiring a\nsearch for billions of vectors. Deep Lake is serverless. All computations run\nclient-side, which enables users to get started in seconds. Unlike Pinecone,\nDeep Lake\u2019s data format can store raw data such as images, videos, and text,\nin addition to embeddings. Deep Lake datasets can be visualized and version\ncontrolled. Pinecone is limited to light metadata on top of the embeddings and\nhas no visualization. Deep Lake also has a performant dataloader for fine-\ntuning your Large Language Models.\n\n**Deep Lake vs Weaviate**\n\nBoth Deep Lake and Weaviate enable users to store and search vectors\n(embeddings) and offer integrations with LangChain and LlamaIndex. However,\nthey are architecturally very different. Weaviate is a Vector Database that\ncan be deployed in a managed service or by the user via Kubernetes or Docker.\nDeep Lake is serverless. All computations run client-side, which enables users\nto support lightweight production apps in seconds. Unlike Weaviate, Deep\nLake\u2019s data format can store raw data such as images, videos, and text, in\naddition to embeddings. Deep Lake datasets can be visualized and version\ncontrolled. Weaviate is limited to light metadata on top of the embeddings and\nhas no visualization. Deep Lake also has a performant dataloader for fine-\ntuning your Large Language Models.",
        "node_237": "* DatasetView \n  * Class Comparison \n    * Dataset \n    * ReadOnlyDataset \n    * DatasetView \n  * Examples \n    * Querying Data \n    * Data Access \n    * Async Operations \n\n# Dataset Classes\u00b6\n\nDeep Lake provides three dataset classes with different access levels:\n\nClass | Description  \n---|---  \nDataset | Full read-write access with all operations  \nReadOnlyDataset | Read-only access to prevent modifications  \nDatasetView | Read-only view of query results  \n  \n## Creation Methods\u00b6\n\n###  `` deeplake.create \u00b6\n\n    \n    \n    create(\n        url: str,\n        creds: dict[str, str] | None = None,\n        token: str | None = None,\n        schema: dict[str, DataType | str | Type] | None = None,\n    ) -> Dataset\n    \n\nCreates a new dataset at the given URL.\n\nTo open an existing dataset, use deeplake.open\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`url` |  `str` |  The URL of the dataset. URLs can be specified using the following protocols:\n\n  * `file://path` local filesystem storage\n  * `al://org_id/dataset_name` A dataset on app.activeloop.ai\n  * `azure://bucket/path` or `az://bucket/path` Azure storage\n  * `gs://bucket/path` or `gcs://bucket/path` or `gcp://bucket/path` Google Cloud storage\n  * `s3://bucket/path` S3 storage\n  * `mem://name` In-memory storage that lasts the life of the process\n\nA URL without a protocol is assumed to be a file:// URL |  _required_  \n`creds` |  `(dict, str)` |  The string `ENV` or a dictionary containing credentials used to access the dataset at the path.",
        "node_177": "Using a `batch_size` of 2, we take the first 10 tables and\nquestions from `table_qa`. For each pair, if `question` is a single string,\nit's converted to a list. The `table_image` is processed in batches, passed\nthrough `processor` and ColPali, and embeddings are generated without\ngradients. These embeddings are stored as lists and appended with each\nquestion and image to `vector_search_images`.Finally,\n`vector_search_images.commit()` saves everything for efficient retrieval.\n\n    \n    \n    batch_size = 8\n    \n    matrix_embeddings: list[torch.Tensor] = []\n    \n    for i in range(0, len(figure_images), batch_size):\n        batch = figure_images[i:i + batch_size]  # Take batch_size images at a time\n        batch_images = processor.process_images(batch).to(model.device)\n        with torch.no_grad():\n            embeddings = model(**batch_images)\n            matrix_embeddings.extend(list(torch.unbind(embeddings.to(\"cpu\"))))\n    \n    # Convert embeddings to list format\n    matrix_embeddings_list = [embedding.tolist() for embedding in matrix_embeddings]\n    \n    # Append question, images, and embeddings to the dataset\n    vector_search_images.append({\n        \"question\": questions,\n        \"image\": [np.array(img).astype(np.uint8) for img in figure_images],\n        \"embedding\": matrix_embeddings_list\n    })\n    \n    # Commit the additions to the dataset\n    vector_search_images.commit()\n    \n\n### Chat with images\u00b6\n\nWe randomly select three questions from `questions` and process them with\n`processor`, sending the batch to the model's device. Embeddings are generated\nwithout gradients and converted to a list format, stored in\n`query_embeddings`.\n\n    \n    \n    queries = [\n        \"At Time (ms) = 0, the membrane potential modeled by n^6 is at -70 ms. If the axis of this graph was extended to t = infinity, what Membrane Voltage would the line modeled by n^6 eventually reach?",
        "node_591": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_370": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_708": "Stephen's Green\n    Review: Good drinks, good food\n    Restaurant name: Eureka! Mountain View\n    Review: Good drinks and burgers\n    Restaurant name: St. Stephen's Green\n    Review: Good drinks an easy going bartenders\n    Restaurant name: Scratch\n    Review: Just had drinks. They were good!\n    Restaurant name: Mifen101 \u82b1\u6eaa\u7c73\u7c89\u738b\n    Review: Feel like I\u2019m back in China.\n    Restaurant name: Ludwigs Biergarten Mountain View\n    Review: Beer is fresh tables are big feel like a proper beer garden\n    Restaurant name: Seasons Noodles & Dumplings Garden\n    Review: Comfort food, excellent service! Feel like back to home.\n    Restaurant name: Casa Lupe\n    Review: Run by a family that makes you feel like part of the family. Awesome food. I love their wet Chili Verde burritos\n    \n\nThis code completes the RAG (Retrieval-Augmented Generation) approach by\ngenerating an LLM-based answer to a user's question, using results retrieved\nin the previous step. Here's how it works:\n\n  1. **Setup and Initialization** : \n\n     * We import `json` for handling JSON responses and initialize the `OpenAI` client to interact with the language model.\n  2. **Define`generate_question` Function**: \n\n     * This function accepts: \n       * `question`: The user's question.\n       * `information`: A list relevant chunks retrieved previously, providing context.\n  3. **System and User Prompts** : \n\n     * The `system_prompt` instructs the model to act as a restaurant assistant, using the provided chunks to answer clearly and without repetition.\n\n     * The model is directed to format its response in JSON.\n\n     * The `user_prompt` combines the user's question and the information chunks.\n\n  4.",
        "node_230": "The API also allows you to update the dataset with new\nannotations.\n\n## Prerequisites\u00b6\n\n    \n    \n    python -m pip install labelbox\n    \n\n## Supported Labelbox Ontologies\u00b6\n\n  * Video Ontology\n\n### Video Ontology\u00b6\n\nFor video ontolgy, python `av` library is used to extract frames from videos.\n\n    \n    \n    python -m pip install av\n    \n\n### Uploading videos for annotation to Labelbox\u00b6\n\nDeeplake supports uploading videos to Labelbox using the Labelbox API.\n\n    \n    \n    from deeplake.integrations import create_labelbox_annotation_project\n    \n    client = labelbox.Client(api_key=LABELBOX_API_KEY)\n    \n    files = [] # list of video urls, can be all local or all remote.\n    \n    # connect the ontology to the project\n    ontology = client.get_ontology('ontology_id_from_labelbox')\n    \n    # create annotation project in labelbox\n    create_labelbox_annotation_project(files, 'dataset-for-deeplake-tests', 'project-for-deeplake-tests', LABELBOX_API_KEY, lb_ontology=ontology)\n    \n\n### Creating a dataset from an annotated Labelbox project\u00b6\n\nTo create a dataset from an annotated Labelbox project, you can use the\nfollowing code:\n\n    \n    \n    from deeplake.integrations import (\n        create_dataset_from_video_annotation_project,\n        converter_for_video_project_with_id\n    )\n    \n    # the path where we want to create the dataset\n    ds_path = \"mem://labelbox_connect_test\"\n    \n    # the project id of the labelbox project that we want to create the dataset from\n    project_id = get_project_id()\n    \n    # we pass the url presigner in cases when the videos are in cloud storage (\n    # for this case azure blob storage) and the videos were added to labelbox with their integrations functionality.\n    # the default one tries to use labelbox api to get the non public remote urls.\n    def url_presigner(url):\n        sas_token = \"<your azure token here>\"\n        # the second value is the headers that will be added to the request\n        return url.partition(\"?\")[0] + \"?\" + sas_token, {}\n    \n    # create the dataset, this will extract the frames from the videos and create the dataset.",
        "node_776": "This feature eliminates the need for additional compressions like zstd, which would otherwise demand more CPU cycles for decompressing on top of formats like jpeg.\n  * **Shuffling:** MDS currently offers more advanced shuffling strategies.\n  * **Version Control & Visualization Support:** A notable feature of Deep Lake is its native version control and in-browser data visualization, a feature not present for MosaicML data format. This can provide significant advantages in managing, understanding, and tracking different versions of the data.\n\n**Deep Lake vs TensorFlow Datasets (TFDS)**\n\nDeep Lake and TFDS seamlessly connect popular datasets to ML frameworks. Deep\nLake datasets are compatible with both PyTorch and TensorFlow, whereas TFDS\nare only compatible with TensorFlow. A key difference between Deep Lake and\nTFDS is that Deep Lake datasets are designed for streaming from the cloud,\nwhereas TFDS must be downloaded locally prior to use. As a result, with Deep\nLake, one can import datasets directly from TensorFlow Datasets and stream\nthem either to PyTorch or TensorFlow. In addition to providing access to\npopular publicly available datasets, Deep Lake also offers powerful tools for\ncreating custom datasets, storing them on a variety of cloud storage\nproviders, and collaborating with others via simple API. TFDS is primarily\nfocused on giving the public easy access to commonly available datasets, and\nmanagement of custom datasets is not the primary focus. A full comparison\narticle can be found here.\n\n**Deep Lake vs HuggingFace** Deep Lake and HuggingFace offer access to popular\ndatasets, but Deep Lake primarily focuses on computer vision, whereas\nHuggingFace focuses on natural language processing.",
        "node_589": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_539": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_140": "'}, score=0.013747293509625419),\n     '5136': Document(id='5136', data={'restaurant_name': 'Scratch', 'restaurant_review': 'Just had drinks. They were good!'}, score=0.024505473374994282),\n     '17426': Document(id='17426', data={'restaurant_name': \"St. Stephen's Green\", 'restaurant_review': 'Good drinks an easy going bartenders'}, score=0.024579178342433523),\n     '17444': Document(id='17444', data={'restaurant_name': \"St. Stephen's Green\", 'restaurant_review': 'Good drinks, good food'}, score=0.02475096426920331),\n     '2496': Document(id='2496', data={'restaurant_name': 'Seasons Noodles & Dumplings Garden', 'restaurant_review': 'Comfort food, excellent service! Feel like back to home.'}, score=0.005430922978171003),\n     '4022': Document(id='4022', data={'restaurant_name': 'Eureka! Mountain View', 'restaurant_review': 'Good drinks and burgers'}, score=0.0246334319067476),\n     '3518': Document(id='3518', data={'restaurant_name': 'Olympus Caffe & Bakery', 'restaurant_review': 'I like the garden to sit down with friends and have a drink.'}, score=0.08915287739833623),\n     '17502': Document(id='17502', data={'restaurant_name': \"St.",
        "node_65": "We do not host or distribute these datasets, vouch for their quality or\nfairness, or claim that you have a license to use the datasets. It is your\nresponsibility to determine whether you have permission to use the datasets\nunder their license.\n\nIf you're a dataset owner and do not want your dataset to be included in this\nlibrary, please get in touch through a GitHub issue. Thank you for your\ncontribution to the ML community!\n\n**Usage Tracking**\n\nBy default, we collect usage data using Bugout (here's the code that does it).\nIt does not collect user data other than anonymized IP address data, and it\nonly logs the Deep Lake library's own actions. This helps our team understand\nhow the tool is used and how to build features that matter to you! After you\nregister with Activeloop, data is no longer anonymous. You can always opt-out\nof reporting by setting an environmental variable `BUGGER_OFF` to `True`:\n\n## Citation\n\nIf you use Deep Lake in your research, please cite Activeloop using:\n\n    \n    \n    @article{deeplake,\n      title = {Deep Lake: a Lakehouse for Deep Learning},\n      author = {Hambardzumyan, Sasun and Tuli, Abhinav and Ghukasyan, Levon and Rahman, Fariz and Topchyan, Hrant and Isayan, David and Harutyunyan, Mikayel and Hakobyan, Tatevik and Stranic, Ivo and Buniatyan, Davit},\n      url = {https://www.cidrdb.org/cidr2023/papers/p69-buniatyan.pdf},\n      booktitle={Proceedings of CIDR},\n      year = {2023},\n    }\n\n## Acknowledgment\n\nThis technology was inspired by our research work at Princeton University.",
        "node_656": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_694": "We then execute both queries, storing vector results in `vs_results` and BM25\nresults in `bm25_results`. This allows us to compare results from both search\nmethods.\n\n    \n    \n    tql_vs = f\"\"\"\n        SELECT *, cosine_similarity(embedding, ARRAY[{embedding_string}]) as score\n        FROM (\n            SELECT *, ROW_NUMBER() AS row_id\n        )\n        ORDER BY cosine_similarity(embedding, ARRAY[{embedding_string}]) DESC \n        LIMIT 5\n    \"\"\"\n    \n    tql_bm25 = f\"\"\"\n        SELECT *, BM25_SIMILARITY(restaurant_review, '{query}') as score\n        FROM (\n            SELECT *, ROW_NUMBER() AS row_id\n        ) \n        ORDER BY BM25_SIMILARITY(restaurant_review, '{query}') DESC \n        LIMIT 5\n    \"\"\"\n    \n    vs_results = vector_search.query(tql_vs)\n    bm25_results = vector_search.query(tql_bm25)\n    print(vs_results)\n    print(bm25_results)\n    \n\nOutput:\n\n    \n    \n    Dataset(columns=(embedding,restaurant_name,restaurant_review,owner_answer,row_id,score), length=5)\n    Dataset(columns=(embedding,restaurant_name,restaurant_review,owner_answer,row_id,score), length=5)\n    \n\n### Show the scores\u00b6\n\n    \n    \n    for el_vs in vs_results:\n        print(f\"vector search score: {el_vs['score']}\")\n    \n    for el_bm25 in bm25_results:\n        print(f\"bm25 score: {el_bm25['score']}\")\n    \n\nOutput:\n\n    \n    \n    vector search score: 0.5322654247283936\n    vector search score: 0.46281781792640686\n    vector search score: 0.4580579102039337\n    vector search score: 0.45585304498672485\n    vector search score: 0.4528498649597168\n    bm25 score: 13.076177597045898\n    bm25 score: 11.206666946411133\n    bm25 score: 11.",
        "node_281": "NOTE: Since binary masks often contain large amounts of data, it is\nrecommended to compress them using lz4.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`sample_compression` |  `str | None` |  How to compress each row's value. Possible values: lz4, null (default: null) |  `None`  \n`chunk_compression` |  `str | None` |  How to compress all the values stored in a single file. Possible values: lz4, null (default: null) |  `None`  \n  \nExamples:\n\n    \n    \n    ds.add_column(\"col1\", types.BinaryMask(sample_compression=\"lz4\"))\n    ds.append([{\"col1\": np.zeros((512, 512, 5), dtype=\"bool\")}])\n    \n    \n    \n    # Basic binary mask\n    ds.add_column(\"masks\", deeplake.types.BinaryMask())\n    \n    # With compression\n    ds.add_column(\"masks\", deeplake.types.BinaryMask(\n        sample_compression=\"lz4\"\n    ))\n    \n\n##  `` deeplake.types.SegmentMask \u00b6\n\n    \n    \n    SegmentMask(\n        dtype: DataType | str = \"uint8\",\n        sample_compression: str | None = None,\n        chunk_compression: str | None = None,\n    ) -> Type\n    \n\nSegmentation masks are 2D representations of class labels where a numerical\nclass value is encoded in an array of same shape as the image.\n\nNOTE: Since segmentation masks often contain large amounts of data, it is\nrecommended to compress them using lz4.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`sample_compression` |  `str | None` |  How to compress each row's value. Possible values: lz4, null (default: null) |  `None`  \n`chunk_compression` |  `str | None` |  How to compress all the values stored in a single file.",
        "node_733": "Risk for sudden death was assessed 1 month after myocardial infarction by a protocol in which exercise and myocardial ischemia were combined; dogs that developed ventricular fibrillation were classified at high risk for sudden death (susceptible) and the survivors were considered low risk (resistant). In resistant dogs, myocardial infarction did not affect any measure of heart rate variability: 1) mean RR interval, 2) standard deviation of the mean RR interval, and 3) the coefficient of variance (standard deviation/RR interval). By contrast, after myocardial infarction, susceptible dogs showed significant decrease in all measures of heart rate variability. Before myocardial infarction, no differences were seen between susceptible and resistant dogs. However, 30 days after infarction, epidemiologic analysis of the coefficient of variance showed high sensitivity and specificity (88% and 80%, respectively), predicting susceptibility.\n    \n\n## 7) Discover Restaurants Using ColPali and the Late Interaction Mechanism\u00b6\n\nIn this final stage, the system uses an **end-to-end neural search** approach\nwith a focus on the **MaxSim** operator, as implemented in ColPali, to improve\nmulti-modal retrieval. MaxSim allows the system to compare different types of\ndata, like text and images, and find the most relevant matches. This helps\nretrieve results that are contextually accurate and meaningful, making it\nespecially useful for complex applications, like scientific and medical\nresearch, where a deep understanding of the content is essential.\n\nRecent advancements in Visual Language Models (VLMs), as highlighted in the\nColPali paper, demonstrate that VLMs can achieve recall rates on document\nretrieval benchmarks comparable to those of traditional OCR pipelines. End-to-\nend learning approaches are positioned to surpass OCR-based methods\nsignificantly. However, representing documents as a `bag of embeddings`\ndemands 30 times more storage than single embeddings. Deep Lake's format,\nwhich inherently supports n-dimensional arrays, enables this storage-intensive\napproach, and the 4.0 query engine introduces MaxSim operations.",
        "node_545": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_791": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_666": "add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"masks\", deeplake.types.BinaryMask())\n    \n\nInsert data into the dataset:\n\n    \n    \n    # Add single samples\n    ds.append([{\n        \"ids\": 1,\n        \"labels\": \"cat\",\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"masks\": mask_array\n    }])\n    \n    # Add batches of data\n    ds.append({\n        \"ids\": [1, 2, 3],\n        \"labels\": [\"cat\", \"dog\", \"bird\"],\n        \"images\": batch_of_images,\n        \"embeddings\": batch_of_embeddings,\n        \"masks\": batch_of_masks\n    })\n    \n    ds.commit() # Commit changes to the storage\n    \n\n## Accessing Data\u00b6\n\nAccess individual samples:\n\n    \n    \n    # Get single items\n    image = ds[\"images\"][0]\n    label = ds[\"labels\"][0]\n    embedding = ds[\"embeddings\"][0]\n    \n    # Get ranges\n    images = ds[\"images\"][0:100]\n    labels = ds[\"labels\"][0:100]\n    \n    # Get specific indices\n    selected_images = ds[\"images\"][[0, 2, 3]]\n    \n\n## Vector Search\u00b6\n\nSearch by embedding similarity:\n\n    \n    \n    # Find similar items\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_114": "**Initialize Lists** : `restaurant_name`, `restaurant_review` and `owner_answer` are initialized to store respective data for each restaurant.\n\n  2. **Populate Lists** : For each entry (`el`) in `scraped_data`, the code appends:\n\n     * `el['restaurant_name']` to `restaurant_name`\n     * `el['restaurant_review']` to `restaurant_review`\n     * `el['owner_answer']` to `owner_answer`\n\nAfter running, each list holds a specific field from all restaurants, ready\nfor further processing.\n\n    \n    \n    restaurant_name = []\n    restaurant_review = []\n    owner_answer = []\n    images = []\n    for el in scraped_data:\n        restaurant_name.append(el['restaurant_name'])\n        restaurant_review.append(el['restaurant_review'])\n        owner_answer.append(el['owner_answer'])\n    \n\n### Add the data to the dataset\u00b6\n\nWe add the collected restaurant names and reviews to the dataset `ds`. Using\n`ds.append()`, we insert two columns: `\"restaurant_name\"` and\n`\"restaurant_review\"`, populated with the values from our lists\n`restaurant_name` and `restaurant_review`. After appending the data,\n`ds.commit()` saves the changes permanently to the dataset, ensuring all new\nentries are stored and ready for further processing.\n\n    \n    \n    ds.append({\n        \"restaurant_name\": restaurant_name,\n        \"restaurant_review\": restaurant_review,\n        \"owner_answer\": owner_answer\n    })\n    ds.commit()\n    print(ds)\n    \n\nOutput:\n\n    \n    \n    Dataset(columns=(restaurant_name,restaurant_review,owner_answer), length=18625)\n    \n\n### Search for the restaurant using a specific word\u00b6\n\nWe define a search query to find any entries in the dataset `ds` where the\nword `\"tapas\"` appears in the `restaurant_review` column. The command\n`ds.query()` runs a TQL query with `SELECT *`, which retrieves all entries\nthat match the condition `CONTAINS(restaurant_review, '{word}')`. This search\nfilters the dataset to show only records containing the specified word\n(`tapas`) in their reviews.",
        "node_643": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_497": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_351": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_736": "The model is loaded using\n`ColPali.from_pretrained()`, with `torch_dtype=torch.bfloat16` for optimized\nmemory use and `\"cuda:0\"` as the device, or `\"mps\"` for Apple Silicon devices.\nAfter loading, we set the model to evaluation mode with `.eval()` to prepare\nit for inference tasks. The `ColPaliProcessor` is also initialized to handle\npreprocessing of images and texts, enabling seamless input preparation for the\nmodel. This setup readies ColPali for high-performance image and document\nprocessing.\n\nThe provided image illustrates the architecture of **ColPali** , a vision-\nlanguage model designed specifically for efficient document retrieval using\nboth visual and textual cues. Here's an overview of its workings and how it's\ndesigned to perform this task efficiently:\n\n  1. **Offline Document Encoding** : \n\n     * On the left side, we see the **offline** processing pipeline, where a document is fed into ColPali's **Vision Language Model (VLM)** .\n\n     * Each document undergoes encoding through a **vision encoder** (to handle images and visual content) and a **language model** (for textual understanding). These two modules generate multi-dimensional embeddings representing both visual and textual aspects of the document.\n\n     * The embeddings are stored in a pre-indexed format, making them ready for fast retrieval during the online phase.\n\n  2. **Online Query Processing** : \n\n     * On the right side, in the **online** section, user queries (such as \"What are ViTs?\") are processed through the **language model** to create a query embedding.\n\n     * ColPali uses a **late interaction mechanism** , where each part of the query embedding is compared with document embeddings through a **MaxSim** operation to find the most similar regions in the document's visual and textual content.\n\n  3. **Similarity Scoring** : \n\n     * ColPali calculates a **similarity score** based on the MaxSim results, which identifies the most relevant documents or document sections matching the query.",
        "node_607": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_309": "schemas.COCOImages(768, objects=True, keypoints=True)\n    schema[\"image_embed\"] = schema.pop(\"embedding\")\n    schema[\"author\"] = types.Text()\n    ds = deeplake.create(\"tmp://\", schema=schema)\n    \n\nAdd a new field to the schema:\n\n    \n    \n    schema = deeplake.schemas.COCOImages(768)\n    schema[\"location\"] = types.Text()\n    ds = deeplake.create(\"tmp://\", schema=schema)\n    \n    \n    \n    # Basic COCO dataset\n    ds = deeplake.create(\"s3://bucket/dataset\",\n        schema=deeplake.schemas.COCOImages(768))\n    \n    # With keypoints and object detection\n    ds = deeplake.create(\"s3://bucket/dataset\",\n        schema=deeplake.schemas.COCOImages(\n            embedding_size=768,\n            keypoints=True,\n            objects=True\n        ))\n    \n    # Customize schema\n    schema = deeplake.schemas.COCOImages(768)\n    schema[\"raw_image\"] = schema.pop(\"image\")\n    schema[\"camera_id\"] = deeplake.types.Text()\n    ds = deeplake.create(\"s3://bucket/dataset\", schema=schema)\n    \n\n## Custom Schema Template\u00b6\n\nCreate custom schema templates:\n\n    \n    \n    # Define custom schema\n    schema = {\n        \"id\": deeplake.types.UInt64(),\n        \"image\": deeplake.types.Image(),\n        \"embedding\": deeplake.types.Embedding(512),\n        \"metadata\": deeplake.types.Dict()\n    }\n    \n    # Create dataset with custom schema\n    ds = deeplake.create(\"s3://bucket/dataset\", schema=schema)\n    \n    # Modify schema\n    schema[\"timestamp\"] = deeplake.types.UInt64()\n    schema.pop(\"metadata\")\n    schema[\"image_embedding\"] = schema.pop(\"embedding\")\n    \n\n## Creating datasets from predefined data formats\u00b6\n\n### from_coco\u00b6\n\nDeep Lake provides a pre-built function to translate COCO format datasets into\nDeep Lake format.",
        "node_742": "\",\n        \"Percent frequency distribution of fiber lengths in cortex and spinal cord by diameter\"\n    ]\n    \n    batch_queries = processor.process_queries(queries).to(model.device)\n    with torch.no_grad():\n        query_embeddings = model(**batch_queries)\n    query_embeddings = query_embeddings.tolist()\n    \n\n### Retrieve the most similar images\u00b6\n\nFor each embedding in `query_embeddings`, we format it as a nested array\nstring for querying. The innermost lists (`q_substrs`) are converted to\n`ARRAY[]` format, and then combined into a single string, `q_str`. This\nformatted string is used in a query on `vector_search_images`, calculating the\n`maxsim` similarity between `q_str` and `embedding`. The query returns the top\n2 results, ordered by similarity score (`score`). This loop performs\nsimilarity searches for each query embedding.\n\n    \n    \n    colpali_results = []\n    n_res = 1\n    \n    for el in query_embeddings:\n        # Convert each sublist of embeddings into a formatted TQL array string\n        q_substrs = [f\"ARRAY[{','.join(str(x) for x in sq)}]\" for sq in el]\n        q_str = f\"ARRAY[{','.join(q_substrs)}]\"\n    \n        # Construct a formatted TQL query\n        tql_colpali = f\"\"\"\n            SELECT *, maxsim(embedding, {q_str}) as score\n            ORDER BY maxsim(embedding, {q_str}) DESC \n            LIMIT {n_res}\n        \"\"\"\n    \n        # Execute the query and append the results\n        colpali_results.append(vector_search_images.query(tql_colpali))\n    \n\nFor each result in `view`, this code prints the `question` text and its\nsimilarity `score`. It then converts the `image` data back to an image format\nwith `Image.fromarray(el[\"image\"])` and displays it using `el_img.show()`.\nThis loop visually presents each query's closest matches alongside their\nsimilarity scores.",
        "node_621": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_516": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_604": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_662": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_73": "commit(\"Added initial data\")\n    \n    # Create version tag\n    ds.tag(\"v1.0\")\n    \n    # View history\n    for version in ds.history:\n        print(version.id, version.message)\n    \n    # Create a new branch\n    ds.branch(\"new-branch\")\n    ### Add new data to the branch .\n    main_ds = ds.branches['main'].open()\n    main_ds.merge(\"new-branch\")\n    \n\n## Async Operations\u00b6\n\nUse async operations for better performance:\n\n    \n    \n    # Async data loading\n    future = ds[\"images\"].get_async(slice(0, 1000))\n    images = future.result()\n    \n    # Async query\n    future = ds.query_async(\n        \"SELECT * WHERE labels = 'cat'\"\n    )\n    cats = future.result()\n    \n\n## Next Steps\u00b6\n\n  * Explore RAG applications\n  * Check out Deep Learning integration\n\n## Support\u00b6\n\nIf you encounter any issues:\n\n  1. Check our GitHub Issues\n  2. Join our Slack Community\n\nBack to top",
        "node_28": "A key difference between Deep Lake and\nTFDS is that Deep Lake datasets are designed for streaming from the cloud,\nwhereas TFDS must be downloaded locally prior to use. As a result, with Deep\nLake, one can import datasets directly from TensorFlow Datasets and stream\nthem either to PyTorch or TensorFlow. In addition to providing access to\npopular publicly available datasets, Deep Lake also offers powerful tools for\ncreating custom datasets, storing them on a variety of cloud storage\nproviders, and collaborating with others via simple API. TFDS is primarily\nfocused on giving the public easy access to commonly available datasets, and\nmanagement of custom datasets is not the primary focus. A full comparison\narticle can be found here.\n\n**Deep Lake vs HuggingFace** Deep Lake and HuggingFace offer access to popular\ndatasets, but Deep Lake primarily focuses on computer vision, whereas\nHuggingFace focuses on natural language processing. HuggingFace Transforms and\nother computational tools for NLP are not analogous to features offered by\nDeep Lake.  **Deep Lake vs WebDatasets** Deep Lake and WebDatasets both offer\nrapid data streaming across networks. They have nearly identical steaming\nspeeds because the underlying network requests and data structures are very\nsimilar. However, Deep Lake offers superior random access and shuffling, its\nsimple API is in python instead of command-line, and Deep Lake enables simple\nindexing and modification of the dataset without having to recreate it.\n**Deep Lake vs Zarr** Deep Lake and Zarr both offer storage of data as chunked\narrays.",
        "node_126": "1. **Generate Embedding for Query** : \n\n     * We call `embedding_function(query)` to generate an embedding for this query. Since `embedding_function` returns a list, we access the first (and only) item with `[0]`, storing the result in `embed_query`.\n  2. **Convert Embedding to String** : \n\n     * We convert `embed_query` (a list of numbers) into a single comma-separated string using `\",\".join(str(c) for c in embed_query)`. This step stores the embedding as a formatted string in `str_query`, preparing it for further processing or use in queries.\n\n    \n    \n    query = \"A restaurant that serves good burritos.\"\n    embed_query = embedding_function(query)[0]\n    str_query = \",\".join(str(c) for c in embed_query)\n    \n\n  1. **Define Query with Cosine Similarity** : \n  2. We construct a TQL query (`query_vs`) to search within the `vector_search` dataset.\n\n  3. The query calculates the **cosine similarity** between the `embedding` column and `str_query`, which is the embedding of our query, `\"A restaurant that serves good burritos.\"`. This similarity score `score` measures how closely each entry matches our query.\n\n  4. **Order by Score and Limit Results** : \n\n  5. The query orders results by `score` in descending order, showing the most relevant matches first. We limit the results to the top 3 matches to focus on the best results.\n\n  6. **Execute Query** : \n\n  7. `vector_search.query(query_vs)` runs the query on the dataset, storing the output in `view_vs`, which contains the top 3 most similar entries based on cosine similarity. This approach helps us retrieve the most relevant records matching our query in `vector_search`.",
        "node_248": "Any existing rows in the dataset will have a `None` value for the new column\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`name` |  `str` |  The name of the column |  _required_  \n`dtype` |  `DataType | str | Type | type | Callable` |  The type of the column. Possible values include:\n\n  * Values from `deeplake.types` such as \"[deeplake.types.Int32][]()\"\n  * Python types: `str`, `int`, `float`\n  * Numpy types: such as `np.int32`\n  * A function reference that returns one of the above types\n\n|  _required_  \n`format` |  `DataFormat` |  The format of the column, if applicable. Only required when the dtype is [deeplake.types.DataType][]. |  _required_  \n  \nExamples:\n\n    \n    \n    ds.add_column(\"labels\", deeplake.types.Int32)\n    \n    ds.add_column(\"categories\", \"int32\")\n    \n    ds.add_column(\"name\", deeplake.types.Text())\n    \n    ds.add_column(\"json_data\", deeplake.types.Dict())\n    \n    ds.add_column(\"images\", deeplake.types.Image(dtype=deeplake.types.UInt8(), sample_compression=\"jpeg\"))\n    \n    ds.add_column(\"embedding\", deeplake.types.Embedding(size=768))\n    \n\nRaises:\n\nType | Description  \n---|---  \n`ColumnAlreadyExistsError` |  If a column with the same name already exists.  \n  \n####  `` append \u00b6\n\n    \n    \n    append(data: list[dict[str, Any]]) -> None\n    \n    \n    \n    append(data: dict[str, Any]) -> None\n    \n    \n    \n    append(data: DatasetView) -> None\n    \n    \n    \n    append(\n        data: (\n            list[dict[str, Any]] | dict[str, Any] | DatasetView\n        ),\n    ) -> None\n    \n\nAdds data to the dataset.",
        "node_408": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_695": "restaurant_name,restaurant_review,owner_answer,row_id,score), length=5)\n    \n\n### Show the scores\u00b6\n\n    \n    \n    for el_vs in vs_results:\n        print(f\"vector search score: {el_vs['score']}\")\n    \n    for el_bm25 in bm25_results:\n        print(f\"bm25 score: {el_bm25['score']}\")\n    \n\nOutput:\n\n    \n    \n    vector search score: 0.5322654247283936\n    vector search score: 0.46281781792640686\n    vector search score: 0.4580579102039337\n    vector search score: 0.45585304498672485\n    vector search score: 0.4528498649597168\n    bm25 score: 13.076177597045898\n    bm25 score: 11.206666946411133\n    bm25 score: 11.023599624633789\n    bm25 score: 10.277934074401855\n    bm25 score: 10.238584518432617\n    \n\nFirst, we import the required libraries and define a Document class, where\neach document has an id, a data dictionary, and an optional score for ranking.\n\n  1. **Setup and Classes** : \n\n     * We import necessary libraries and define a `Document` class using `pydantic.BaseModel`. Each `Document` has an `id`, a `data` dictionary, and an optional `score` for ranking.\n  2. **Softmax Function** : \n\n     * The `softmax` function normalizes a list of scores (`retrieved_score`) using the softmax formula. Scores are exponentiated, limited by `max_weight`, and then normalized to sum up to 1. This returns `new_weights`, a list of normalized scores.",
        "node_199": "First,\ndownload and unzip the small classification dataset below called the _animals\ndataset_.\n\nIn [57]:\n\nCopied!\n\n    \n    \n    # Download dataset\n    from IPython.display import clear_output\n    # !wget https://github.com/activeloopai/examples/blob/main/colabs/starting_data/animals.tar\n    !curl -L -o animals.tar https://github.com/activeloopai/examples/blob/main/colabs/starting_data/animals.tar\n    \n\n# Download dataset from IPython.display import clear_output # !wget\nhttps://github.com/activeloopai/examples/blob/main/colabs/starting_data/animals.tar\n!curl -L -o animals.tar\nhttps://github.com/activeloopai/examples/blob/main/colabs/starting_data/animals.tar\n\n    \n    \n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100  167k    0  167k    0     0   426k      0 --:--:-- --:--:-- --:--:--  425k\n    \n\nIn [56]:\n\nCopied!\n\n    \n    \n    # Unzip to './animals' folder\n    !tar -xvf ./animals.tar\n    \n\n# Unzip to './animals' folder !tar -xvf ./animals.tar\n\n    \n    \n    tar: Error opening archive: Unrecognized archive format\n    \n\nIn [ ]:\n\nCopied!\n\n    \n    \n    animals\n    - cats\n      - image_1.jpg\n      - image_2.jpg\n    - dogs\n      - image_3.jpg\n      - image_4.jpg\n    \n\nanimals \\- cats \\- image_1.jpg \\- image_2.jpg \\- dogs \\- image_3.jpg \\-\nimage_4.jpg\n\nNow that you have the data, you can **create a Deep Lake`Dataset`** and\ninitialize its tensors. Running the following code will create a Deep Lake\ndataset inside of the `./animals_dl` folder.\n\nIn [58]:\n\nCopied!",
        "node_779": "**Deep Lake vs Zarr** Deep Lake and Zarr both offer storage of data as chunked\narrays. However, Deep Lake is primarily designed for returning data as arrays\nusing a simple API, rather than actually storing raw arrays (even though\nthat's also possible). Deep Lake stores data in use-case-optimized formats,\nsuch as jpeg or png for images, or mp4 for video, and the user treats the data\nas if it's an array, because Deep Lake handles all the data processing in\nbetween. Deep Lake offers more flexibility for storing arrays with dynamic\nshape (ragged tensors), and it provides several features that are not naively\navailable in Zarr such as version control, data streaming, and connecting data\nto ML Frameworks.\n\n## Community\n\nJoin our **Slack community** to learn more about unstructured dataset\nmanagement using Deep Lake and to get help from the Activeloop team and other\nusers.\n\nWe'd love your feedback by completing our 3-minute **survey**.\n\nAs always, thanks to our amazing contributors!\n\nMade with contributors-img.\n\nPlease read CONTRIBUTING.md to get started with making contributions to Deep\nLake.\n\n## README Badge\n\nUsing Deep Lake? Add a README badge to let everyone know:\n\n    \n    \n    [![deeplake](https://img.shields.io/badge/powered%20by-Deep%20Lake%20-ff5a1f.svg)](https://github.com/activeloopai/deeplake)\n\n## Disclaimers\n\n**Dataset Licenses**\n\nDeep Lake users may have access to a variety of publicly available datasets.",
        "node_414": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_532": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_172": "This trade-off facilitates\nseamless integration within VLM/LLM frameworks, leading to more accurate and\ngenuinely multimodal responses.\n\nUnlike CLIP, which primarily focuses on aligning visual and text\nrepresentations, ColPali leverages advanced Vision Language Model (VLM)\ncapabilities to deeply understand both textual and visual content. This allows\nColPali to capture rich document structures\u2014like tables, figures, and\nlayouts\u2014directly from images without needing extensive preprocessing steps\nlike OCR or document segmentation. ColPali also utilizes a late interaction\nmechanism, which significantly improves retrieval accuracy by enabling more\ndetailed matching between query elements and document content. These features\nmake ColPali faster, more accurate, and especially effective for visually rich\ndocument retrieval, surpassing CLIP's capabilities in these areas\u200b.\n\nFor more details, see the ColPali paper.\n\n    \n    \n    !pip install colpali-engine accelerate\n    \n\n### Download the ColPali model\u00b6\n\nWe initialize the **ColPali** model and its processor to handle images\nefficiently. The model version is set to `\"vidore/colpali-v1.2\"`, specifying\nthe desired ColPali release. The model is loaded using\n`ColPali.from_pretrained()`, with `torch_dtype=torch.bfloat16` for optimized\nmemory use and `\"cuda:0\"` as the device, or `\"mps\"` for Apple Silicon devices.\nAfter loading, we set the model to evaluation mode with `.eval()` to prepare\nit for inference tasks. The `ColPaliProcessor` is also initialized to handle\npreprocessing of images and texts, enabling seamless input preparation for the\nmodel. This setup readies ColPali for high-performance image and document\nprocessing.\n\nThe provided image illustrates the architecture of **ColPali** , a vision-\nlanguage model designed specifically for efficient document retrieval using\nboth visual and textual cues. Here's an overview of its workings and how it's\ndesigned to perform this task efficiently:\n\n  1.",
        "node_363": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_523": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_61": "A full comparison\narticle can be found here.\n\n**Deep Lake vs HuggingFace** Deep Lake and HuggingFace offer access to popular\ndatasets, but Deep Lake primarily focuses on computer vision, whereas\nHuggingFace focuses on natural language processing. HuggingFace Transforms and\nother computational tools for NLP are not analogous to features offered by\nDeep Lake.  **Deep Lake vs WebDatasets** Deep Lake and WebDatasets both offer\nrapid data streaming across networks. They have nearly identical steaming\nspeeds because the underlying network requests and data structures are very\nsimilar. However, Deep Lake offers superior random access and shuffling, its\nsimple API is in python instead of command-line, and Deep Lake enables simple\nindexing and modification of the dataset without having to recreate it.\n**Deep Lake vs Zarr** Deep Lake and Zarr both offer storage of data as chunked\narrays. However, Deep Lake is primarily designed for returning data as arrays\nusing a simple API, rather than actually storing raw arrays (even though\nthat's also possible). Deep Lake stores data in use-case-optimized formats,\nsuch as jpeg or png for images, or mp4 for video, and the user treats the data\nas if it's an array, because Deep Lake handles all the data processing in\nbetween. Deep Lake offers more flexibility for storing arrays with dynamic\nshape (ragged tensors), and it provides several features that are not naively\navailable in Zarr such as version control, data streaming, and connecting data\nto ML Frameworks.",
        "node_316": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nBest Practices for Deep Lake Usage\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices  Best Practices  Table of contents \n      * Data Ingestion \n        * Do commits for version control only \n        * Prefer creating schema before adding data \n        * Select the right data type for your data \n        * Prefer appending data in batches \n        * Avoid decompressing images when adding them to the dataset \n        * Use Link Image for cloud storage images \n        * Avoid multiprocessing when ingesting data \n        * Avoid too much random updates of the data \n      * Data Access and Querying \n        * Prefer opening datasets in read-only mode.",
        "node_182": "4. **Send API Request** : Using `requests.post`, the function sends the payload to the OpenAI API. If successful, it parses and returns the answer; otherwise, it returns `False`.\n\nThis approach enables an AI-powered visual analysis of images to generate\ncontextually relevant answers.\n\n    \n    \n    import json\n    \n    def generate_VQA(base64_image: str, question:str):\n    \n        system_prompt = f\"\"\"You are a visual language model specialized in analyzing images. Below is an image provided by the user along with a question. Analyze the image carefully, paying attention to details relevant to the question. Construct a clear and informative answer that directly addresses the user's question, based on visual cues.\n    \n        The output must be in JSON format with the following structure:\n        {{\n            \"answer\": \"The answer to the question based on visual analysis.\"\n        }}\n    \n        Here is the question: {question}\n        \"\"\"\n    \n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages = [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": system_prompt},\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                        },\n                    ],\n                }\n            ],\n            response_format={\"type\": \"json_object\"},\n        )\n    \n        try:\n    \n            response = response.choices[0].message.content\n            response = json.loads(response)\n            answer = response[\"answer\"]\n            return answer\n        except Exception as e:\n            print(f\"Error: {e}\")\n            return False\n    \n\nThis code sets `question` to the first item in `queries`, converts the first\nimage in `colpali_results` to an image format, and saves it as `\"image.jpg\"`.",
        "node_740": "Using a `batch_size` of 2, we take the first 10 tables and\nquestions from `table_qa`. For each pair, if `question` is a single string,\nit's converted to a list. The `table_image` is processed in batches, passed\nthrough `processor` and ColPali, and embeddings are generated without\ngradients. These embeddings are stored as lists and appended with each\nquestion and image to `vector_search_images`.Finally,\n`vector_search_images.commit()` saves everything for efficient retrieval.\n\n    \n    \n    batch_size = 8\n    \n    matrix_embeddings: list[torch.Tensor] = []\n    \n    for i in range(0, len(figure_images), batch_size):\n        batch = figure_images[i:i + batch_size]  # Take batch_size images at a time\n        batch_images = processor.process_images(batch).to(model.device)\n        with torch.no_grad():\n            embeddings = model(**batch_images)\n            matrix_embeddings.extend(list(torch.unbind(embeddings.to(\"cpu\"))))\n    \n    # Convert embeddings to list format\n    matrix_embeddings_list = [embedding.tolist() for embedding in matrix_embeddings]\n    \n    # Append question, images, and embeddings to the dataset\n    vector_search_images.append({\n        \"question\": questions,\n        \"image\": [np.array(img).astype(np.uint8) for img in figure_images],\n        \"embedding\": matrix_embeddings_list\n    })\n    \n    # Commit the additions to the dataset\n    vector_search_images.commit()\n    \n\n### Chat with images\u00b6\n\nWe randomly select three questions from `questions` and process them with\n`processor`, sending the batch to the model's device. Embeddings are generated\nwithout gradients and converted to a list format, stored in\n`query_embeddings`.\n\n    \n    \n    queries = [\n        \"At Time (ms) = 0, the membrane potential modeled by n^6 is at -70 ms. If the axis of this graph was extended to t = infinity, what Membrane Voltage would the line modeled by n^6 eventually reach?",
        "node_340": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_785": "### Footer navigation\n\n  * Terms\n  * Privacy\n  * Security\n  * Status\n  * Docs\n  * Contact\n  * Manage cookies \n  * Do not share my personal information \n\nYou can\u2019t perform that action at this time.",
        "node_781": "We do not host or distribute these datasets, vouch for their quality or\nfairness, or claim that you have a license to use the datasets. It is your\nresponsibility to determine whether you have permission to use the datasets\nunder their license.\n\nIf you're a dataset owner and do not want your dataset to be included in this\nlibrary, please get in touch through a GitHub issue. Thank you for your\ncontribution to the ML community!\n\n**Usage Tracking**\n\nBy default, we collect usage data using Bugout (here's the code that does it).\nIt does not collect user data other than anonymized IP address data, and it\nonly logs the Deep Lake library's own actions. This helps our team understand\nhow the tool is used and how to build features that matter to you! After you\nregister with Activeloop, data is no longer anonymous.",
        "node_769": "Current integrations include:\n\n  * **LLM Apps**\n    * Use Deep Lake as a vector store for LLM apps. Our integration combines the Langchain VectorStores API with Deep Lake datasets as the underlying data storage. The integration is a serverless vector store that can be deployed locally or in a cloud of your choice.\n\n## \ud83d\udcda Documentation\n\nGetting started guides, examples, tutorials, API reference, and other useful\ninformation can be found on our documentation page.\n\n## \ud83c\udf93 For Students and Educators\n\nDeep Lake users can access and visualize a variety of popular datasets through\na free integration with Deep Lake's App. Universities can get up to 1TB of\ndata storage and 100,000 monthly queries on the Tensor Database for free per\nmonth. Chat in on our website: to claim the access!\n\n## \ud83d\udc69\u200d\ud83d\udcbb Comparisons to Familiar Tools\n\n**Deep Lake vs Chroma**\n\nBoth Deep Lake & ChromaDB enable users to store and search vectors\n(embeddings) and offer integrations with LangChain and LlamaIndex. However,\nthey are architecturally very different. ChromaDB is a Vector Database that\ncan be deployed locally or on a server using Docker and will offer a hosted\nsolution shortly. Deep Lake is a serverless Vector Store deployed on the\nuser\u2019s own cloud, locally, or in-memory. All computations run client-side,\nwhich enables users to support lightweight production apps in seconds. Unlike\nChromaDB, Deep Lake\u2019s data format can store raw data such as images, videos,\nand text, in addition to embeddings.",
        "node_139": "1. **Initialize Results Dictionary** : \n\n     * We create an empty dictionary, `results`, to store documents with their combined scores from both search methods.\n  2. **Combine Scores** : \n\n     * We iterate over the unique document IDs from `docs_vs` and `docs_bm25`.\n\n     * For each document: \n\n       * We add it to `results`, defaulting to the version available (vector or BM25).\n       * We calculate a weighted score: `vs_score` from vector results (if present in `docs_vs`) and `bm_score` from BM25 results (if present in `docs_bm25`).\n       * The final `results[k].score` is set by adding `vs_score` and `bm_score`.\n\nThis produces a fused score for each document in `results`, ready to rank in\nthe hybrid search.\n\n    \n    \n    def fusion(docs_vs: Dict[str, Document], docs_bm25: Dict[str, Document]) -> Dict[str, Document]:\n        VECTOR_WEIGHT = 0.5\n        LEXICAL_WEIGHT = 0.5\n    \n        results: Dict[str, Dict[str, Document]] = {}\n    \n    \n        for k in set(docs_vs) | set(docs_bm25):\n            results[k] = docs_vs.get(k, None) or docs_bm25.get(k, None)\n            vs_score = VECTOR_WEIGHT * docs_vs[k].score if k in docs_vs else 0\n            bm_score = LEXICAL_WEIGHT * docs_bm25[k].score if k in docs_bm25 else 0\n            results[k].score = vs_score + bm_score\n    \n        return results\n    \n    \n    \n    results = fusion(docs_vs, docs_bm25)\n    print(results)\n    \n\nOutput:\n\n    \n    \n    {'2637': Document(id='2637', data={'restaurant_name': 'Mifen101 \u82b1\u6eaa\u7c73\u7c89\u738b', 'restaurant_review': 'Feel like I\u2019m back in China.'}, score=0.013747293509625419),\n     '5136': Document(id='5136', data={'restaurant_name': 'Scratch', 'restaurant_review': 'Just had drinks.",
        "node_466": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_391": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_66": "We\nwould like to thank William Silversmith @SeungLab for his awesome cloud-volume\ntool.\n\n## About\n\nDatabase for AI. Store Vectors, Images, Texts, Videos, etc. Use with\nLLMs/LangChain. Store, query, version, & visualize any AI data. Stream data in\nreal-time to PyTorch/TensorFlow.",
        "node_650": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_783": "We\nwould like to thank William Silversmith @SeungLab for his awesome cloud-volume\ntool.\n\n## About\n\nDatabase for AI. Store Vectors, Images, Texts, Videos, etc. Use with\nLLMs/LangChain. Store, query, version, & visualize any AI data. Stream data in\nreal-time to PyTorch/TensorFlow.",
        "node_354": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_170": "Risk for sudden death was assessed 1 month after myocardial infarction by a protocol in which exercise and myocardial ischemia were combined; dogs that developed ventricular fibrillation were classified at high risk for sudden death (susceptible) and the survivors were considered low risk (resistant). In resistant dogs, myocardial infarction did not affect any measure of heart rate variability: 1) mean RR interval, 2) standard deviation of the mean RR interval, and 3) the coefficient of variance (standard deviation/RR interval). By contrast, after myocardial infarction, susceptible dogs showed significant decrease in all measures of heart rate variability. Before myocardial infarction, no differences were seen between susceptible and resistant dogs. However, 30 days after infarction, epidemiologic analysis of the coefficient of variance showed high sensitivity and specificity (88% and 80%, respectively), predicting susceptibility.\n    \n\n## 7) Discover Restaurants Using ColPali and the Late Interaction Mechanism\u00b6\n\nIn this final stage, the system uses an **end-to-end neural search** approach\nwith a focus on the **MaxSim** operator, as implemented in ColPali, to improve\nmulti-modal retrieval. MaxSim allows the system to compare different types of\ndata, like text and images, and find the most relevant matches. This helps\nretrieve results that are contextually accurate and meaningful, making it\nespecially useful for complex applications, like scientific and medical\nresearch, where a deep understanding of the content is essential.\n\nRecent advancements in Visual Language Models (VLMs), as highlighted in the\nColPali paper, demonstrate that VLMs can achieve recall rates on document\nretrieval benchmarks comparable to those of traditional OCR pipelines. End-to-\nend learning approaches are positioned to surpass OCR-based methods\nsignificantly. However, representing documents as a `bag of embeddings`\ndemands 30 times more storage than single embeddings. Deep Lake's format,\nwhich inherently supports n-dimensional arrays, enables this storage-intensive\napproach, and the 4.0 query engine introduces MaxSim operations.",
        "node_317": "* Prefer batch access instead of row by row access \n        * Use query for complex data filtering and search \n        * Avoid accessing the data of the whole column \n        * Consider using async data access \n      * Storage and Data Management \n        * Understand the storage differences \n        * Use mem:// for temporary data and testing \n        * Avoid local storage for large datasets \n        * Prefer accessing the cloud storage from the same region \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Data Ingestion \n    * Do commits for version control only \n    * Prefer creating schema before adding data \n    * Select the right data type for your data \n    * Prefer appending data in batches \n    * Avoid decompressing images when adding them to the dataset \n    * Use Link Image for cloud storage images \n    * Avoid multiprocessing when ingesting data \n    * Avoid too much random updates of the data \n  * Data Access and Querying \n    * Prefer opening datasets in read-only mode. \n    * Prefer batch access instead of row by row access \n    * Use query for complex data filtering and search \n    * Avoid accessing the data of the whole column \n    * Consider using async data access \n  * Storage and Data Management \n    * Understand the storage differences \n    * Use mem:// for temporary data and testing \n    * Avoid local storage for large datasets \n    * Prefer accessing the cloud storage from the same region \n\n# Best Practices for Deep Lake Usage\u00b6\n\nDeep Lake supports wide range of data types and powerful tools to ingest, load\nand query your data. This page provides tips for optimizing your usage of Deep\nLake for best experience and performance.\n\n## Data Ingestion\u00b6\n\n### Do commits for version control only\u00b6\n\nWhen adding new data to the dataset, Deep Lake automatically flushes the data\nto the storage. No need to commit each time you add data to the dataset.\nCommit is only needed when you want to create a new version or checkpoint of\nthe dataset.\n\n### Prefer creating schema before adding data\u00b6\n\nIt is recommended to create the dataset schema before ingestion.",
        "node_49": "**Native Compression with Lazy NumPy-like Indexing** Store images, audio, and\nvideos in their native compression. Slice, index, iterate, and interact with\nyour data like a collection of NumPy arrays in your system's memory. Deep Lake\nlazily loads data only when needed, e.g., when training a model or running\nqueries.  **Dataloaders for Popular Deep Learning Frameworks** Deep Lake comes\nwith built-in dataloaders for Pytorch and TensorFlow. Train your model with a\nfew lines of code - we even take care of dataset shuffling. :)  **Integrations\nwith Powerful Tools** Deep Lake has integrations with Langchain and LLamaIndex\nas a vector store for LLM apps, Weights & Biases for data lineage during model\ntraining, MMDetection for training object detection models, and MMSegmentation\nfor training semantic segmentation models.  **100+ most-popular image, video,\nand audio datasets available in seconds** Deep Lake community has uploaded\n100+ image, video and audio datasets like MNIST, COCO, ImageNet, CIFAR, GTZAN\nand others.  **Instant Visualization Support in theDeep Lake App** Deep Lake\ndatasets are instantly visualized with bounding boxes, masks, annotations,\netc. in Deep Lake Visualizer (see below).\n\n## \ud83d\ude80 How to install Deep Lake\n\nDeep Lake can be installed using pip:\n\n    \n    \n    pip install deeplake\n\n### To access all of Deep Lake's features, please register in the Deep Lake\nApp.",
        "node_166": "Each embedding from\nall_vectors is transformed using `.tolist()`, creating list_of_embeddings, and\n`len(list_of_embeddings)` confirms the total count matches the processed text\nentries.\n\n    \n    \n    medical_dataset[\"embedding\"][0:len(list_of_embeddings)] = list_of_embeddings\n    medical_dataset.commit()\n    \n\nThis code performs a semantic search using ColBERT embeddings, leveraging the\nMaxSim operator, executed directly in the cloud (as described in the `index-\non-the-lake` section), for efficient similarity computations.\n\n  1. **Query Embedding** : The query is embedded with `ckpt.queryFromText` and converted into a format compatible with TQL queries.\n\n    \n    \n    query_vectors = ckpt.queryFromText([\"What were the key risk factors for the development of posthemorrhagic/postoperative epilepsy in the study?\"])[0]\n    query_vectors = query_vectors.tolist()\n    \n\n  1. **TQL Query Construction** : The `maxsim` function compares the query embedding to dataset embeddings, ranking results by similarity and limiting them to the top `n_res` matches.\n\n  2. **Query Execution** : `medical_dataset.query` retrieves the most relevant entries based on semantic similarity.",
        "node_438": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_352": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_228": "Migrate in batches with progress tracking\n    for i in range(0, len(source), batch_size):\n        batch = source[i:i+batch_size]\n        ds.append(batch)\n        if i % 10000 == 0:\n            ds.commit()\n    \n\n## Validating Your Migration\u00b6\n\nAfter migration, verify your ML workflows:\n\n  1. Check vector search functionality: \n    \n        # Verify similarity search\n    array_str = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT * \n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{array_str}]) \n        LIMIT 10\n    \"\"\")\n    \n\n  2. Validate ML training pipelines: \n    \n        # Test PyTorch/TensorFlow integration\n    train_loader = ds.pytorch(transform=transforms)\n    \n\n  3. Verify data integrity: \n    \n        # Compare dataset statistics  \n    assert len(old_ds) == len(new_ds)\n    \n\n## Troubleshooting\u00b6\n\nCommon issues during migration:\n\n  * **Memory Issues** : For large datasets, use smaller batch sizes during migration\n  * **Schema Mismatches** : Verify column types match between v3 and v4 datasets \n  * **Missing Embeddings** : Ensure embedding dimensions are correctly specified\n  * **Training Issues** : Update data loading code to use new v4 API\n\nNeed help? Join our Slack Community\n\nBack to top",
        "node_434": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_685": "Wolderful!!!\n    Restaurant name: Los Amigos\n    Review: Fantastic burritos!\n    Restaurant name: Cheztakos!!!\n    Review: Great burritos\n    Restaurant name: La Coste\u00f1a\n    Review: Awesome burritos!\n    Restaurant name: La Coste\u00f1a\n    Review: Awesome burritos\n    Restaurant name: La Coste\u00f1a\n    Review: Bomb burritos\n    \n\n## 3) Create the Dataset and use Vector Similarity Search\u00b6\n\nIf you want to generate text embeddings for similarity search, you can choose\na proprietary model like `text-embedding-3-large` from `OpenAI`, or you can\nopt for an `open-source` model. The MTEB leaderboard on Hugging Face provides\na selection of open-source models that have been tested for their\neffectiveness at converting text into embeddings, which are numerical\nrepresentations that capture the meaning and nuances of words and sentences.\nUsing these embeddings, you can perform similarity search, grouping similar\npieces of text (like sentences or documents) based on their meaning.\n\nSelecting a model from the MTEB leaderboard offers several benefits: these\nmodels are ranked based on performance across a variety of tasks and\nlanguages, ensuring that you're choosing a model that's both accurate and\nversatile. If you prefer not to use a proprietary model, a high-performing\nmodel from this list is an excellent alternative.\n\nWe start by installing and importing the `openai` library to access OpenAI's\nAPI for generating embeddings. Next, we define the function\n`embedding_function`, which takes `texts` as input (either a single string or\na list of strings) and a model name, defaulting to `\"text-embedding-3-large\"`.\nThen, for each text, we replace newline characters with spaces to maintain\nclean, uniform text. Finally, we use `openai.embeddings.create()` to generate\nembeddings for each text and return a list of these embeddings, which can be\nused for cosine similarity comparisons.",
        "node_304": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nSchemas\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas  Schemas  Table of contents \n      * Schema Templates \n      * Text Embeddings Schema \n        * TextEmbeddings \n      * COCO Images Schema \n        * COCOImages \n      * Custom Schema Template \n      * Creating datasets from predefined data formats \n        * from_coco \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Schema Templates \n  * Text Embeddings Schema \n    * TextEmbeddings \n  * COCO Images Schema \n    * COCOImages \n  * Custom Schema Template \n  * Creating datasets from predefined data formats \n    * from_coco \n\n# Schemas\u00b6\n\nDeep Lake provides pre-built schema templates for common data structures.\n\n## Schema Templates\u00b6\n\nSchema templates are Python dictionaries that define the structure of the\ndataset. Each schema template is a dictionary with field names as keys and\nfield types as values.",
        "node_245": "* If 'aws_access_key_id', 'aws_secret_access_key', 'aws_session_token' are present, these take precedence over credentials present in the environment or in credentials file. Currently only works with s3 paths.\n  * It supports 'aws_access_key_id', 'aws_secret_access_key', 'aws_session_token', 'endpoint_url', 'aws_region', 'profile_name' as keys.\n  * To use credentials managed in your Activeloop organization, use they key 'creds_key': 'managed_key_name'. This requires the org_id dataset argument to be set.\n  * If nothing is given is, credentials are fetched from the environment variables. This is also the case when creds is not passed for cloud datasets\n\n|  `None`  \n`token` |  `str` |  Activeloop token to authenticate user. |  `None`  \n  \nExamples:\n\n    \n    \n    ds = deeplake.open_read_only(\"directory_path\")\n    ds.summary()\n    \n    Example Output:\n    Dataset length: 5\n    Columns:\n      id       : int32\n      url      : text\n      embedding: embedding(768)\n    \n    ds = deeplake.open_read_only(\"file:///path/to/dataset\")\n    \n    ds = deeplake.open_read_only(\"s3://bucket/path/to/dataset\")\n    \n    ds = deeplake.open_read_only(\"azure://bucket/path/to/dataset\")\n    \n    ds = deeplake.open_read_only(\"gcs://bucket/path/to/dataset\")\n    \n    ds = deeplake.open_read_only(\"mem://in-memory\")\n    \n\n###  `` deeplake.like \u00b6\n\n    \n    \n    like(\n        src: DatasetView,\n        dest: str,\n        creds: dict[str, str] | None = None,\n        token: str | None = None,\n    ) -> Dataset\n    \n\nCreates a new dataset by copying the `source` dataset's structure to a new\nlocation.\n\nNote\n\nNo data is copied.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`src` |  `DatasetView` |  The dataset to copy the structure from.",
        "node_729": "Each embedding from\nall_vectors is transformed using `.tolist()`, creating list_of_embeddings, and\n`len(list_of_embeddings)` confirms the total count matches the processed text\nentries.\n\n    \n    \n    medical_dataset[\"embedding\"][0:len(list_of_embeddings)] = list_of_embeddings\n    medical_dataset.commit()\n    \n\nThis code performs a semantic search using ColBERT embeddings, leveraging the\nMaxSim operator, executed directly in the cloud (as described in the `index-\non-the-lake` section), for efficient similarity computations.\n\n  1. **Query Embedding** : The query is embedded with `ckpt.queryFromText` and converted into a format compatible with TQL queries.\n\n    \n    \n    query_vectors = ckpt.queryFromText([\"What were the key risk factors for the development of posthemorrhagic/postoperative epilepsy in the study?\"])[0]\n    query_vectors = query_vectors.tolist()\n    \n\n  1. **TQL Query Construction** : The `maxsim` function compares the query embedding to dataset embeddings, ranking results by similarity and limiting them to the top `n_res` matches.\n\n  2. **Query Execution** : `medical_dataset.query` retrieves the most relevant entries based on semantic similarity.",
        "node_96": "Go to the `Storage accounts` page in the Azure UI, which can be done by\nsearching \"Storage accounts\" in the console.\n\n2\\. Select the `Storage account` to which you want to add Application\nCredentials.\n\n3\\. Select `Access Control (IAM)` and click `Add`, and `select Add role\nassignment`.\n\n4\\. Search and select `Storage Blob Data Contributor` under the role names and\nclick `Next`.\n\n5\\. Click on the `Select members` link, and in the tab that opens up on the\nright, search by name and select the application you created in Step 1. Click\n`Select` at the bottom of the page.\n\n6\\. The application should appear in the list of Members, at which point you\ncan click `Review + assign`.\n\n#### Step 2b: Apply the Application Credentials to a specific Azure contained\nin your Azure storage account\u00b6\n\n1\\. Go to the `Storage accounts` page in the Azure UI, which can be done by\nsearching \"Storage accounts\" in the console.\n\n2\\. Select the `Storage account` to which you want to add Application\nCredentials.\n\n3\\. Select the `Container` to which you add the Application Credentials.\n\n4\\. Select `Access Control (IAM)` and click `Add`, and `select Add role\nassignment`.\n\n#### IMPORTANT TO PERFORM STEPS BELOW TO COMPLETE 2b - PLEASE DO NOT SKIP\u00b6\n\n5\\. **Perform substeps 5-7 from Step 2a above, in order to add the Application\nCredentials to the Container**\n\n6\\. **Execute the steps in Step 2a above on your Storage Account, except set\nthe Storage Account Role Assignment to`Storage Blob Delegator` in substep 5.**\n\n## Next Steps\u00b6\n\n  * Enabling CORS in Azure\n\nBack to top",
        "node_447": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_298": "It is returned by the deeplake.Dataset.branches property.\n\n#####  `` __str__ \u00b6\n\n    \n    \n    __str__() -> str\n    \n\n#####  `` __getitem__ \u00b6\n\n    \n    \n    __getitem__(name: str) -> Branch\n    \n\nReturn a branch by name or id\n\n#####  `` __len__ \u00b6\n\n    \n    \n    __len__() -> int\n    \n\nThe number of branches in the dataset\n\n#####  `` names \u00b6\n\n    \n    \n    names() -> list[str]\n    \n\nReturn a list of branch names\n\n    \n    \n    # Create branch\n    ds.branch(\"B1\")\n    \n    # List all branches\n    for name in ds.branches.names():\n        br = ds.branches[name]\n        print(f\"Branch: {br.name} based on {br.base}\")\n    \n    # Check number of branches\n    num_branches = len(ds.branches)\n    \n    # Access specific branch\n    branch = ds.branches[\"main\"]\n    \n    # Common operations with branches\n    branch_ds = ds.branches[\"B1\"].open()  # Open branch\n    \n    # Error handling\n    try:\n        branch = ds.branches[\"non_existent\"]\n    except deeplake.BranchNotFoundError:\n        print(\"Branch not found\")\n    \n\n### BranchView\u00b6\n\n####  `` deeplake.BranchView \u00b6\n\nDescribes a read-only branch within the dataset.",
        "node_26": "An additional\ndistinction is that DVC primarily uses a command-line interface, whereas Deep\nLake is a Python package. Lastly, Deep Lake offers an API to easily connect\ndatasets to ML frameworks and other common ML tools and enables instant\ndataset visualization through Activeloop's visualization tool.\n\n**Deep Lake vs MosaicML MDS format**\n\n  * **Data Storage Format:** Deep Lake operates on a columnar storage format, whereas MDS utilizes a row-wise storage approach. This fundamentally impacts how data is read, written, and organized in each system.\n  * **Compression:** Deep Lake offers a more flexible compression scheme, allowing control over both chunk-level and sample-level compression for each column or tensor. This feature eliminates the need for additional compressions like zstd, which would otherwise demand more CPU cycles for decompressing on top of formats like jpeg.\n  * **Shuffling:** MDS currently offers more advanced shuffling strategies.\n  * **Version Control & Visualization Support:** A notable feature of Deep Lake is its native version control and in-browser data visualization, a feature not present for MosaicML data format. This can provide significant advantages in managing, understanding, and tracking different versions of the data.\n\n**Deep Lake vs TensorFlow Datasets (TFDS)**\n\nDeep Lake and TFDS seamlessly connect popular datasets to ML frameworks. Deep\nLake datasets are compatible with both PyTorch and TensorFlow, whereas TFDS\nare only compatible with TensorFlow. A key difference between Deep Lake and\nTFDS is that Deep Lake datasets are designed for streaming from the cloud,\nwhereas TFDS must be downloaded locally prior to use.",
        "node_667": "\"images\": batch_of_images,\n        \"embeddings\": batch_of_embeddings,\n        \"masks\": batch_of_masks\n    })\n    \n    ds.commit() # Commit changes to the storage\n    \n\n## Accessing Data\u00b6\n\nAccess individual samples:\n\n    \n    \n    # Get single items\n    image = ds[\"images\"][0]\n    label = ds[\"labels\"][0]\n    embedding = ds[\"embeddings\"][0]\n    \n    # Get ranges\n    images = ds[\"images\"][0:100]\n    labels = ds[\"labels\"][0:100]\n    \n    # Get specific indices\n    selected_images = ds[\"images\"][[0, 2, 3]]\n    \n\n## Vector Search\u00b6\n\nSearch by embedding similarity:\n\n    \n    \n    # Find similar items\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n    # Process results - Method 1: iterate through items\n    for item in results:\n        image = item[\"images\"]\n        label = item[\"labels\"]\n    \n    # Process results - Method 2: direct column access\n    images = results[\"images\"][:]\n    labels = results[\"labels\"][:]  # Recommended for better performance\n    \n\n## Data Versioning\u00b6\n\n    \n    \n    # Commit changes\n    ds.commit(\"Added initial data\")\n    \n    # Create version tag\n    ds.tag(\"v1.0\")\n    \n    # View history\n    for version in ds.history:\n        print(version.id, version.message)\n    \n    # Create a new branch\n    ds.branch(\"new-branch\")\n    ### Add new data to the branch .\n    main_ds = ds.branches['main'].open()\n    main_ds.merge(\"new-branch\")\n    \n\n## Async Operations\u00b6\n\nUse async operations for better performance:\n\n    \n    \n    # Async data loading\n    future = ds[\"images\"].get_async(slice(0, 1000))\n    images = future.",
        "node_626": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_256": "####  `` version `property` \u00b6\n\n    \n    \n    version: str\n    \n\nThe currently checked out version of the dataset\n\n####  `` history `property` \u00b6\n\n    \n    \n    history: History\n    \n\nThis dataset's version history\n\n####  `` schema `property` \u00b6\n\n    \n    \n    schema: Schema\n    \n\nThe schema of the dataset.\n\n####  `` indexing_mode `instance-attribute` \u00b6\n\n    \n    \n    indexing_mode: IndexingMode\n    \n\nThe indexing mode of the dataset. This property can be set to change the\nindexing mode of the dataset for the current session, other sessions will not\nbe affected.\n\nExamples:\n\n    \n    \n    ds = deeplake.open(\"mem://ds_id\")\n    ds.indexing_mode = deeplake.IndexingMode.Automatic\n    ds.commit()\n    \n\n## ReadOnlyDataset Class\u00b6\n\nRead-only version of Dataset. Cannot modify data but provides access to all\ndata and metadata.\n\n###  `` deeplake.ReadOnlyDataset \u00b6\n\nBases: `DatasetView`\n\n####  `` query \u00b6\n\n    \n    \n    query(query: str) -> DatasetView\n    \n\nExecutes the given TQL query against the dataset and return the results as a\ndeeplake.DatasetView.\n\nExamples:\n\n    \n    \n    result = ds.query(\"select * where category == 'active'\")\n    for row in result:\n        print(\"Id is: \", row[\"id\"])\n    \n\n####  `` query_async \u00b6\n\n    \n    \n    query_async(query: str) -> Future\n    \n\nAsynchronously executes the given TQL query against the dataset and return a\nfuture that will resolve into deeplake.DatasetView.",
        "node_56": "**Deep Lake vs Weaviate**\n\nBoth Deep Lake and Weaviate enable users to store and search vectors\n(embeddings) and offer integrations with LangChain and LlamaIndex. However,\nthey are architecturally very different. Weaviate is a Vector Database that\ncan be deployed in a managed service or by the user via Kubernetes or Docker.\nDeep Lake is serverless. All computations run client-side, which enables users\nto support lightweight production apps in seconds. Unlike Weaviate, Deep\nLake\u2019s data format can store raw data such as images, videos, and text, in\naddition to embeddings. Deep Lake datasets can be visualized and version\ncontrolled. Weaviate is limited to light metadata on top of the embeddings and\nhas no visualization. Deep Lake also has a performant dataloader for fine-\ntuning your Large Language Models.\n\n**Deep Lake vs DVC**\n\nDeep Lake and DVC offer dataset version control similar to git for data, but\ntheir methods for storing data differ significantly. Deep Lake converts and\nstores data as chunked compressed arrays, which enables rapid streaming to ML\nmodels, whereas DVC operates on top of data stored in less efficient\ntraditional file structures. The Deep Lake format makes dataset versioning\nsignificantly easier compared to traditional file structures by DVC when\ndatasets are composed of many files (i.e., many images). An additional\ndistinction is that DVC primarily uses a command-line interface, whereas Deep\nLake is a Python package.",
        "node_193": "three men walk out of the Zone\",\n            metadata={\n                \"year\": 1979,\n                \"rating\": 9.9,\n                \"director\": \"Andrei Tarkovsky\",\n                \"genre\": \"science fiction\",\n                \"rating\": 9.9,\n            },\n        ),\n    ]\n    \n\nSince this feature uses Deep Lake's Tensor Query Language under the hood, the\nVector Store must be stored in or connected to Deep Lake, which requires\nregistration with Activeloop:\n\n    \n    \n    org_id = <YOUR_ORG_ID>\n    dataset_path = f\"al://{org_id}/self_query\"\n    \n    vectorstore = DeeplakeVectorStore.from_documents(\n        docs, embeddings, dataset_path = dataset_path, overwrite = True,\n    )\n    \n\nNext, let's instantiate our retriever by providing information about the\nmetadata fields that our documents support and a short description of the\ndocument contents.\n\n    \n    \n    from langchain.llms import OpenAI\n    from langchain.retrievers.self_query.base import SelfQueryRetriever\n    from langchain.chains.query_constructor.base import AttributeInfo\n    \n    metadata_field_info = [\n        AttributeInfo(\n            name=\"genre\",\n            description=\"The genre of the movie\",\n            type=\"string or list[string]\",\n        ),\n        AttributeInfo(\n            name=\"year\",\n            description=\"The year the movie was released\",\n            type=\"integer\",\n        ),\n        AttributeInfo(\n            name=\"director\",\n            description=\"The name of the movie director\",\n            type=\"string\",\n        ),\n        AttributeInfo(\n            name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n        ),\n    ]\n    \n    document_content_description = \"Brief summary of a movie\"\n    llm = OpenAI(temperature=0)\n    \n    retriever = SelfQueryRetriever.from_llm(\n        llm, vectorstore, document_content_description, metadata_field_info, verbose=True\n    )\n    \n\nAnd now we can try actually using our retriever!\n\n    \n    \n    # This example only specifies a relevant query\n    retriever.",
        "node_620": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_13": "ai\n\nactiveloop.ai\n\n### License\n\nApache-2.0 license\n\n8.5k stars  656 forks  Branches Tags Activity\n\nStar\n\nNotifications  You must be signed in to change notification settings\n\n  * Code\n  * Issues 49\n  * Pull requests 9\n  * Discussions\n  * Actions\n  * Projects 0\n  * Wiki\n  * Security\n  * Insights\n\nAdditional navigation options\n\n  * Code \n  * Issues \n  * Pull requests \n  * Discussions \n  * Actions \n  * Projects \n  * Wiki \n  * Security \n  * Insights \n\n# activeloopai/deeplake\n\nmain\n\nBranchesTags\n\nGo to file\n\nCode\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\n## History\n\n9,213 Commits  \n.github| .github|  |   \npython/deeplake| python/deeplake|  |   \n.gitignore| .gitignore|  |   \n.pre-commit-config.yaml| .pre-commit-config.yaml|  |   \nCONTRIBUTING.md| CONTRIBUTING.md|  |   \nLICENSE| LICENSE|  |   \nREADME.md| README.md|  |   \nSECURITY.md| SECURITY.md|  |   \nView all files  \n  \n## Repository files navigation\n\n  * README\n  * Apache-2.",
        "node_501": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_717": "1. **Set Device** :\n\n     * We define `device` to use GPU if available, otherwise defaulting to CPU, ensuring compatibility across hardware.\n  2. **Load CLIP Model** :\n\n     * We load the CLIP model (`ViT-B/32`) with its associated preprocessing steps using `clip.load()`. This model is optimized for multi-modal tasks and is set to run on the specified `device`.\n\nThis setup allows us to efficiently process images for embedding, supporting\nmulti-modal applications like image-text similarity.\n\nThe following image illustrates the `CLIP` (Contrastive Language-Image\nPretraining) model's structure, which aligns text and images in a shared\nembedding space, enabling cross-modal understanding.\n\n    \n    \n    import torch\n    import clip\n    \n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n    \n\n### Create the embedding function for images\u00b6\n\nTo prepare images for embedding generation, we define a transformation\npipeline and a function to process images in batches.\n\n  1. **Define Transformations (`tform`)**:\n\n     * The transformation pipeline includes:\n       * **Resize** : Scales images to 224x224 pixels.\n       * **ToTensor** : Converts images to tensor format.\n       * **Lambda** : Ensures grayscale images are replicated across three channels to match the RGB format.\n       * **Normalize** : Standardizes pixel values based on common RGB means and standard deviations.\n  2. **Define`embedding_function_images`**:\n\n     * This function generates embeddings for a list of image.\n     * If `images` is a single filename, it's converted to a list.\n     * **Batch Processing** : Images are processed in batches (default size 4), with transformations applied to each image. The batch is then loaded to the device.\n     * **Embedding Creation** : The model encodes each batch into embeddings, stored in the `embeddings` list, which is returned as a single list.",
        "node_337": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nDataset Copying and Synchronization\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets  Synchronize Datasets  Table of contents \n      * Copying Datasets \n      * Dataset Synchronization \n        * Pull Changes \n        * Push Changes \n      * Synchronization Example \n      * Summary \n\nTable of contents\n\n  * Copying Datasets \n  * Dataset Synchronization \n    * Pull Changes \n    * Push Changes \n  * Synchronization Example \n  * Summary \n\n# Dataset Copying and Synchronization\u00b6\n\nDeep Lake allows copying and synchronizing datasets across different storage\nlocations.",
        "node_518": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_763": "213 Commits  \n.github| .github|  |   \npython/deeplake| python/deeplake|  |   \n.gitignore| .gitignore|  |   \n.pre-commit-config.yaml| .pre-commit-config.yaml|  |   \nCONTRIBUTING.md| CONTRIBUTING.md|  |   \nLICENSE| LICENSE|  |   \nREADME.md| README.md|  |   \nSECURITY.md| SECURITY.md|  |   \nView all files  \n  \n## Repository files navigation\n\n  * README\n  * Apache-2.0 license\n  * Security\n\n  \n\n# Deep Lake: Database for AI\n\n###  **Docs** \u2022 **Get Started** \u2022 **API Reference** \u2022 **LangChain & VectorDBs\nCourse** \u2022 **Blog** \u2022 **Whitepaper** \u2022 **Slack** \u2022 **Twitter**\n\n## What is Deep Lake?\n\nDeep Lake is a Database for AI powered by a storage format optimized for deep-\nlearning applications. Deep Lake can be used for:\n\n  1. Storing and searching data plus vectors while building LLM applications\n  2. Managing datasets while training deep learning models\n\nDeep Lake simplifies the deployment of enterprise-grade LLM-based products by\noffering storage for all data types (embeddings, audio, text, videos, images,\ndicom, pdfs, annotations, and more), querying and vector search, data\nstreaming while training models at scale, data versioning and lineage, and\nintegrations with popular tools such as LangChain, LlamaIndex, Weights &\nBiases, and many more.",
        "node_570": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_625": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_651": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_719": "* **Embedding Creation** : The model encodes each batch into embeddings, stored in the `embeddings` list, which is returned as a single list.\n\nThis function supports efficient, batched embedding generation, useful for\nmulti-modal tasks like image-based search.\n\n    \n    \n    from torchvision import transforms\n    \n    tform = transforms.Compose([\n        transforms.Resize((224,224)), \n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: torch.cat([x, x, x], dim=0) if x.shape[0] == 1 else x),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ])\n    \n    def embedding_function_images(images, model = model, transform = tform, batch_size = 4):\n        \"\"\"Creates a list of embeddings based on a list of image. Images are processed in batches.\"\"\"\n    \n        if isinstance(images, str):\n            images = [images]\n    \n        # Proceess the embeddings in batches, but return everything as a single list\n        embeddings = []\n        for i in range(0, len(images), batch_size):\n            batch = torch.stack([transform(item) for item in images[i:i+batch_size]])\n            batch = batch.to(device)\n            with torch.no_grad():\n                embeddings+= model.encode_image(batch).cpu().numpy().tolist()\n    \n        return embeddings\n    \n\n### Create a new dataset to save the images\u00b6\n\nWe set up a dataset for restaurant images and embeddings. The dataset includes\nan `embedding` column for 512-dimensional image embeddings, a\n`restaurant_name` column for names, and an `image` column for storing images\nin UInt8 format. After defining the structure, `vector_search_images.commit()`\nsaves it, making the dataset ready for storing data for multi-modal search\ntasks with images and metadata.",
        "node_735": "This trade-off facilitates\nseamless integration within VLM/LLM frameworks, leading to more accurate and\ngenuinely multimodal responses.\n\nUnlike CLIP, which primarily focuses on aligning visual and text\nrepresentations, ColPali leverages advanced Vision Language Model (VLM)\ncapabilities to deeply understand both textual and visual content. This allows\nColPali to capture rich document structures\u2014like tables, figures, and\nlayouts\u2014directly from images without needing extensive preprocessing steps\nlike OCR or document segmentation. ColPali also utilizes a late interaction\nmechanism, which significantly improves retrieval accuracy by enabling more\ndetailed matching between query elements and document content. These features\nmake ColPali faster, more accurate, and especially effective for visually rich\ndocument retrieval, surpassing CLIP's capabilities in these areas\u200b.\n\nFor more details, see the ColPali paper.\n\n    \n    \n    !pip install colpali-engine accelerate\n    \n\n### Download the ColPali model\u00b6\n\nWe initialize the **ColPali** model and its processor to handle images\nefficiently. The model version is set to `\"vidore/colpali-v1.2\"`, specifying\nthe desired ColPali release. The model is loaded using\n`ColPali.from_pretrained()`, with `torch_dtype=torch.bfloat16` for optimized\nmemory use and `\"cuda:0\"` as the device, or `\"mps\"` for Apple Silicon devices.\nAfter loading, we set the model to evaluation mode with `.eval()` to prepare\nit for inference tasks. The `ColPaliProcessor` is also initialized to handle\npreprocessing of images and texts, enabling seamless input preparation for the\nmodel. This setup readies ColPali for high-performance image and document\nprocessing.\n\nThe provided image illustrates the architecture of **ColPali** , a vision-\nlanguage model designed specifically for efficient document retrieval using\nboth visual and textual cues. Here's an overview of its workings and how it's\ndesigned to perform this task efficiently:\n\n  1.",
        "node_336": "It utilizes the Deep Lake format under-the-hood,\nand it can be connected to datasets stored in all Deep Lake storage locations.\n\n## Visualization\u00b6\n\nTo visualize your dataset, you need to create an account in the Deep Lake web\nUI. Once you have an account, you can create a new dataset and upload your\ndata. Alternatively you can connect the dataset in your storage location to\nthe Deep Lake web UI.\n\nThe datasets created in the Deep Lake web UI have activeloop url in the format\n`al://organization/dataset`. To visualize your dataset please find the dataset\nin your organization profile or directly visit the\n`https://app.activeloop.ai/organization/dataset`.\n\n## Visualizer Controls and Modes\u00b6\n\nBack to top",
        "node_463": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_78": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nStorage Options\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options  Storage Options  Table of contents \n        * Overview \n        * Authentication for each cloud storage provider \n          * Activeloop Storage and Managed Datasets \n          * AWS S3 \n          * Microsoft Azure \n          * Google Cloud Storage \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Overview \n  * Authentication for each cloud storage provider \n    * Activeloop Storage and Managed Datasets \n    * AWS S3 \n    * Microsoft Azure \n    * Google Cloud Storage \n\n# Storage Options\u00b6\n\nHow to authenticate using Activeloop storage, AWS S3, and Google Cloud\nStorage.\n\n## Overview\u00b6\n\n**Deep Lake datasets can be stored locally, or on several cloud storage\nproviders including Deep Lake Storage, AWS S3, Microsoft Azure, and Google\nCloud Storage.",
        "node_579": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_280": "This is useful for storing raw binary data.\n\nReturns:\n\nName | Type | Description  \n---|---|---  \n`DataType` |  `DataType` |  A new byte array data type.  \n  \nExamples:\n\nCreate columns with byte array type:\n\n    \n    \n    ds.add_column(\"col1\", types.Bytes)\n    ds.add_column(\"col2\", \"bytes\")\n    \n\nAppend raw binary data to a byte array column:\n\n    \n    \n    ds.append([{\"col1\": b\"hello\", \"col2\": b\"world\"}])\n    \n\n##  `` deeplake.types.BinaryMask \u00b6\n\n    \n    \n    BinaryMask(\n        sample_compression: str | None = None,\n        chunk_compression: str | None = None,\n    ) -> Type\n    \n\nIn binary mask, pixel value is a boolean for whether there is/is-not an object\nof a class present.\n\nNOTE: Since binary masks often contain large amounts of data, it is\nrecommended to compress them using lz4.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`sample_compression` |  `str | None` |  How to compress each row's value. Possible values: lz4, null (default: null) |  `None`  \n`chunk_compression` |  `str | None` |  How to compress all the values stored in a single file.",
        "node_196": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nQuickstart\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart  Quickstart  Table of contents \n        * Installing Deep Lake \n        * Opening Your First Deep Lake Dataset \n        * Reading Data \n        * Visualizing Datasets \n        * Creating Your Own Datasets \n\nNotebook\n\n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Installing Deep Lake \n  * Opening Your First Deep Lake Dataset \n  * Reading Data \n  * Visualizing Datasets \n  * Creating Your Own Datasets \n\nNotebook\n\n# Deep Learning QuickStart\u00b6\n\n## Installing Deep Lake\u00b6\n\nDeep Lake can be installed using PyPi.\n\nIn [ ]:\n\nCopied!",
        "node_483": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_399": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_8": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_19": "## \ud83e\udde0 Deep Lake Code Examples by Application\n\n### Vector Store Applications\n\nUsing Deep Lake as a Vector Store for building LLM applications:\n\n### \\- Vector Store Quickstart\n\n### \\- Vector Store Tutorials\n\n### \\- LangChain Integration\n\n### \\- LlamaIndex Integration\n\n### \\- Image Similarity Search with Deep Lake\n\n### Deep Learning Applications\n\nUsing Deep Lake for managing data while training Deep Learning models:\n\n### \\- Deep Learning Quickstart\n\n### \\- Tutorials for Training Models\n\n## \u2699\ufe0f Integrations\n\nDeep Lake offers integrations with other tools in order to streamline your\ndeep learning workflows. Current integrations include:\n\n  * **LLM Apps**\n    * Use Deep Lake as a vector store for LLM apps. Our integration combines the Langchain VectorStores API with Deep Lake datasets as the underlying data storage. The integration is a serverless vector store that can be deployed locally or in a cloud of your choice.\n\n## \ud83d\udcda Documentation\n\nGetting started guides, examples, tutorials, API reference, and other useful\ninformation can be found on our documentation page.\n\n## \ud83c\udf93 For Students and Educators\n\nDeep Lake users can access and visualize a variety of popular datasets through\na free integration with Deep Lake's App. Universities can get up to 1TB of\ndata storage and 100,000 monthly queries on the Tensor Database for free per\nmonth. Chat in on our website: to claim the access!",
        "node_270": "Parameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`index` |  `int | slice | list | tuple` |  Can be:\n\n  * int: Single item index\n  * slice: Range of indices (e.g., 0:10)\n  * list/tuple: Multiple specific indices\n\n|  _required_  \n  \nReturns:\n\nType | Description  \n---|---  \n`ndarray | list | Dict | str | bytes | None` |  The data at the specified index/indices. Type depends on the column's data type.  \n  \nExamples:\n\n    \n    \n    # Get single item\n    image = column[0]\n    \n    # Get range\n    batch = column[0:32]\n    \n    # Get specific indices\n    items = column[[1, 5, 10]]\n    \n\n####  `` get_async \u00b6\n\n    \n    \n    get_async(index: int | slice | list | tuple) -> Future\n    \n\nAsynchronously retrieve data from the column. Useful for large datasets or\nwhen loading multiple items in ML pipelines.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`index` |  `int | slice | list | tuple` |  Can be:\n\n  * int: Single item index\n  * slice: Range of indices\n  * list/tuple: Multiple specific indices\n\n|  _required_  \n  \nReturns:\n\nName | Type | Description  \n---|---|---  \n`Future` |  `Future` |  A Future object that resolves to the requested data.  \n  \nExamples:\n\n    \n    \n    # Async batch load\n    future = column.get_async(slice(0, 32))\n    batch = future.result()\n    \n    # Using with async/await\n    async def load_batch():\n        batch = await column.get_async(slice(0, 32))\n        return batch\n    \n\n####  `` metadata `property` \u00b6\n\n    \n    \n    metadata: ReadOnlyMetadata\n    \n\nAccess the column's metadata. Useful for storing statistics, preprocessing\nparameters, or other information about the column data.",
        "node_760": "# Search code, repositories, users, issues, pull requests.\n\nSearch\n\nClear\n\nSearch syntax tips\n\n#  Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\nInclude my email address so I can be contacted\n\nCancel  Submit feedback\n\n#  Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName\n\nQuery\n\nTo see all available qualifiers, see our documentation.\n\nCancel  Create saved search\n\nSign in\n\nSign up  Reseting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\n{{ message }}\n\nactiveloopai  / **deeplake ** Public\n\n  * Notifications  You must be signed in to change notification settings\n  * Fork 656\n  * Star  8.5k\n\nDatabase for AI. Store Vectors, Images, Texts, Videos, etc. Use with\nLLMs/LangChain. Store, query, version, & visualize any AI data. Stream data in\nreal-time to PyTorch/TensorFlow. https://activeloop.ai\n\nactiveloop.ai\n\n### License\n\nApache-2.0 license\n\n8.",
        "node_533": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_87": "please enable Cross-Origin Resource Sharing (CORS) in the\nbuckets containing the Deep Lake dataset and any linked data, by inserting the\nsnippet below in the CORS section of the Permissions tab for the bucket:\n\n    \n    \n    [\n        {\n            \"AllowedHeaders\": [\n                \"*\"\n            ],\n            \"AllowedMethods\": [\n                \"GET\",\n                \"HEAD\"\n            ],\n            \"AllowedOrigins\": [\n                \"*.activeloop.ai\"\n            ],\n            \"ExposeHeaders\": []\n        }\n    ] \n    \n\n## Next Steps\u00b6\n\n  * Provisioning Role-Based Access\n\nBack to top",
        "node_661": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_712": "information = [f'Review: {el[\"restaurant_review\"]}, Restaurant name: {el[\"restaurant_name\"]}' for el in view_vs]\n    result = generate_question(query, information)\n    print(result)\n    \n\nOutput:\n\n    \n    \n    \"If you're feeling like a drink, consider visiting Taqueria La Espuela, which is known for its refreshing horchata. Alternatively, you might enjoy Chaat Bhavan Mountain View, a great place with good food and a lively atmosphere.\"\n    \n\n### Let's run a search on a multiple dataset\u00b6\n\nIn this approach, we perform the hybrid search across two separate datasets:\n`vector_search` for vector-based search results and `ds_bm25` for BM25-based\ntext search results. This allows us to independently query and retrieve scores\nfrom each dataset, then combine them using the same fusion method as before.\n\n    \n    \n    ds_bm25 = deeplake.open(f\"al://{org_id}/{dataset_name_bm25}\")\n    vs_results = vector_search.query(tql_vs)\n    bm25_results = ds_bm25.query(tql_bm25)\n    \n    \n    \n    vs_score = vs_results[\"score\"]\n    bm_score = bm25_results[\"score\"]\n    \n    vss = softmax(vs_score)\n    bm25s = softmax(bm_score)\n    \n    \n    \n    docs_vs = {}\n    docs_bm25 = {}\n    for el, score in zip(vs_results, vss):\n        docs_vs[str(el[\"row_id\"])] = Document(id=str(el[\"row_id\"]), data={\"restaurant_name\": el[\"restaurant_name\"], \"restaurant_review\": el[\"restaurant_review\"]}, score=score)\n    \n    for el, score in zip(bm25_results, bm25s):\n        docs_bm25[str(el[\"row_id\"])] = Document(id=str(el[\"row_id\"]), data={\"restaurant_name\": el[\"restaurant_name\"], \"restaurant_review\": el[\"restaurant_review\"]}, score=score)\n    \n    \n    \n    results = fusion(docs_vs, docs_bm25)\n    \n    \n    \n    for v in sorted_documents.values():\n        print(f\"Restaurant name: {v.data['restaurant_name']} \\nReview: {v.data['restaurant_review']}\")\n    \n\nOutput:\n\n    \n    \n    Restaurant name: Olympus Caffe & Bakery\n    Review: I like the garden to sit down with friends and have a drink.",
        "node_80": "Use Deep Lake on a machine in the AWS ecosystem that has access to the relevant S3 bucket via AWS IAM, in which case there is no need to pass credentials in order to access datasets in that bucket.\n\n  2. Configure AWS through the cli using `aws configure`. This creates a credentials file on your machine that is automatically access by Deep Lake during authentication.\n\n  3. Save the `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_SESSION_TOKEN` (optional) in environmental variables of the same name, which are loaded as default credentials if no other credentials are specified.\n\n  4. Create a dictionary with the `aws_access_key_id`, `aws_secret_access_key`, and `aws_session_token` (optional), and pass it to Deep Lake using:\n\n    \n    \n    # Low Level API\n    ds = deeplake.open(\"s3://my-bucket/dataset\",\n        creds = {\n        \"aws_access_key_id\": \"my_access_key_id\",\n        \"aws_secret_access_key\": \"my_aws_secret_access_key\",\n        \"aws_session_token\": \"my_aws_session_token\", # Optional\n    })\n    \n\n#### Custom Storage with S3 API\u00b6\n\nIn order to connect to other object storages supporting S3-like API such as\nMinIO, StorageGrid and others, simply add endpoint_url the creds dictionary.\n\n    \n    \n    ds = deeplake.open('s3://...',\n       creds = {\n       'aws_access_key_id': \"my_access_key_id\",\n       'aws_secret_access_key': \"my_aws_secret_access_key\",\n       'aws_session_token': \"my_aws_session_token\", # Optional\n       'endpoint_url': 'http://localhost:8888'\n       })\n    \n\n### Microsoft Azure\u00b6\n\nAuthentication with Microsoft Azure has 4 options:\n\n  1. Log in from your machine's CLI using az login.\n\n  2. Save the `AZURE_STORAGE_ACCOUNT`, `AZURE_STORAGE_KEY`, or other credentials in environmental variables of the same name, which are loaded as default credentials if no other credentials are specified.\n\n  3.",
        "node_417": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_373": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_188": "In\ngeneral, more chunks increases the relevancy of data that is fed into the\nlanguage model, since granular data can be selected with higher precision.\nHowever, since an embedding will be created for each chunk, more chunks\nincrease the computational complexity.\n\n    \n    \n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n    texts = text_splitter.split_documents(docs)\n    \n\n## Creating the Deep Lake Vector Store\u00b6\n\nFirst, we specify a path for storing the Deep Lake dataset containing the\nembeddings and their metadata.\n\n    \n    \n    dataset_path = 'al://<org-id>/twitter_algorithm'\n    \n\nNext, we specify an OpenAI algorithm for creating the embeddings, and create\nthe VectorStore. This process creates an embedding for each element in the\ntexts lists and stores it in Deep Lake format at the specified path.\n\n    \n    \n    embeddings = OpenAIEmbeddings()\n    \n    \n    \n    db = DeeplakeVectorStore.from_documents(dataset_path=dataset_path, embedding=embeddings, documents=texts, overwrite=True)\n    \n\nThe Deep Lake Vector Store has 4 columns including the `texts`, `embeddings`,\n`ids`, and `metadata`.\n\n    \n    \n    ds.dataset.summary()\n    \n    \n    \n    Dataset length: 31305\n    Columns:\n      documents : text\n      embeddings: embedding(1536, clustered)\n      ids       : text\n      metadata  : dict\n    \n\n## Use the Vector Store in a Q&A App\u00b6\n\nWe can now use the VectorStore in Q&A app, where the embeddings will be used\nto filter relevant documents (`texts`) that are fed into an LLM in order to\nanswer a question.\n\nIf we were on another machine, we would load the existing Vector Store without\nrecalculating the embeddings:\n\n    \n    \n    db = DeeplakeVectorStore(dataset_path=dataset_path, read_only=True, embedding_function=embeddings)\n    \n\nWe have to create a `retriever` object and specify the search parameters.",
        "node_526": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_413": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_48": "Deep Lake works with data of any size, it is\nserverless, and it enables you to store all of your data in your own cloud and\nin one place. Deep Lake is used by Intel, Bayer Radiology, Matterport, ZERO\nSystems, Red Cross, Yale, & Oxford.\n\n### Deep Lake includes the following features:\n\n**Multi-Cloud Support (S3, GCP, Azure)** Use one API to upload, download, and\nstream datasets to/from S3, Azure, GCP, Activeloop cloud, local storage, or\nin-memory storage. Compatible with any S3-compatible storage such as MinIO.\n**Native Compression with Lazy NumPy-like Indexing** Store images, audio, and\nvideos in their native compression. Slice, index, iterate, and interact with\nyour data like a collection of NumPy arrays in your system's memory. Deep Lake\nlazily loads data only when needed, e.g., when training a model or running\nqueries.  **Dataloaders for Popular Deep Learning Frameworks** Deep Lake comes\nwith built-in dataloaders for Pytorch and TensorFlow. Train your model with a\nfew lines of code - we even take care of dataset shuffling. :)  **Integrations\nwith Powerful Tools** Deep Lake has integrations with Langchain and LLamaIndex\nas a vector store for LLM apps, Weights & Biases for data lineage during model\ntraining, MMDetection for training object detection models, and MMSegmentation\nfor training semantic segmentation models.",
        "node_701": "I love their wet Chili Verde burritos'}, score=0.04177094836797888)}\n    \n\n### Fusion method\u00b6\n\nWe define weights for our hybrid search: `VECTOR_WEIGHT` and `LEXICAL_WEIGHT`\nare both set to `0.5`, giving equal importance to vector-based and BM25\nscores.\n\n  1. **Initialize Results Dictionary** : \n\n     * We create an empty dictionary, `results`, to store documents with their combined scores from both search methods.\n  2. **Combine Scores** : \n\n     * We iterate over the unique document IDs from `docs_vs` and `docs_bm25`.\n\n     * For each document: \n\n       * We add it to `results`, defaulting to the version available (vector or BM25).\n       * We calculate a weighted score: `vs_score` from vector results (if present in `docs_vs`) and `bm_score` from BM25 results (if present in `docs_bm25`).\n       * The final `results[k].score` is set by adding `vs_score` and `bm_score`.\n\nThis produces a fused score for each document in `results`, ready to rank in\nthe hybrid search.",
        "node_175": "Additionally, its **late interaction** mechanism enables fast and accurate retrieval, optimizing the model for low-latency performance even in large-scale applications\u200b.\n\n    \n    \n    import torch\n    from PIL import Image\n    \n    from colpali_engine.models import ColPali, ColPaliProcessor\n    \n    model_name = \"vidore/colpali-v1.2\"\n    \n    model = ColPali.from_pretrained(\n        model_name,\n        torch_dtype=torch.bfloat16,\n        device_map=\"cuda:0\",  # or \"mps\" if on Apple Silicon\n    ).eval()\n    \n    processor = ColPaliProcessor.from_pretrained(model_name)\n    \n\nWe load the **FigQA** dataset using `deeplake`, specifically retrieving the\n`\"train\"` split of the `\"FigQA\"` subset within the `\"futurehouse/lab-bench\"`\ndataset. This dataset, contains figure data tailored for question-answering\ntasks, making it an ideal input format for the ColPali model. ColPali's\nadvanced capabilities in handling structured and tabular data enable effective\nextraction of answers and insights from these figures, enhancing overall\nperformance on complex, figure-based queries.\n\n    \n    \n    figQA_dataset = \"figQA_dataset\"\n    fig_qa = deeplake.open_read_only(f\"al://activeloop/{figQA_dataset}\")\n    figure_images = [Image.fromarray(el[\"image\"]) for el in fig_qa]\n    questions = [el[\"question\"] for el in  fig_qa]\n    \n\n### Create a new dataset to store the ColPali embeddings\u00b6\n\nWe create a Deep Lake dataset named `\"tabqa_colpali\"` for ColPali's table-\nbased question answering. Stored in `vector_search_images`, it includes an\n`embedding`** column for 2D float arrays, a `question` column for text, and an\n`image` column for table images. After defining the structure,\n`vector_search_images.commit()` saves the setup, optimizing it for ColPali's\nmulti-modal retrieval in table QA tasks.",
        "node_181": "1. **Convert Image to Base64** : The image (`img`) is encoded to a base64 string, allowing it to be embedded in the API request.\n\n  2. **System Prompt** : A structured prompt instructs the model to analyze the image, focusing on visual details that can answer the question.\n\n  3. **Payload and Headers** : The request payload includes the model (`gpt-4o-mini`), the system prompt, and the base64-encoded image. The model is expected to respond in JSON format, specifically returning an `answer` field with insights based on the image.\n\n  4. **Send API Request** : Using `requests.post`, the function sends the payload to the OpenAI API. If successful, it parses and returns the answer; otherwise, it returns `False`.\n\nThis approach enables an AI-powered visual analysis of images to generate\ncontextually relevant answers.\n\n    \n    \n    import json\n    \n    def generate_VQA(base64_image: str, question:str):\n    \n        system_prompt = f\"\"\"You are a visual language model specialized in analyzing images. Below is an image provided by the user along with a question. Analyze the image carefully, paying attention to details relevant to the question. Construct a clear and informative answer that directly addresses the user's question, based on visual cues.\n    \n        The output must be in JSON format with the following structure:\n        {{\n            \"answer\": \"The answer to the question based on visual analysis.\"",
        "node_59": "This feature eliminates the need for additional compressions like zstd, which would otherwise demand more CPU cycles for decompressing on top of formats like jpeg.\n  * **Shuffling:** MDS currently offers more advanced shuffling strategies.\n  * **Version Control & Visualization Support:** A notable feature of Deep Lake is its native version control and in-browser data visualization, a feature not present for MosaicML data format. This can provide significant advantages in managing, understanding, and tracking different versions of the data.\n\n**Deep Lake vs TensorFlow Datasets (TFDS)**\n\nDeep Lake and TFDS seamlessly connect popular datasets to ML frameworks. Deep\nLake datasets are compatible with both PyTorch and TensorFlow, whereas TFDS\nare only compatible with TensorFlow. A key difference between Deep Lake and\nTFDS is that Deep Lake datasets are designed for streaming from the cloud,\nwhereas TFDS must be downloaded locally prior to use. As a result, with Deep\nLake, one can import datasets directly from TensorFlow Datasets and stream\nthem either to PyTorch or TensorFlow. In addition to providing access to\npopular publicly available datasets, Deep Lake also offers powerful tools for\ncreating custom datasets, storing them on a variety of cloud storage\nproviders, and collaborating with others via simple API. TFDS is primarily\nfocused on giving the public easy access to commonly available datasets, and\nmanagement of custom datasets is not the primary focus. A full comparison\narticle can be found here.\n\n**Deep Lake vs HuggingFace** Deep Lake and HuggingFace offer access to popular\ndatasets, but Deep Lake primarily focuses on computer vision, whereas\nHuggingFace focuses on natural language processing.",
        "node_324": "* Google Cloud Storage: `gs://bucket/dataset`.\n\nThese datasets are useful for storing large datasets and sharing them across\nmultiple machines.\n\n### Use `mem://` for temporary data and testing\u00b6\n\nIn memory datasets are not persisted and are useful for temporary data storage\nand testing. They are automatically deleted when the process is terminated.\n\nIf you created in memory dataset and you want to persist it you can use\ndeeplake.copy method to copy the dataset to the local or cloud storage.\n\n### Avoid local storage for large datasets\u00b6\n\nLocal storage shows better latency compared to the cloud dataset. However it\nis not recommended to use local storage for large dataset as with the scale\nthe performance can be degraded.\n\nIf you plan to store large data (>=100GB) cloud storage will be more efficient\nand reliable.\n\nIf you have local dataset and you want to move it to the cloud you can use\ndeeplake.copy method to copy the dataset to the cloud storage.\n\n### Prefer accessing the cloud storage from the same region\u00b6\n\nWhen you are using cloud storage, prefer to access the storage from the same\nregion where the storage is located. This will reduce the latency and improve\nthe performance of the data access.\n\nBack to top",
        "node_273": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nTypes\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types  Types  Table of contents \n      * Numeric Types \n      * Image \n      * Embedding \n      * Text \n      * Dict \n      * Array \n      * Bytes \n      * BinaryMask \n      * SegmentMask \n      * BoundingBox \n      * Point \n      * Medical \n      * Struct \n      * Sequence \n      * Link \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Numeric Types \n  * Image \n  * Embedding \n  * Text \n  * Dict \n  * Array \n  * Bytes \n  * BinaryMask \n  * SegmentMask \n  * BoundingBox \n  * Point \n  * Medical \n  * Struct \n  * Sequence \n  * Link \n\n# Types\u00b6\n\nDeep Lake provides a comprehensive type system designed for efficient data\nstorage and retrieval.",
        "node_741": "Embeddings are generated\nwithout gradients and converted to a list format, stored in\n`query_embeddings`.\n\n    \n    \n    queries = [\n        \"At Time (ms) = 0, the membrane potential modeled by n^6 is at -70 ms. If the axis of this graph was extended to t = infinity, what Membrane Voltage would the line modeled by n^6 eventually reach?\",\n        \"Percent frequency distribution of fiber lengths in cortex and spinal cord by diameter\"\n    ]\n    \n    batch_queries = processor.process_queries(queries).to(model.device)\n    with torch.no_grad():\n        query_embeddings = model(**batch_queries)\n    query_embeddings = query_embeddings.tolist()\n    \n\n### Retrieve the most similar images\u00b6\n\nFor each embedding in `query_embeddings`, we format it as a nested array\nstring for querying. The innermost lists (`q_substrs`) are converted to\n`ARRAY[]` format, and then combined into a single string, `q_str`. This\nformatted string is used in a query on `vector_search_images`, calculating the\n`maxsim` similarity between `q_str` and `embedding`. The query returns the top\n2 results, ordered by similarity score (`score`). This loop performs\nsimilarity searches for each query embedding.",
        "node_752": "Running the following code will create a Deep Lake\ndataset inside of the `./animals_dl` folder.\n\nIn [58]:\n\nCopied!\n\n    \n    \n    import deeplake\n    import numpy as np\n    import os\n    \n    ds = deeplake.create('./animals_dl') # Creates the dataset\n    \n\nimport deeplake import numpy as np import os ds =\ndeeplake.create('./animals_dl') # Creates the dataset\n\nNext, let's inspect the folder structure for the source dataset './animals' to\nfind the class names and the files that need to be uploaded to the Deep Lake\ndataset.\n\nIn [59]:\n\nCopied!\n\n    \n    \n    # Find the class_names and list of files that need to be uploaded\n    dataset_folder = '/Users/istranic/ActiveloopCode/Datasets/animals'\n    \n    # Find the subfolders, but filter additional files like DS_Store that are added on Mac machines.\n    class_names = [item for item in os.listdir(dataset_folder) if os.path.isdir(os.path.join(dataset_folder, item))]\n    \n    files_list = []\n    for dirpath, dirnames, filenames in os.walk(dataset_folder):\n        for filename in filenames:\n            files_list.append(os.path.join(dirpath, filename))\n    \n\n# Find the class_names and list of files that need to be uploaded\ndataset_folder = '/Users/istranic/ActiveloopCode/Datasets/animals' # Find the\nsubfolders, but filter additional files like DS_Store that are added on Mac\nmachines. class_names = [item for item in os.listdir(dataset_folder) if\nos.path.isdir(os.path.join(dataset_folder, item))] files_list = [] for\ndirpath, dirnames, filenames in os.walk(dataset_folder): for filename in\nfilenames: files_list.append(os.path.join(dirpath, filename))\n\nNext, let's **create the dataset columns and upload data**.\n\nIn [ ]:\n\nCopied!",
        "node_578": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_541": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_326": "* Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax  TQL Syntax  Table of contents \n      * Overview \n      * Basic Usage \n        * Dataset Queries \n        * Query Syntax \n      * Vector Operations \n        * Similarity Search \n        * ColPali MAXSIM Search \n        * Text Search \n      * Advanced Features \n        * Cross-Cloud Dataset Joins \n        * Virtual Columns \n        * Logical Operations \n        * Data Sampling \n        * Grouping and Sequences \n      * Built-in Functions \n        * Array Operations \n        * Row Information \n        * Array Logic \n        * Aggregations \n        * RANDOM Numbers \n      * Custom Functions \n    * Visualize Datasets \n    * Synchronize Datasets",
        "node_377": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_540": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_449": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_226": "0 introduces major improvements for ML and AI applications:\n\n  * Enhanced vector similarity search performance\n  * Optimized embedding storage and retrieval \n  * Improved support for large-scale RAG applications\n  * Better handling of multi-modal ML data\n  * Advanced data versioning for ML experiments\n\nThis guide walks through migrating your v3 datasets to take advantage of these\ncapabilities.\n\n## Working with v3 Datasets\u00b6\n\n### Read-Only Access\u00b6\n\nWhile v3 datasets cannot be modified in v4, you can still access them in read-\nonly mode using `deeplake.query()`:\n\n    \n    \n    # Query v3 dataset directly\n    results = deeplake.query('SELECT * FROM \"al://org_name/v3_dataset\"')\n    \n    # Use in ML training pipeline\n    train_data = results.pytorch()\n    val_data = results.tensorflow()\n    \n    # Vector similarity search still works\n    similar = deeplake.query(\"\"\"\n        SELECT * FROM \"al://org_name/v3_dataset\"\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[...]) DESC \n        LIMIT 100\n    \"\"\")\n    \n\nThis allows you to continue using existing v3 datasets while gradually\nmigrating to v4.\n\n## Migration Options\u00b6\n\n### Option 1: Automatic Migration (Recommended)\u00b6\n\nUse the built-in conversion tool to automatically migrate your dataset:\n\n    \n    \n    deeplake.convert(\n        src='al://org_name/v3_dataset', \n        dst='al://org_name/v4_dataset'\n    )\n    \n\n### Option 2: Manual Migration\u00b6\n\nFor custom schemas or complex ML datasets:\n\n    \n    \n    # 1. Create v4 dataset with desired schema\n    ds = deeplake.create(\"s3://new/dataset\")\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"images\", deeplake.types.Image()) \n    ds.commit()\n    \n    # 2. Load v3 data through query\n    source = deeplake.query(f'SELECT * FROM \"{old_ds_path}\"')\n    \n    # 3.",
        "node_365": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_141": "Feel like back to home.'}, score=0.005430922978171003),\n     '4022': Document(id='4022', data={'restaurant_name': 'Eureka! Mountain View', 'restaurant_review': 'Good drinks and burgers'}, score=0.0246334319067476),\n     '3518': Document(id='3518', data={'restaurant_name': 'Olympus Caffe & Bakery', 'restaurant_review': 'I like the garden to sit down with friends and have a drink.'}, score=0.08915287739833623),\n     '17502': Document(id='17502', data={'restaurant_name': \"St. Stephen's Green\", 'restaurant_review': 'Nice place for a drink'}, score=0.02653095210662131),\n     '11383': Document(id='11383', data={'restaurant_name': 'Ludwigs Biergarten Mountain View', 'restaurant_review': 'Beer is fresh tables are big feel like a proper beer garden'}, score=0.011447537567869991),\n     '10788': Document(id='10788', data={'restaurant_name': 'Casa Lupe', 'restaurant_review': 'Run by a family that makes you feel like part of the family. Awesome food. I love their wet Chili Verde burritos'}, score=0.00522136854599736)}\n    \n\nWe sort the results dictionary by each document's combined score in descending\norder, ensuring that the highest-ranking documents appear first.\n\n    \n    \n    sorted_documents = dict(sorted(results.items(), key=lambda item: item[1].score, reverse=True))\n    print(sorted_documents)\n    \n\nOutput:\n\n    \n    \n    {'3518': Document(id='3518', data={'restaurant_name': 'Olympus Caffe & Bakery', 'restaurant_review': 'I like the garden to sit down with friends and have a drink.'}, score=0.3566115095933449),\n     '17502': Document(id='17502', data={'restaurant_name': \"St.",
        "node_543": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_296": "This timestamp is not guaranteed to be accurate, and\ndeeplake.Version.timestamp should generally be used instead.\n\n#####  `` id `property` \u00b6\n\n    \n    \n    id: str\n    \n\nThe unique version identifier\n\n#####  `` message `property` \u00b6\n\n    \n    \n    message: str | None\n    \n\nThe description of the version provided at commit time.\n\n#####  `` open \u00b6\n\n    \n    \n    open() -> ReadOnlyDataset\n    \n\nFetches the dataset corresponding to the version\n\n#####  `` timestamp `property` \u00b6\n\n    \n    \n    timestamp: datetime\n    \n\nThe version timestamp.\n\nThis is based on the storage provider's clock, and so generally more accurate\nthan deeplake.Version.client_timestamp.\n\n    \n    \n    # Get current version\n    version_id = ds.version\n    \n    # Access specific version\n    version = ds.history[version_id]\n    print(f\"Version: {version.id}\")\n    print(f\"Message: {version.message}\")\n    print(f\"Timestamp: {version.timestamp}\")\n    \n    # Open dataset at specific version\n    old_ds = version.open()\n    \n\n## History\u00b6\n\n####  `` deeplake.History \u00b6\n\nThe version history of a deeplake.Dataset.\n\n#####  `` __getitem__ \u00b6\n\n    \n    \n    __getitem__(offset: int) -> Version\n    \n    \n    \n    __getitem__(version: str) -> Version\n    \n    \n    \n    __getitem__(input: int | str) -> Version\n    \n\n#####  `` __iter__ \u00b6\n\n    \n    \n    __iter__() -> Iterator[Version]\n    \n\nIterate over the history, starting at the initial version\n\n#####  `` __len__ \u00b6\n\n    \n    \n    __len__() -> int\n    \n\nThe number of versions within the history\n\n    \n    \n    # View all versions\n    for version in ds.history:\n        print(f\"Version {version.id}: {version.message}\")\n        print(f\"Created: {version.timestamp}\")\n    \n    # Get specific version\n    version = ds.history[version_id]\n    \n    # Get version by index\n    first_version = ds.history[0]\n    latest_version = ds.history[-1]\n    \n\n## Branching\u00b6\n\n### Branch\u00b6\n\n####  `` deeplake.Branch \u00b6\n\nDescribes a branch within the dataset.\n\nBranches are created using deeplake.Dataset.branch.",
        "node_382": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_164": "1. **Dataset Copy and Setup** : \n\n     * The `deeplake.copy()` function duplicates the `medical_dataset` from the Activeloop repository into your organization's workspace.\n\n     * `deeplake.open()` then opens the dataset for modifications, allowing us to add or manipulate columns.\n\n  2. **Adding an Embedding Column** : \n\n     * A new column named `embedding` is added to the dataset with the data type `types.Array(types.Float32(), dimensions=2)`, preparing it to store 2D embeddings generated from the medical text.\n\n    \n    \n    deeplake.copy(f\"al://activeloop/medical_dataset\", f\"al://{org_id}/medical_dataset\")\n    \n    \n    \n    medical_dataset = deeplake.open(f\"al://{org_id}/medical_dataset\")\n    medical_dataset.summary()\n    \n\nOutput:\n\n    \n    \n    Dataset(columns=(text,embedding), length=19719)\n    +---------+---------------------------------------+\n    | column  |                 type                  |\n    +---------+---------------------------------------+\n    |  text   |                 text                  |\n    +---------+---------------------------------------+\n    |embedding|array(dtype=float32, shape=[None,None])|\n    +---------+---------------------------------------+\n    \n    \n    \n    medical_dataset.add_column(name=\"embedding\", dtype=types.Array(types.Float32(),dimensions=2))\n    medical_dataset.commit()\n    \n\n  1. **Text Extraction** : \n\n     * The text data from the medical dataset is extracted into a list (`medical_text`) by iterating over the dataset and pulling the `text` field for each entry.\n  2. **Batch Embedding Generation** : \n\n     * The text data is processed in batches of 1,000 entries using the ColBERT model (`ckpt.docFromText`), which generates embeddings for each batch.\n\n     * The embeddings are appended to a list (`all_vectors`) for later use.\n\n  3. **Efficient Processing** :\n\n     * Batching ensures efficient processing, especially when dealing with large datasets, as it prevents memory overload and speeds up embedding generation.",
        "node_335": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nDataset Visualization\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets  Visualize Datasets  Table of contents \n      * Visualization \n      * Visualizer Controls and Modes \n    * Synchronize Datasets \n\nTable of contents\n\n  * Visualization \n  * Visualizer Controls and Modes \n\n# Dataset Visualization\u00b6\n\nDeep Lake has a web interface for visualizing, versioning, and querying\nmachine learning datasets. It utilizes the Deep Lake format under-the-hood,\nand it can be connected to datasets stored in all Deep Lake storage locations.\n\n## Visualization\u00b6\n\nTo visualize your dataset, you need to create an account in the Deep Lake web\nUI. Once you have an account, you can create a new dataset and upload your\ndata. Alternatively you can connect the dataset in your storage location to\nthe Deep Lake web UI.",
        "node_37": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake? \n\n# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.",
        "node_710": "4. **Generate and Parse the Response** : \n\n     * Using `client.chat.completions.create()`, the system and user prompts are sent to the LLM (specified as `gpt-4o-mini`).\n\n     * The response is parsed as JSON, extracting the `answer` field. If parsing fails, `False` is returned.\n\n    \n    \n    import json\n    from openai import OpenAI\n    \n    client = OpenAI()\n    \n    def generate_question(question:str, information:list):\n        system_prompt = f\"\"\"You are a helpful assistant specialized in providing answers to questions about restaurants. Below is a question from a user, along with the top four relevant information chunks about restaurants from a Deep Lake database. Using these chunks, construct a clear and informative answer that addresses the question, incorporating key details without repeating information.\n        The output must be in JSON format with the following structure:\n        {{\n            \"answer\": \"The answer to the question.\"\n        }}\n    \n        \"\"\"\n    \n        user_prompt = f\"Here is a question from a user: {question}\\n\\nHere are the top relevant information about restaurants {information}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},\n            ],\n            response_format={\"type\": \"json_object\"},\n        )\n    \n        try:\n            response = response.choices[0].message.content\n            response = json.loads(response)\n            questions = response[\"answer\"]\n            return questions\n        except:\n            return False\n    \n\nThis function takes a restaurant-related question and retrieves the best\nresponse based on the given context. It completes the RAG process by combining\nrelevant information and LLM-generated content into a concise answer.",
        "node_94": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nProvisioning Federated Credentials\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials  Provisioning Federated Credentials  Table of contents \n            * Setting up Federated Credentials in Microsoft Azure \n            * Next Steps \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Setting up Federated Credentials in Microsoft Azure \n  * Next Steps \n\n# Provisioning Federated Credentials\u00b6\n\n## Setting up Federated Credentials in Microsoft Azure\u00b6\n\nThe most secure method for connecting data from your Azure storage to Deep\nLake is using Federated Credentials, which are set up using the steps below:\n\n#### Step 1: Register Application Credentials with the Microsoft Identity\nPlatform\u00b6\n\n1\\. Login to the Azure account where the App will be registered and where the\ndata is stored.\n\n2\\.",
        "node_218": "dict(type='LoadAnnotations'),\n        dict(type='Resize', img_scale=(320, 240), ratio_range=(0.5, 2.0)),\n        dict(type='RandomCrop', crop_size=crop_size),\n        dict(type='RandomFlip', flip_ratio=0.5),\n        dict(type='PhotoMetricDistortion'),\n        dict(type='Normalize', **img_norm_cfg),\n        dict(type='Pad', size=crop_size, pad_val=0),\n        dict(type='DefaultFormatBundle'),\n        dict(type='Collect', keys=['img', 'gt_semantic_seg']),\n        ]\n    \n    test_pipeline = [\n        dict(type='LoadImageFromFile'),\n        dict(\n            type='MultiScaleFlipAug',\n            img_scale=(320, 240),\n            # img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n            flip=False,\n            transforms=[\n                dict(type='Resize', img_scale=(320, 240), keep_ratio=True),\n                dict(type='RandomFlip'),\n                dict(type='Normalize', **img_norm_cfg),\n                dict(type='ImageToTensor', keys=['img']),\n                dict(type='Collect', keys=['img']),\n            ])\n    ]\n    \n    evaluation = dict(metric=[\"mIoU\"], interval=10000)\n    \n    data = dict(\n        samples_per_gpu=4,\n        workers_per_gpu=8,\n        train=dict(\n            pipeline=train_pipeline,\n            deeplake_path=\"hub://activeloop/coco-train-seg-mask\",\n            deeplake_tensors = {\"img\": \"images\", \"gt_semantic_seg\": \"seg_masks\"},\n            deeplake_dataloader={\"shuffle\": False, \"num_workers\": 0, \"drop_last\": True}\n        ),\n        val=dict(\n            pipeline=test_pipeline,\n            deeplake_path=\"hub://activeloop/coco-val-seg-mask\"\",\n            deeplake_tensors = {\"img\": \"images\", \"gt_semantic_seg\": \"seg_masks\"},",
        "node_313": "#####  `` __getitem__ \u00b6\n\n    \n    \n    __getitem__(key: str) -> Any\n    \n\nGets metadata value for the given key.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`key` |  `str` |  Metadata key to retrieve |  _required_  \n  \nReturns:\n\nType | Description  \n---|---  \n`Any` |  The stored metadata value  \n  \nExamples:\n\n    \n    \n    mean = ds[\"images\"].metadata[\"mean\"]\n    std = ds[\"images\"].metadata[\"std\"]\n    \n\n#####  `` __setitem__ \u00b6\n\n    \n    \n    __setitem__(key: str, value: Any) -> None\n    \n\nSets metadata value for given key. Changes are persisted immediately.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`key` |  `str` |  Metadata key to set |  _required_  \n`value` |  `Any` |  Value to store |  _required_  \n  \nExamples:\n\n    \n    \n    ds.metadata[\"train_split\"] = 0.8\n    ds.metadata[\"val_split\"] = 0.1\n    ds.metadata[\"test_split\"] = 0.1\n    \n\n#####  `` keys \u00b6\n\n    \n    \n    keys() -> list[str]\n    \n\nLists all available metadata keys.",
        "node_191": "> This section of the tutorial requires installation of additional packages:\n> `pip install deeplake lark`\n\nFirst let's create a Deep Lake Vector Store with relevant data using the\ndocuments below.\n\n    \n    \n    from langchain_core.documents import Document\n    \n    docs = [\n        Document(\n            page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n            metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n        ),\n        Document(\n            page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a .\",\n            metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n        ),\n        Document(\n            page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n            metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n        ),\n        Document(\n            page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n            metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n        ),\n        Document(\n            page_content=\"Toys come alive and have a blast doing so\",\n            metadata={\"year\": 1995, \"genre\": \"animated\"},\n        ),\n        Document(\n            page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n            metadata={\n                \"year\": 1979,\n                \"rating\": 9.9,\n                \"director\": \"Andrei Tarkovsky\",\n                \"genre\": \"science fiction\",\n                \"rating\": 9.9,\n            },\n        ),\n    ]\n    \n\nSince this feature uses Deep Lake's Tensor Query Language under the hood, the\nVector Store must be stored in or connected to Deep Lake,",
        "node_767": "**100+ most-popular image, video,\nand audio datasets available in seconds** Deep Lake community has uploaded\n100+ image, video and audio datasets like MNIST, COCO, ImageNet, CIFAR, GTZAN\nand others.  **Instant Visualization Support in theDeep Lake App** Deep Lake\ndatasets are instantly visualized with bounding boxes, masks, annotations,\netc. in Deep Lake Visualizer (see below).\n\n## \ud83d\ude80 How to install Deep Lake\n\nDeep Lake can be installed using pip:\n\n    \n    \n    pip install deeplake\n\n### To access all of Deep Lake's features, please register in the Deep Lake\nApp.\n\n## \ud83e\udde0 Deep Lake Code Examples by Application\n\n### Vector Store Applications\n\nUsing Deep Lake as a Vector Store for building LLM applications:\n\n### \\- Vector Store Quickstart\n\n### \\- Vector Store Tutorials\n\n### \\- LangChain Integration\n\n### \\- LlamaIndex Integration\n\n### \\- Image Similarity Search with Deep Lake\n\n### Deep Learning Applications\n\nUsing Deep Lake for managing data while training Deep Learning models:\n\n### \\- Deep Learning Quickstart\n\n### \\- Tutorials for Training Models\n\n## \u2699\ufe0f Integrations\n\nDeep Lake offers integrations with other tools in order to streamline your\ndeep learning workflows. Current integrations include:\n\n  * **LLM Apps**\n    * Use Deep Lake as a vector store for LLM apps. Our integration combines the Langchain VectorStores API with Deep Lake datasets as the underlying data storage. The integration is a serverless vector store that can be deployed locally or in a cloud of your choice.",
        "node_490": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_499": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_272": "Useful for storing statistics, preprocessing\nparameters, or other information about the column data.\n\nReturns:\n\nName | Type | Description  \n---|---|---  \n`ReadOnlyMetadata` |  `ReadOnlyMetadata` |  A ReadOnlyMetadata object for reading metadata.  \n  \nExamples:\n\n    \n    \n    # Access preprocessing parameters\n    mean = column.metadata[\"mean\"]\n    std = column.metadata[\"std\"]\n    \n    # Check available metadata\n    for key in column.metadata.keys():\n        print(f\"{key}: {column.metadata[key]}\")\n    \n\n####  `` name `property` \u00b6\n\n    \n    \n    name: str\n    \n\nGet the name of the column.\n\nReturns:\n\nName | Type | Description  \n---|---|---  \n`str` |  `str` |  The column name.  \n  \n## Class Comparison\u00b6\n\n### Column\u00b6\n\n  * Provides read-write access\n  * Can modify data\n  * Can update metadata\n  * Available in Dataset\n\n    \n    \n    # Get mutable column\n    ds = deeplake.open(\"s3://bucket/dataset\")\n    column = ds[\"images\"]\n    \n    # Read data\n    image = column[0]\n    batch = column[0:100]\n    \n    # Write data\n    column[0] = new_image\n    column[0:100] = new_batch\n    \n    # Async operations\n    future = column.set_async(0, new_image)\n    future.wait()\n    \n\n### ColumnView\u00b6\n\n  * Read-only access\n  * Cannot modify data\n  * Can read metadata\n  * Available in ReadOnlyDataset and DatasetView\n\n    \n    \n    # Get read-only column\n    ds = deeplake.open_read_only(\"s3://bucket/dataset\")\n    column = ds[\"images\"]\n    \n    # Read data\n    image = column[0]\n    batch = column[0:100]\n    \n    # Async read\n    future = column.get_async(slice(0, 100))\n    batch = future.result()\n    \n\n## Examples\u00b6\n\n### Data Access\u00b6\n\n    \n    \n    # Direct indexing\n    single_item = column[0]\n    batch = column[0:100]\n    selected = column[[1, 5, 10]]\n    \n    # Async data access \n    future = column.get_async(slice(0, 1000))\n    data = future.result()\n    \n\n### Metadata\u00b6\n\n    \n    \n    # Read metadata from any column type\n    name = column.name\n    metadata = column.metadata\n    \n    # Update metadata (Column only)\n    column.metadata[\"mean\"] = [0.485, 0.456, 0.406]\n    column.metadata[\"std\"] = [0.229, 0.224, 0.225]\n    \n\nBack to top",
        "node_263": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.",
        "node_544": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_311": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nMetadata\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata  Metadata  Table of contents \n      * Dataset Metadata \n      * Column Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Dataset Metadata \n  * Column Metadata \n\n# Metadata\u00b6\n\nMetadata provides key-value storage for datasets and columns.\n\n## Dataset Metadata\u00b6\n\n####  `` deeplake.Metadata \u00b6\n\nBases: `ReadOnlyMetadata`\n\nWritable access to dataset and column metadata for ML workflows.\n\nStores important information about datasets like:\n\n  * Model parameters and hyperparameters\n  * Preprocessing statistics\n  * Data splits and fold definitions\n  * Version and training information\n\nChanges are persisted immediately without requiring `commit()`.",
        "node_437": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_251": "See deeplake.Dataset.commit for more information.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`message` |  `str` |  A message to store in history describing the changes made in the commit |  `None`  \n  \nExamples:\n\n    \n    \n    ds.commit_async().wait()\n    \n    ds.commit_async(\"Added data from updated documents\").wait()\n    \n    async def do_commit():\n        await ds.commit_async()\n    \n    future = ds.commit_async() # then you can check if the future is completed using future.is_completed()\n    \n\n####  `` current_branch `property` \u00b6\n\n    \n    \n    current_branch: Branch\n    \n\nThe current active branch\n\n####  `` branch \u00b6\n\n    \n    \n    branch(name: str, version: str | None = None) -> Branch\n    \n\nCreates a branch with the given version of the current branch. If no version\nis given, the current version will be picked up.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`name` |  `str` |  The name of the branch |  _required_  \n`version` |  `str | None` |  The version of the dataset |  `None`  \n  \n####  `` branches `property` \u00b6\n\n    \n    \n    branches: Branches\n    \n\nThe collection of deeplake.Branchs within the dataset\n\n####  `` merge \u00b6\n\n    \n    \n    merge(branch_name: str, version: str | None = None) -> None\n    \n\nMerge the given branch into the current branch. If no version is given, the\ncurrent version will be picked up.",
        "node_18": "**100+ most-popular image, video,\nand audio datasets available in seconds** Deep Lake community has uploaded\n100+ image, video and audio datasets like MNIST, COCO, ImageNet, CIFAR, GTZAN\nand others.  **Instant Visualization Support in theDeep Lake App** Deep Lake\ndatasets are instantly visualized with bounding boxes, masks, annotations,\netc. in Deep Lake Visualizer (see below).\n\n## \ud83d\ude80 How to install Deep Lake\n\nDeep Lake can be installed using pip:\n\n    \n    \n    pip install deeplake\n\n### To access all of Deep Lake's features, please register in the Deep Lake\nApp.\n\n## \ud83e\udde0 Deep Lake Code Examples by Application\n\n### Vector Store Applications\n\nUsing Deep Lake as a Vector Store for building LLM applications:\n\n### \\- Vector Store Quickstart\n\n### \\- Vector Store Tutorials\n\n### \\- LangChain Integration\n\n### \\- LlamaIndex Integration\n\n### \\- Image Similarity Search with Deep Lake\n\n### Deep Learning Applications\n\nUsing Deep Lake for managing data while training Deep Learning models:\n\n### \\- Deep Learning Quickstart\n\n### \\- Tutorials for Training Models\n\n## \u2699\ufe0f Integrations\n\nDeep Lake offers integrations with other tools in order to streamline your\ndeep learning workflows. Current integrations include:\n\n  * **LLM Apps**\n    * Use Deep Lake as a vector store for LLM apps. Our integration combines the Langchain VectorStores API with Deep Lake datasets as the underlying data storage. The integration is a serverless vector store that can be deployed locally or in a cloud of your choice.",
        "node_30": "**Deep Lake vs Zarr** Deep Lake and Zarr both offer storage of data as chunked\narrays. However, Deep Lake is primarily designed for returning data as arrays\nusing a simple API, rather than actually storing raw arrays (even though\nthat's also possible). Deep Lake stores data in use-case-optimized formats,\nsuch as jpeg or png for images, or mp4 for video, and the user treats the data\nas if it's an array, because Deep Lake handles all the data processing in\nbetween. Deep Lake offers more flexibility for storing arrays with dynamic\nshape (ragged tensors), and it provides several features that are not naively\navailable in Zarr such as version control, data streaming, and connecting data\nto ML Frameworks.\n\n## Community\n\nJoin our **Slack community** to learn more about unstructured dataset\nmanagement using Deep Lake and to get help from the Activeloop team and other\nusers.\n\nWe'd love your feedback by completing our 3-minute **survey**.\n\nAs always, thanks to our amazing contributors!\n\nMade with contributors-img.\n\nPlease read CONTRIBUTING.md to get started with making contributions to Deep\nLake.\n\n## README Badge\n\nUsing Deep Lake? Add a README badge to let everyone know:\n\n    \n    \n    [![deeplake](https://img.shields.io/badge/powered%20by-Deep%20Lake%20-ff5a1f.svg)](https://github.com/activeloopai/deeplake)\n\n## Disclaimers\n\n**Dataset Licenses**\n\nDeep Lake users may have access to a variety of publicly available datasets.",
        "node_384": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake? \n\n# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.",
        "node_392": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_364": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_52": "Current integrations include:\n\n  * **LLM Apps**\n    * Use Deep Lake as a vector store for LLM apps. Our integration combines the Langchain VectorStores API with Deep Lake datasets as the underlying data storage. The integration is a serverless vector store that can be deployed locally or in a cloud of your choice.\n\n## \ud83d\udcda Documentation\n\nGetting started guides, examples, tutorials, API reference, and other useful\ninformation can be found on our documentation page.\n\n## \ud83c\udf93 For Students and Educators\n\nDeep Lake users can access and visualize a variety of popular datasets through\na free integration with Deep Lake's App. Universities can get up to 1TB of\ndata storage and 100,000 monthly queries on the Tensor Database for free per\nmonth. Chat in on our website: to claim the access!\n\n## \ud83d\udc69\u200d\ud83d\udcbb Comparisons to Familiar Tools\n\n**Deep Lake vs Chroma**\n\nBoth Deep Lake & ChromaDB enable users to store and search vectors\n(embeddings) and offer integrations with LangChain and LlamaIndex. However,\nthey are architecturally very different. ChromaDB is a Vector Database that\ncan be deployed locally or on a server using Docker and will offer a hosted\nsolution shortly. Deep Lake is a serverless Vector Store deployed on the\nuser\u2019s own cloud, locally, or in-memory. All computations run client-side,\nwhich enables users to support lightweight production apps in seconds. Unlike\nChromaDB, Deep Lake\u2019s data format can store raw data such as images, videos,\nand text, in addition to embeddings.",
        "node_306": "This schema includes the following fields: \\- id (uint64): Unique identifier\nfor each entry. \\- chunk_index (uint16): Position of the text chunk within the\ndocument. \\- document_id (uint64): Unique identifier for the document the\nembedding came from. \\- date_created (uint64): Timestamp when the document was\nread. \\- text_chunk (text): The text of the shard. \\- embedding\n(dtype=float32, size=embedding_size): The embedding of the text.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`embedding_size` |  `int` |  int Size of the embeddings. |  _required_  \n`quantize` |  `bool` |  bool, optional If true, quantize the embeddings to slightly decrease accuracy while greatly increasing query speed. Default is False. |  `False`  \n  \nExamples:\n\nCreate a dataset with the standard schema:\n\n    \n    \n    ds = deeplake.create(\"tmp://\", schema=deeplake.schemas.TextEmbeddings(768))\n    \n\nCustomize the schema before creating the dataset:\n\n    \n    \n    schema = deeplake.schemas.TextEmbeddings(768)\n    schema[\"text_embed\"] = schema.pop(\"embedding\")\n    schema[\"author\"] = types.Text()\n    ds = deeplake.create(\"tmp://\", schema=schema)\n    \n\nAdd a new field to the schema:\n\n    \n    \n    schema = deeplake.schemas.TextEmbeddings(768)\n    schema[\"language\"] = types.Text()\n    ds = deeplake.create(\"tmp://\", schema=schema)\n    \n    \n    \n    # Create dataset with text embeddings schema\n    ds = deeplake.create(\"s3://bucket/dataset\",\n        schema=deeplake.schemas.TextEmbeddings(768))\n    \n    # Customize before creation\n    schema = deeplake.schemas.TextEmbeddings(768)\n    schema[\"text_embedding\"] = schema.pop(\"embedding\")\n    schema[\"source\"] = deeplake.types.Text()\n    ds = deeplake.create(\"s3://bucket/dataset\", schema=schema)\n    \n    # Add field to existing schema\n    schema = deeplake.schemas.TextEmbeddings(768)\n    schema[\"language\"] = deeplake.types.Text()\n    ds = deeplake.create(\"s3://bucket/dataset\", schema=schema)\n    \n\n## COCO Images Schema\u00b6\n\n###  `` deeplake.schemas.COCOImages \u00b6\n\n    \n    \n    COCOImages(\n        embedding_size: int,\n        quantize: bool = False,\n        objects: bool = True,\n        keypoints: bool = False,\n        stuffs: bool = False,\n    ) -> dict[str, DataType | str | Type]\n    \n\nA schema for storing COCO-based image data.",
        "node_286": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nQuery\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query  Query  Table of contents \n      * Query Functions \n        * query \n        * query_async \n      * Vector Search \n      * Text Search \n      * Array Operations \n      * Joining Datasets \n      * Filtering \n      * Query Results \n      * Async Queries \n      * Querying Views \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Query Functions \n    * query \n    * query_async \n  * Vector Search \n  * Text Search \n  * Array Operations \n  * Joining Datasets \n  * Filtering \n  * Query Results \n  * Async Queries \n  * Querying Views \n\n# Query\u00b6\n\nDeep Lake provides powerful query capabilities through its Tensor Query\nLanguage (TQL), with special focus on vector similarity search, text search,\nand operations on multidimensional arrays.",
        "node_679": "You can find the official documentation\nhere.\n\n    \n    \n    word = 'burritos'\n    view = ds.query(f\"\"\"\n        SELECT * \n        WHERE CONTAINS(restaurant_review, '{word}')\n        LIMIT 4\n    \"\"\")\n    print(view)\n    \n\nOutput:\n\n    \n    \n    Dataset(columns=(restaurant_name,restaurant_review,owner_answer), length=4)\n    \n\n### Show the results\u00b6\n\n    \n    \n    for row in view:\n        print(f\"Restaurant name: {row['restaurant_name']} \\nReview: {row['restaurant_review']}\")\n    \n\nOutput:\n\n    \n    \n    Restaurant name: Los Amigos\n    Review: Best Burritos i have ever tried!!!!! Wolderful!!!\n    Restaurant name: Los Amigos\n    Review: Really good breakfast burrito, and just burritos in general\n    Restaurant name: Los Amigos\n    Review: Ordered two of their veggie burritos, nothing crazy just added extra cheese and sour cream. They even repeated the order back to me and everything was fine, then when I picked the burritos up and got home they put zucchini and squash in it.. like what??\n    Restaurant name: Los Amigos\n    Review: Don't make my mistake and over order. The portions are monstrous. The wet burritos are as big as a football.\n    \n\nAI data retrieval systems today face 3 challenges: `limited modalities`, `lack\nof accuracy`, and `high costs at scale`. Deep Lake 4.0 fixes this by enabling\ntrue multi-modality, enhancing accuracy, and reducing query costs by 2x with\nindex-on-the-lake technology.\n\nConsider a scenario where we store all our data locally on a computer.\nInitially, this may be adequate, but as the volume of data grows, managing it\nbecomes increasingly challenging. The computer's storage becomes limited, data\naccess slows, and sharing information with others is less efficient.\n\nTo address these challenges, we can transition our data storage to the cloud\nusing Deep Lake. Designed specifically for handling large-scale datasets and\nAI workloads, Deep Lake enables up to 10 times faster data access.",
        "node_520": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_494": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_312": "## Dataset Metadata\u00b6\n\n####  `` deeplake.Metadata \u00b6\n\nBases: `ReadOnlyMetadata`\n\nWritable access to dataset and column metadata for ML workflows.\n\nStores important information about datasets like:\n\n  * Model parameters and hyperparameters\n  * Preprocessing statistics\n  * Data splits and fold definitions\n  * Version and training information\n\nChanges are persisted immediately without requiring `commit()`.\n\nExamples:\n\nStoring model metadata:\n\n    \n    \n    ds.metadata[\"model_name\"] = \"resnet50\"\n    ds.metadata[\"hyperparameters\"] = {\n        \"learning_rate\": 0.001,\n        \"batch_size\": 32\n    }\n    \n\nSetting preprocessing stats:\n\n    \n    \n    ds[\"images\"].metadata[\"mean\"] = [0.485, 0.456, 0.406]\n    ds[\"images\"].metadata[\"std\"] = [0.229, 0.224, 0.225]\n    \n\n#####  `` __contains__ \u00b6\n\n    \n    \n    __contains__(key: str) -> bool\n    \n\nChecks if the metadata contains the given key.\n\n#####  `` __getitem__ \u00b6\n\n    \n    \n    __getitem__(key: str) -> Any\n    \n\nGets metadata value for the given key.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`key` |  `str` |  Metadata key to retrieve |  _required_  \n  \nReturns:\n\nType | Description  \n---|---  \n`Any` |  The stored metadata value  \n  \nExamples:\n\n    \n    \n    mean = ds[\"images\"].metadata[\"mean\"]\n    std = ds[\"images\"].metadata[\"std\"]\n    \n\n#####  `` __setitem__ \u00b6\n\n    \n    \n    __setitem__(key: str, value: Any) -> None\n    \n\nSets metadata value for given key. Changes are persisted immediately.",
        "node_692": "Restaurant name: Chaat Bhavan Mountain View\n    Review: Great place with good food\n    Owner Answer: Thank you for your positive feedback! We're thrilled to hear that you had a great experience at our restaurant and enjoyed our delicious food. Your satisfaction is our priority, and we can't wait to welcome you back for another wonderful dining experience.\n    \n    Thanks,\n    Team Chaat Bhavan\n    Restaurant name: Chaat Bhavan Mountain View\n    Review: Good food.\n    Owner Answer: Thank you for your 4-star rating! We're glad to hear that you had a positive experience at our restaurant. Your feedback is valuable to us, and we appreciate your support. If there's anything specific we can improve upon to earn that extra star next time, please let us know. We look forward to serving you again soon.\n    \n    Thanks,\n    Team Chaat Bhavan\n    \n\n## 4) Explore Results with Hybrid Search\u00b6\n\nIn the stage, the system enhances its search capabilities by combining BM25\nwith Approximate Nearest Neighbors (ANN) for a hybrid search. This approach\nblends lexical search with semantic search, improving relevance by considering\nboth keywords and semantic meaning. The introduction of a Large Language Model\n(LLM) allows the system to generate text-based answers, delivering direct\nresponses instead of simply listing relevant documents.\n\nWe open the `vector_search` dataset to perform a hybrid search. First, we\ndefine a query `\"Let's grab a drink\"` and generate its embedding using\n`embedding_function(query)[0]`. We then convert this embedding into a comma-\nseparated string `embedding_string`, preparing it for use in combined text and\nvector-based searches.\n\n    \n    \n    vector_search = deeplake.open(f\"al://{org_id}/{dataset_name_vs}\")\n    \n\n### Search for the correct restaurant using a specific sentence\u00b6\n\n    \n    \n    query = \"I feel like a drink\"\n    embed_query = embedding_function(query)[0]\n    embedding_string = \",\".join(str(c) for c in embed_query)\n    \n\nWe create two queries:\n\n  1.",
        "node_357": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_510": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_264": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nColumn Classes\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column  Column  Table of contents \n      * Column Class \n        * Column \n      * ColumnView Class \n        * ColumnView \n      * Class Comparison \n        * Column \n        * ColumnView \n      * Examples \n        * Data Access \n        * Metadata \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Column Class \n    * Column \n  * ColumnView Class \n    * ColumnView \n  * Class Comparison \n    * Column \n    * ColumnView \n  * Examples \n    * Data Access \n    * Metadata \n\n# Column Classes\u00b6\n\nDeep Lake provides two column classes for different access levels:\n\nClass | Description  \n---|---  \nColumn | Full read-write access to column data  \nColumnView | Read-only access to column data  \n  \n## Column Class\u00b6\n\n###  `` deeplake.",
        "node_563": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_542": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_546": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_503": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_200": "# Unzip to './animals' folder\n    !tar -xvf ./animals.tar\n    \n\n# Unzip to './animals' folder !tar -xvf ./animals.tar\n\n    \n    \n    tar: Error opening archive: Unrecognized archive format\n    \n\nIn [ ]:\n\nCopied!\n\n    \n    \n    animals\n    - cats\n      - image_1.jpg\n      - image_2.jpg\n    - dogs\n      - image_3.jpg\n      - image_4.jpg\n    \n\nanimals \\- cats \\- image_1.jpg \\- image_2.jpg \\- dogs \\- image_3.jpg \\-\nimage_4.jpg\n\nNow that you have the data, you can **create a Deep Lake`Dataset`** and\ninitialize its tensors. Running the following code will create a Deep Lake\ndataset inside of the `./animals_dl` folder.\n\nIn [58]:\n\nCopied!\n\n    \n    \n    import deeplake\n    import numpy as np\n    import os\n    \n    ds = deeplake.create('./animals_dl') # Creates the dataset\n    \n\nimport deeplake import numpy as np import os ds =\ndeeplake.create('./animals_dl') # Creates the dataset\n\nNext, let's inspect the folder structure for the source dataset './animals' to\nfind the class names and the files that need to be uploaded to the Deep Lake\ndataset.\n\nIn [59]:\n\nCopied!\n\n    \n    \n    # Find the class_names and list of files that need to be uploaded\n    dataset_folder = '/Users/istranic/ActiveloopCode/Datasets/animals'\n    \n    # Find the subfolders, but filter additional files like DS_Store that are added on Mac machines.",
        "node_635": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_134": "**Apply Softmax to Scores** : \n\n     * We extract `score` values from `vs_results` and `bm25_results` and apply `softmax` to them, storing the results in `vss` and `bm25s`. This step scales both sets of scores for easy comparison.\n  2. **Create Document Dictionaries** : \n\n     * We create dictionaries `docs_vs` and `docs_bm25` to store documents from `vs_results` and `bm25_results`, respectively. For each result, we add the `restaurant_name` and `restaurant_review` along with the normalized score. Each document is identified by `row_id`.\n\nThis code standardizes scores and organizes results, allowing comparison\nacross both vector and BM25 search methods.",
        "node_600": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_687": "`embedding`: Stores vector embeddings with a dimension size of 3072, which will enable vector-based similarity searches.\n\n  2. `restaurant_name`: A text column with a **BM25 index** , optimizing it for relevance-based text search.\n\n  3. `restaurant_review`: Another text column with a **BM25 index** , also optimized for efficient and ranked search results.\n\n  4. `owner_answer`: A text column with an **inverted index** , allowing fast and efficient filtering based on specific owner answer.\n\nFinally, we use `vector_search.commit()` to save these new columns, ensuring\nthe dataset structure is ready for further data additions and queries.\n\n    \n    \n    dataset_name_vs = \"vector_indexes\"\n    vector_search = deeplake.create(f\"al://{org_id}/{dataset_name_vs}\")\n    \n    # Add columns to the dataset\n    vector_search.add_column(name=\"embedding\", dtype=types.Embedding(3072))\n    vector_search.add_column(name=\"restaurant_name\", dtype=types.Text(index_type=types.BM25))\n    vector_search.add_column(name=\"restaurant_review\", dtype=types.Text(index_type=types.BM25))\n    vector_search.add_column(name=\"owner_answer\", dtype=types.Text(index_type=types.Inverted))\n    \n    vector_search.commit()\n    \n\nThis function processes each review in `restaurant_review` and converts it\ninto a numerical embedding. These embeddings, stored in\n`embeddings_restaurant_review`, represent each review as a vector, enabling us\nto perform cosine similarity searches and comparisons within the dataset.\n\nDeep Lake will handle the search computations, providing us with the final\nresults.",
        "node_610": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_93": "If that's not the case in your Azure account, please enable CORS in order to\nuse the Deep Lake app to visualize Deep Lake datasets stored in your own Azure\nstorage. CORS should be enabled in the storage account containing the Deep\nLake dataset and any linked data.\n\n## Steps for enabling CORS in Azure\u00b6\n\n1\\. Login to the Azure.\n\n2\\. Navigate to the `Storage account` with the relevant data.\n\n3\\. Open the `Resource sharing (CORS)` section on the left nav.\n\n4\\. Add the following items to the permissions.\n\nAllowed origins | Allowed methods | Allowed headers  \n---|---|---  \nhttps://app.activeloop.ai | GET, HEAD | *  \n  \n## Next Steps\u00b6\n\n  * Provisioning Federated Credentials\n\nBack to top",
        "node_583": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_112": "ColPali and the Late Interaction Mechanism \n    * Download the ColPali model \n    * Create a new dataset to store the ColPali embeddings \n    * Save the data in the dataset \n    * Chat with images \n    * Retrieve the most similar images \n    * VQA: Visual Question Answering \n\n# Advancing Search Capabilities: From Lexical to Multi-Modal with Deep Lake\u00b6\n\nInstall the main libraries:\n\n    \n    \n    pip install deeplake\n    \n\n## Load the Data from Deep Lake\u00b6\n\nThe following code opens the dataset in read-only mode from Deep Lake at the\nspecified path `al://activeloop/restaurant_reviews_complete`. The\n`scraped_data` object now contains the complete restaurant dataset, featuring\n160 restaurants and over 24,000 images, ready for data extraction and\nprocessing.\n\n    \n    \n    import deeplake\n    scraped_data = deeplake.open_read_only(f\"al://activeloop/restaurant_reviews_complete\")\n    \n    \n    \n    print(f\"Scraped {len(scraped_data)} reviews\")\n    \n\nOutput:\n\n    \n    \n    Scraped 18625 reviews\n    \n\n## 1) Create the Dataset and Use an Inverted Index for Filtering\u00b6\n\nIn the first stage of this course, we'll cover Lexical Search, a traditional\nand foundational approach to information retrieval.\n\nAn inverted index is a data structure commonly used in search engines and\ndatabases to facilitate fast full-text searches. Unlike a row-wise search,\nwhich scans each row of a document or dataset for a search term, an inverted\nindex maps each unique word or term to the locations (such as document IDs or\nrow numbers) where it appears. This setup allows for very efficient retrieval\nof information, especially in large datasets.\n\nFor small datasets with up to 1,000 documents, row-wise search can provide\nefficient performance without needing an inverted index. For medium-sized\ndatasets (10,000+ documents), inverted indexes become useful, particularly if\nsearch queries are frequent.",
        "node_772": "However,\nthey are architecturally very different. Pinecone is a fully-managed Vector\nDatabase that is optimized for highly demanding applications requiring a\nsearch for billions of vectors. Deep Lake is serverless. All computations run\nclient-side, which enables users to get started in seconds. Unlike Pinecone,\nDeep Lake\u2019s data format can store raw data such as images, videos, and text,\nin addition to embeddings. Deep Lake datasets can be visualized and version\ncontrolled. Pinecone is limited to light metadata on top of the embeddings and\nhas no visualization. Deep Lake also has a performant dataloader for fine-\ntuning your Large Language Models.\n\n**Deep Lake vs Weaviate**\n\nBoth Deep Lake and Weaviate enable users to store and search vectors\n(embeddings) and offer integrations with LangChain and LlamaIndex. However,\nthey are architecturally very different. Weaviate is a Vector Database that\ncan be deployed in a managed service or by the user via Kubernetes or Docker.\nDeep Lake is serverless. All computations run client-side, which enables users\nto support lightweight production apps in seconds. Unlike Weaviate, Deep\nLake\u2019s data format can store raw data such as images, videos, and text, in\naddition to embeddings. Deep Lake datasets can be visualized and version\ncontrolled. Weaviate is limited to light metadata on top of the embeddings and\nhas no visualization. Deep Lake also has a performant dataloader for fine-\ntuning your Large Language Models.",
        "node_443": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_100": "This enables users to manage and define Deep Lake permissions for\njobs that many not be attributed to a specific user.\n\nSet up a Workload Identity using the following steps:\n\n  1. Define an Azure Managed Identity in your cloud\n  2. Attached the Azure Managed Identity to your workload\n  3. Create a Deep Lake Workload Identity using the Azure Managed Identity\n\n  4. Run the workload in Azure\n\n## Step 1: Define the workload identity in Azure\u00b6\n\n  1. Navigate to Managed Identities in Azure\n\n  2. Click `Create` a Managed Identity\n\n  3. Select the `Subscription` and `Resource Group` containing the workload, and give the Managed Identity a `Name`. Click `Review + Create`.\n\n## Step 2: Attached the Azure Managed Identity to your workload\u00b6\n\nWhen creating or updating a resource that will serve as the Client running\nDeep Lake, assign the Managed Identity from Step 1 to this resource.\n\nFor example, in Azure Machine Learning Studio, when creating a compute\ninstance, toggle `Assign Identity` and select the `Managed Identity` from Step\n1.\n\n## Step 3: Create a Deep Lake Workload Identity using the Azure Managed\nIdentity\u00b6\n\nNavigate to the `Permissions` tab for your organization in the Deep Lake App,\nlocate the `Workload Identities`, and select `Add`.\n\nSpecify a `Display Name`, `Client ID` (for the Managed Identity), and `Tenant\nID`. The `Client ID` can be found in the main page for the Managed Identity,\nand the `Tenant ID` can be found in `Tenant Properties` in Azure. Click `Add`.\n\n## Step 4: Run the workload\u00b6\n\nSpecify the environmental variables below in the Deep Lake client and run\nother Deep APIs as normal.",
        "node_568": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_693": "The introduction of a Large Language Model\n(LLM) allows the system to generate text-based answers, delivering direct\nresponses instead of simply listing relevant documents.\n\nWe open the `vector_search` dataset to perform a hybrid search. First, we\ndefine a query `\"Let's grab a drink\"` and generate its embedding using\n`embedding_function(query)[0]`. We then convert this embedding into a comma-\nseparated string `embedding_string`, preparing it for use in combined text and\nvector-based searches.\n\n    \n    \n    vector_search = deeplake.open(f\"al://{org_id}/{dataset_name_vs}\")\n    \n\n### Search for the correct restaurant using a specific sentence\u00b6\n\n    \n    \n    query = \"I feel like a drink\"\n    embed_query = embedding_function(query)[0]\n    embedding_string = \",\".join(str(c) for c in embed_query)\n    \n\nWe create two queries:\n\n  1. **Vector Search** (`tql_vs`): Calculates cosine similarity with `embedding_string` and returns the top 5 matches by score.\n\n  2. **BM25 Search** (`tql_bm25`): Ranks `restaurant_review` by BM25 similarity to `query`, also limited to the top 5.\n\nWe then execute both queries, storing vector results in `vs_results` and BM25\nresults in `bm25_results`. This allows us to compare results from both search\nmethods.\n\n    \n    \n    tql_vs = f\"\"\"\n        SELECT *, cosine_similarity(embedding, ARRAY[{embedding_string}]) as score\n        FROM (\n            SELECT *, ROW_NUMBER() AS row_id\n        )\n        ORDER BY cosine_similarity(embedding, ARRAY[{embedding_string}]) DESC \n        LIMIT 5\n    \"\"\"\n    \n    tql_bm25 = f\"\"\"\n        SELECT *, BM25_SIMILARITY(restaurant_review, '{query}') as score\n        FROM (\n            SELECT *, ROW_NUMBER() AS row_id\n        ) \n        ORDER BY BM25_SIMILARITY(restaurant_review, '{query}') DESC \n        LIMIT 5\n    \"\"\"\n    \n    vs_results = vector_search.query(tql_vs)\n    bm25_results = vector_search.",
        "node_70": "## Installation\u00b6\n\nDeep Lake can be installed using pip:\n\n    \n    \n    pip install deeplake\n    \n\n## Creating a Dataset\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a local dataset\n    ds = deeplake.create(\"path/to/dataset\")\n    \n    # Or create in cloud storage\n    ds = deeplake.create(\"s3://my-bucket/dataset\")\n    ds = deeplake.create(\"gcs://my-bucket/dataset\")\n    ds = deeplake.create(\"azure://container/dataset\")\n    \n\n## Adding Data\u00b6\n\nAdd columns to store different types of data:\n\n    \n    \n    # Add basic data types\n    ds.add_column(\"ids\", \"int32\")\n    ds.add_column(\"labels\", \"text\")\n    \n    # Add specialized data types\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"masks\", deeplake.types.BinaryMask())\n    \n\nInsert data into the dataset:\n\n    \n    \n    # Add single samples\n    ds.append([{\n        \"ids\": 1,\n        \"labels\": \"cat\",\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"masks\": mask_array\n    }])\n    \n    # Add batches of data\n    ds.append({\n        \"ids\": [1, 2, 3],\n        \"labels\": [\"cat\", \"dog\", \"bird\"],\n        \"images\": batch_of_images,\n        \"embeddings\": batch_of_embeddings,\n        \"masks\": batch_of_masks\n    })\n    \n    ds.",
        "node_571": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_782": "We do not host or distribute these datasets, vouch for their quality or\nfairness, or claim that you have a license to use the datasets. It is your\nresponsibility to determine whether you have permission to use the datasets\nunder their license.\n\nIf you're a dataset owner and do not want your dataset to be included in this\nlibrary, please get in touch through a GitHub issue. Thank you for your\ncontribution to the ML community!\n\n**Usage Tracking**\n\nBy default, we collect usage data using Bugout (here's the code that does it).\nIt does not collect user data other than anonymized IP address data, and it\nonly logs the Deep Lake library's own actions. This helps our team understand\nhow the tool is used and how to build features that matter to you! After you\nregister with Activeloop, data is no longer anonymous. You can always opt-out\nof reporting by setting an environmental variable `BUGGER_OFF` to `True`:\n\n## Citation\n\nIf you use Deep Lake in your research, please cite Activeloop using:\n\n    \n    \n    @article{deeplake,\n      title = {Deep Lake: a Lakehouse for Deep Learning},\n      author = {Hambardzumyan, Sasun and Tuli, Abhinav and Ghukasyan, Levon and Rahman, Fariz and Topchyan, Hrant and Isayan, David and Harutyunyan, Mikayel and Hakobyan, Tatevik and Stranic, Ivo and Buniatyan, Davit},\n      url = {https://www.cidrdb.org/cidr2023/papers/p69-buniatyan.pdf},\n      booktitle={Proceedings of CIDR},\n      year = {2023},\n    }\n\n## Acknowledgment\n\nThis technology was inspired by our research work at Princeton University.",
        "node_138": "I love their wet Chili Verde burritos'}, score=0.04177094836797888)}\n    \n\n### Fusion method\u00b6\n\nWe define weights for our hybrid search: `VECTOR_WEIGHT` and `LEXICAL_WEIGHT`\nare both set to `0.5`, giving equal importance to vector-based and BM25\nscores.\n\n  1. **Initialize Results Dictionary** : \n\n     * We create an empty dictionary, `results`, to store documents with their combined scores from both search methods.\n  2. **Combine Scores** : \n\n     * We iterate over the unique document IDs from `docs_vs` and `docs_bm25`.\n\n     * For each document: \n\n       * We add it to `results`, defaulting to the version available (vector or BM25).\n       * We calculate a weighted score: `vs_score` from vector results (if present in `docs_vs`) and `bm_score` from BM25 results (if present in `docs_bm25`).\n       * The final `results[k].score` is set by adding `vs_score` and `bm_score`.\n\nThis produces a fused score for each document in `results`, ready to rank in\nthe hybrid search.",
        "node_279": "Parameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`dtype` |  `DataType | str` |  DataType | str The datatype of values in the array |  _required_  \n`dimensions` |  `int | None` |  int | None The number of dimensions/axes in the array. Unlike specifying `shape`, there is no constraint on the size of each dimension. |  _required_  \n`shape` |  `list[int] | None` |  list[int] | None Constrain the size of each dimension in the array |  _required_  \n  \nReturns:\n\nName | Type | Description  \n---|---|---  \n`DataType` |  `DataType` |  A new array data type with the specified parameters.  \n  \nExamples:\n\nCreate a three-dimensional array, where each dimension can have any number of\nelements:\n\n    \n    \n    ds.add_column(\"col1\", types.Array(\"int32\", dimensions=3))\n    \n\nCreate a three-dimensional array, where each dimension has a known size:\n\n    \n    \n    ds.add_column(\"col2\", types.Array(types.Float32(), shape=[50, 30, 768]))\n    \n    \n    \n    # Fixed-size array\n    ds.add_column(\"features\", deeplake.types.Array(\n        \"float32\",\n        shape=[512]  # Enforces size\n    ))\n    \n    # Variable-size array\n    ds.add_column(\"sequences\", deeplake.types.Array(\n        \"int32\",\n        dimensions=1  # Allows any size\n    ))\n    \n\n##  `` deeplake.types.Bytes \u00b6\n\n    \n    \n    Bytes() -> DataType\n    \n\nCreates a byte array value type. This is useful for storing raw binary data.\n\nReturns:\n\nName | Type | Description  \n---|---|---  \n`DataType` |  `DataType` |  A new byte array data type.",
        "node_405": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_770": "Chat in on our website: to claim the access!\n\n## \ud83d\udc69\u200d\ud83d\udcbb Comparisons to Familiar Tools\n\n**Deep Lake vs Chroma**\n\nBoth Deep Lake & ChromaDB enable users to store and search vectors\n(embeddings) and offer integrations with LangChain and LlamaIndex. However,\nthey are architecturally very different. ChromaDB is a Vector Database that\ncan be deployed locally or on a server using Docker and will offer a hosted\nsolution shortly. Deep Lake is a serverless Vector Store deployed on the\nuser\u2019s own cloud, locally, or in-memory. All computations run client-side,\nwhich enables users to support lightweight production apps in seconds. Unlike\nChromaDB, Deep Lake\u2019s data format can store raw data such as images, videos,\nand text, in addition to embeddings. ChromaDB is limited to light metadata on\ntop of the embeddings and has no visualization. Deep Lake datasets can be\nvisualized and version controlled. Deep Lake also has a performant dataloader\nfor fine-tuning your Large Language Models.\n\n**Deep Lake vs Pinecone**\n\nBoth Deep Lake and Pinecone enable users to store and search vectors\n(embeddings) and offer integrations with LangChain and LlamaIndex. However,\nthey are architecturally very different. Pinecone is a fully-managed Vector\nDatabase that is optimized for highly demanding applications requiring a\nsearch for billions of vectors. Deep Lake is serverless. All computations run\nclient-side, which enables users to get started in seconds.",
        "node_455": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_453": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_72": "\"images\": batch_of_images,\n        \"embeddings\": batch_of_embeddings,\n        \"masks\": batch_of_masks\n    })\n    \n    ds.commit() # Commit changes to the storage\n    \n\n## Accessing Data\u00b6\n\nAccess individual samples:\n\n    \n    \n    # Get single items\n    image = ds[\"images\"][0]\n    label = ds[\"labels\"][0]\n    embedding = ds[\"embeddings\"][0]\n    \n    # Get ranges\n    images = ds[\"images\"][0:100]\n    labels = ds[\"labels\"][0:100]\n    \n    # Get specific indices\n    selected_images = ds[\"images\"][[0, 2, 3]]\n    \n\n## Vector Search\u00b6\n\nSearch by embedding similarity:\n\n    \n    \n    # Find similar items\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n    # Process results - Method 1: iterate through items\n    for item in results:\n        image = item[\"images\"]\n        label = item[\"labels\"]\n    \n    # Process results - Method 2: direct column access\n    images = results[\"images\"][:]\n    labels = results[\"labels\"][:]  # Recommended for better performance\n    \n\n## Data Versioning\u00b6\n\n    \n    \n    # Commit changes\n    ds.commit(\"Added initial data\")\n    \n    # Create version tag\n    ds.tag(\"v1.0\")\n    \n    # View history\n    for version in ds.history:\n        print(version.id, version.message)\n    \n    # Create a new branch\n    ds.branch(\"new-branch\")\n    ### Add new data to the branch .\n    main_ds = ds.branches['main'].open()\n    main_ds.merge(\"new-branch\")\n    \n\n## Async Operations\u00b6\n\nUse async operations for better performance:\n\n    \n    \n    # Async data loading\n    future = ds[\"images\"].get_async(slice(0, 1000))\n    images = future.",
        "node_221": "abspath(cfg.work_dir))\n    \n        # train_segmentor\n        mmseg_deeplake.train_segmentor(\n            model,\n            cfg,\n            distributed=True,  # Set to True for multi-GPU training\n            validate=True, # Set to True if you have validation data\n        )\n    \n\n## Deep Lake Integration Benefits\u00b6\n\n  1. **Efficient Mask Handling** : Deep Lake efficiently stores and loads segmentation masks, which can be large and memory-intensive.\n\n  2. **Automatic Format Conversion** : Deep Lake handles conversion between different mask formats (binary, RLE, polygon) automatically.\n\n  3. **Smart Batching** : Deep Lake's dataloader handles variable-sized images and masks efficiently.\n\n  4. **Memory Management** : Data is loaded on-demand, preventing out-of-memory issues with large datasets.\n\n  5. **Distributed Training Support** : Seamless integration with MMSegmentation's distributed training.\n\n## Monitoring Training\u00b6\n\nMonitor training progress:\n\n    \n    \n    # Check latest log file\n    log_file = os.path.join(cfg.work_dir, 'latest.log')\n    if os.path.exists(log_file):\n        with open(log_file, 'r') as f:\n            print(f.read())\n    \n\n## Inference\u00b6\n\nAfter training, use the model for inference:\n\n    \n    \n    from mmseg.apis import inference_segmentor, init_segmentor\n    \n    # Load trained model\n    checkpoint = os.path.join(cfg.work_dir, 'latest.pth')\n    model = init_segmentor(config_path, checkpoint)\n    \n    # Load an image\n    img = 'path/to/test/image.jpg'\n    \n    # Run inference\n    result = inference_segmentor(model, img)\n    \n\n### Key Integration Parameters\u00b6\n\n  * **`data`** : Central to the MMSegmentation configuration file, it specifies the training and validation datasets, transformations, and paths.\n    * **`train`** : Contains dataset path, credentials, and transformations for training data.\n    * **`val`** : Contains dataset path, credentials, and transformations for validation data.\n    * **`pipeline`** : A list of transformations applied to the dataset.",
        "node_169": "By contrast, after myocardial infarction, susceptible dogs showed significant decrease in all measures of heart rate variability. Before myocardial infarction, no differences were seen between susceptible and resistant dogs. However, 30 days after infarction, epidemiologic analysis of the coefficient of variance showed high sensitivity and specificity (88% and 80%, respectively), predicting susceptibility. Therefore, results of analysis of 30 min of beat to beat heart period at rest 30 days after myocardial infarction are highly predictive for increased risk of sudden death. \\n5\\tMultiple organ failure: inflammatory priming and activation sequences promote autologous tissue injury.\n    Text: However, no paired studies have been reported to examine heart rate variability before and after myocardial infarction. The hypothesis was tested that low values of heart rate variability provided risk assessment both before and after myocardial infarction with use of an established canine model of sudden cardiac death. Risk for sudden death was assessed 1 month after myocardial infarction by a protocol in which exercise and myocardial ischemia were combined; dogs that developed ventricular fibrillation were classified at high risk for sudden death (susceptible) and the survivors were considered low risk (resistant). In resistant dogs, myocardial infarction did not affect any measure of heart rate variability: 1) mean RR interval, 2) standard deviation of the mean RR interval, and 3) the coefficient of variance (standard deviation/RR interval). By contrast, after myocardial infarction, susceptible dogs showed significant decrease in all measures of heart rate variability. Before myocardial infarction, no differences were seen between susceptible and resistant dogs. However, 30 days after infarction, epidemiologic analysis of the coefficient of variance showed high sensitivity and specificity (88% and 80%, respectively), predicting susceptibility.",
        "node_723": "vector_search_images = deeplake.open(f\"al://{org_id}/{image_dataset_name}\")\n    vector_search_images\n    \n    \n    \n    query = \"https://www.moltofood.it/wp-content/uploads/2024/09/Hamburger.jpg\"\n    \n    image_query = requests.get(query)\n    image_query_pil = Image.open(BytesIO(image_query.content))\n    \n\n### Performing a similar image search based on a specific image\u00b6\n\n    \n    \n    print(image_query_pil)\n    \n\nOutput:\n\nWe generate an embedding for the query image, `image_query_pil`, by calling\n`embedding_function_images([image_query_pil])[0]`. This embedding is then\nconverted into a comma-separated string, `query_embedding_string`, for\ncompatibility in the query.The query, `tql`, retrieves entries from the\ndataset by calculating cosine similarity between `embedding` and\n`query_embedding_string`. It ranks results by similarity score in descending\norder, limiting the output to the top 6 most similar images.\n\n    \n    \n    query_embedding = embedding_function_images([image_query_pil])[0]\n    query_embedding_string = \",\".join([str(item) for item in query_embedding])\n    \n    tql = f\"\"\"\n        SELECT *, cosine_similarity(embedding, ARRAY[{query_embedding_string}]) as score\n        FROM (\n            SELECT *, ROW_NUMBER() AS row_id\n        )\n        ORDER BY cosine_similarity(embedding, ARRAY[{query_embedding_string}]) DESC \n        LIMIT 6\n    \"\"\"\n    \n    \n    \n    similar_images_result = vector_search_images.query(tql)\n    print(similar_images_result)\n    \n\nOutput:\n\n    \n    \n    Dataset(columns=(embedding,restaurant_name,restaurant_review,owner_answer,row_id,score), length=6)\n    \n\n### Show similar images and the their respective restaurants\u00b6\n\n    \n    \n    !pip install matplotlib\n    \n\nThe `show_images` function displays a grid of similar images, along with\nrestaurant names and similarity scores. It defines a grid with 3 columns and\ncalculates the required number of rows based on the number of images.",
        "node_209": "Note: We use MMDetection 2.x versions as they're currently supported by the\nDeep Lake integration.\n\n## Setup\u00b6\n\nLet's set up our imports and authentication:\n\n    \n    \n    import deeplake\n    from mmcv import Config\n    from mmdet.models import build_detector\n    import os\n    import mmcv\n    \n    # Set your Deep Lake token\n    token = os.environ[\"ACTIVELOOP_TOKEN\"]\n    \n\n## Configuration\u00b6\n\nMMDetection uses config files to define models and training parameters. Here's\nour YOLOv3 config with Deep Lake integration:\n\n    \n    \n    _base_ = \"<mmdetection_path>/configs/yolo/yolov3_d53_mstrain-416_273e_coco.py\"\n    \n    # use caffe img_norm\n    img_norm_cfg = dict(mean=[0, 0, 0], std=[255., 255., 255.], to_rgb=True)\n    \n    train_pipeline = [\n        dict(type='LoadImageFromFile'),\n        dict(type='LoadAnnotations', with_bbox=True),\n        dict(\n            type='Expand',\n            mean=img_norm_cfg['mean'],\n            to_rgb=img_norm_cfg['to_rgb'],\n            ratio_range=(1, 2)),\n        dict(type='Resize', img_scale=[(320, 320), (416, 416)], keep_ratio=True),\n        dict(type='RandomFlip', flip_ratio=0.0),\n        dict(type='PhotoMetricDistortion'),\n        dict(type='Normalize', **img_norm_cfg),\n        dict(type='Pad', size_divisor=32),\n        dict(type='DefaultFormatBundle'),\n        dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n    ]\n    test_pipeline = [\n        dict(type='LoadImageFromFile'),\n        dict(\n            type='MultiScaleFlipAug',\n            img_scale=(416, 416),\n            flip=False,\n            transforms=[\n                dict(type='Resize', keep_ratio=True),\n                dict(type='RandomFlip', flip_ratio=0.0),\n                dict(type='Normalize', **img_norm_cfg),\n                dict(type='Pad', size_divisor=32),\n                dict(type='ImageToTensor', keys=['img']),\n                dict(type='Collect', keys=['img'])\n            ])\n    ]\n    \n    \n    data = dict(\n        train=dict(\n            pipeline=train_pipeline,\n            deeplake_path=\"hub://activeloop/coco-train\",\n            # If not specified, Deep Lake will auto-infer the mapping, but it might make mistakes if datasets have many tensors\n            deeplake_tensors = {\"img\": \"images\", \"gt_bboxes\": \"boxes\", \"gt_labels\": \"categories\"},\n    \n            # the parameters in other parts of the cfg file such as samples_per_gpu, and others.",
        "node_398": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_266": "Parameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`index` |  `int | slice | list | tuple` |  Can be:\n\n  * int: Single item index\n  * slice: Range of indices (e.g., 0:10)\n  * list/tuple: Multiple specific indices\n\n|  _required_  \n  \nReturns:\n\nType | Description  \n---|---  \n`ndarray | list | Dict | str | bytes | None` |  The data at the specified index/indices. Type depends on the column's data type.  \n  \nExamples:\n\n    \n    \n    # Get single item\n    image = column[0]\n    \n    # Get range\n    batch = column[0:32]\n    \n    # Get specific indices\n    items = column[[1, 5, 10]]\n    \n\n####  `` __setitem__ \u00b6\n\n    \n    \n    __setitem__(index: int | slice, value: Any) -> None\n    \n\nSet data in the column at the specified index or range.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`index` |  `int | slice` |  Can be:\n\n  * int: Single item index\n  * slice: Range of indices (e.g., 0:10)\n\n|  _required_  \n`value` |  `Any` |  The data to store. Must match the column's data type. |  _required_  \n  \nExamples:\n\n    \n    \n    # Update single item\n    column[0] = new_image\n    \n    # Update range\n    column[0:32] = new_batch\n    \n\n####  `` get_async \u00b6\n\n    \n    \n    get_async(index: int | slice | list | tuple) -> Future\n    \n\nAsynchronously retrieve data from the column. Useful for large datasets or\nwhen loading multiple items in ML pipelines.",
        "node_121": "Using `ds_bm25.query()`, we search and rank entries in\n`restaurant_review` based on **BM25 similarity** to the query. The code orders\nresults by how well they match the query (`BM25_SIMILARITY`), from highest to\nlowest relevance, and limits the output to the top 10 results. The final list\nof results is stored in `view_bm25`.\n\n    \n    \n    query = \"I want burritos\"\n    view_bm25 = ds_bm25.query(f\"\"\"\n        SELECT *, BM25_SIMILARITY(restaurant_review, '{query}') AS score\n        ORDER BY BM25_SIMILARITY(restaurant_review, '{query}') DESC \n        LIMIT 6\n    \"\"\")\n    print(view_bm25)\n    \n\nOutput:\n\n    \n    \n    Dataset(columns=(restaurant_name,restaurant_review,owner_answer), length=6)\n    \n\n### Show the results\u00b6\n\n    \n    \n    for row in view_bm25:\n        print(f\"Restaurant name: {row['restaurant_name']} \\nReview: {row['restaurant_review']}\")\n    \n\nOutput:\n\n    \n    \n    Restaurant name: Los Amigos\n    Review: Best Burritos i have ever tried!!!!! Wolderful!!!\n    Restaurant name: Los Amigos\n    Review: Fantastic burritos!\n    Restaurant name: Cheztakos!!!\n    Review: Great burritos\n    Restaurant name: La Coste\u00f1a\n    Review: Awesome burritos!\n    Restaurant name: La Coste\u00f1a\n    Review: Awesome burritos\n    Restaurant name: La Coste\u00f1a\n    Review: Bomb burritos\n    \n\n## 3) Create the Dataset and use Vector Similarity Search\u00b6\n\nIf you want to generate text embeddings for similarity search, you can choose\na proprietary model like `text-embedding-3-large` from `OpenAI`, or you can\nopt for an `open-source` model. The MTEB leaderboard on Hugging Face provides\na selection of open-source models that have been tested for their\neffectiveness at converting text into embeddings, which are numerical\nrepresentations that capture the meaning and nuances of words and sentences.",
        "node_54": "All computations run client-side,\nwhich enables users to support lightweight production apps in seconds. Unlike\nChromaDB, Deep Lake\u2019s data format can store raw data such as images, videos,\nand text, in addition to embeddings. ChromaDB is limited to light metadata on\ntop of the embeddings and has no visualization. Deep Lake datasets can be\nvisualized and version controlled. Deep Lake also has a performant dataloader\nfor fine-tuning your Large Language Models.\n\n**Deep Lake vs Pinecone**\n\nBoth Deep Lake and Pinecone enable users to store and search vectors\n(embeddings) and offer integrations with LangChain and LlamaIndex. However,\nthey are architecturally very different. Pinecone is a fully-managed Vector\nDatabase that is optimized for highly demanding applications requiring a\nsearch for billions of vectors. Deep Lake is serverless. All computations run\nclient-side, which enables users to get started in seconds. Unlike Pinecone,\nDeep Lake\u2019s data format can store raw data such as images, videos, and text,\nin addition to embeddings. Deep Lake datasets can be visualized and version\ncontrolled. Pinecone is limited to light metadata on top of the embeddings and\nhas no visualization. Deep Lake also has a performant dataloader for fine-\ntuning your Large Language Models.\n\n**Deep Lake vs Weaviate**\n\nBoth Deep Lake and Weaviate enable users to store and search vectors\n(embeddings) and offer integrations with LangChain and LlamaIndex. However,\nthey are architecturally very different.",
        "node_34": "We\nwould like to thank William Silversmith @SeungLab for his awesome cloud-volume\ntool.\n\n## About\n\nDatabase for AI. Store Vectors, Images, Texts, Videos, etc. Use with\nLLMs/LangChain. Store, query, version, & visualize any AI data. Stream data in\nreal-time to PyTorch/TensorFlow.",
        "node_649": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_1": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_530": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_156": "* **Embedding Creation** : The model encodes each batch into embeddings, stored in the `embeddings` list, which is returned as a single list.\n\nThis function supports efficient, batched embedding generation, useful for\nmulti-modal tasks like image-based search.\n\n    \n    \n    from torchvision import transforms\n    \n    tform = transforms.Compose([\n        transforms.Resize((224,224)), \n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: torch.cat([x, x, x], dim=0) if x.shape[0] == 1 else x),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ])\n    \n    def embedding_function_images(images, model = model, transform = tform, batch_size = 4):\n        \"\"\"Creates a list of embeddings based on a list of image. Images are processed in batches.\"\"\"\n    \n        if isinstance(images, str):\n            images = [images]\n    \n        # Proceess the embeddings in batches, but return everything as a single list\n        embeddings = []\n        for i in range(0, len(images), batch_size):\n            batch = torch.stack([transform(item) for item in images[i:i+batch_size]])\n            batch = batch.to(device)\n            with torch.no_grad():\n                embeddings+= model.encode_image(batch).cpu().numpy().tolist()\n    \n        return embeddings\n    \n\n### Create a new dataset to save the images\u00b6\n\nWe set up a dataset for restaurant images and embeddings. The dataset includes\nan `embedding` column for 512-dimensional image embeddings, a\n`restaurant_name` column for names, and an `image` column for storing images\nin UInt8 format. After defining the structure, `vector_search_images.commit()`\nsaves it, making the dataset ready for storing data for multi-modal search\ntasks with images and metadata.",
        "node_721": "For each restaurant, we extract image URLs,\nrequest each URL, and filter for successful responses (status code 200). These\nresponses are then converted to PIL images and added to restaurants_images as\nlists of images, with each sublist containing the images for one restaurant.\n\n    \n    \n    !pip install requests\n    \n    \n    \n    import requests\n    from PIL import Image\n    from io import BytesIO\n    \n    restaurants_images = []\n    for urls in images:\n        pil_images = []\n        for url in urls:\n            response = requests.get(url)\n            if response.status_code == 200:\n                image = Image.open(BytesIO(response.content))\n                if image.mode == \"RGB\":\n                    pil_images.append(image)\n        if len(pil_images) == 0:\n            pil_images.append(Image.new(\"RGB\", (224, 224), (255, 255, 255)))\n        restaurants_images.append(pil_images)\n    \n\nWe populate `vector_search_images` with restaurant image data and embeddings.\nFor each restaurant in `scraped_data`, we retrieve its name and images, create\nembeddings for the images, and convert them to `UInt8` arrays. Then, we append\nthe restaurant names, images, and embeddings to the dataset and save with\n`vector_search_images.commit()`.\n\n    \n    \n    import numpy as np\n    \n    for sd, rest_images in zip(scraped_data, restaurants_images):\n        restaurant_name = [sd[\"restaurant_name\"]] * len(rest_images)\n        embeddings = embedding_function_images(rest_images, model=model, transform=tform, batch_size=4)\n        vector_search_images.append({\"restaurant_name\": restaurant_name, \"image\": [np.array(fn).astype(np.uint8) for fn in rest_images], \"embedding\": embeddings})\n    \n    vector_search_images.commit()\n    \n\n### Search similar images\u00b6\n\nIf you want direct access to the images and the embeddings, you can copy the\nActiveloop dataset.",
        "node_287": "## Query Functions\u00b6\n\n###  `` deeplake.query \u00b6\n\n    \n    \n    query(\n        query: str,\n        token: str | None = None,\n        creds: dict[str, str] | None = None,\n    ) -> DatasetView\n    \n\nExecutes TQL queries optimized for ML data filtering and search.\n\nTQL is a SQL-like query language designed for ML datasets, supporting:\n\n  * Vector similarity search\n  * Text semantic search\n  * Complex data filtering\n  * Joining across datasets\n  * Efficient sorting and pagination\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`query` |  `str` |  TQL query string supporting:\n\n  * Vector similarity: COSINE_SIMILARITY, L2_NORM\n  * Text search: BM25_SIMILARITY, CONTAINS\n  * MAXSIM similarity for ColPali embeddings: MAXSIM\n  * Filtering: WHERE clauses\n  * Sorting: ORDER BY\n  * Joins: JOIN across datasets\n\n|  _required_  \n`token` |  `str | None` |  Optional Activeloop authentication token |  `None`  \n`creds` |  `dict` |  Dictionary containing credentials used to access the dataset at the path.\n\n  * If 'aws_access_key_id', 'aws_secret_access_key', 'aws_session_token' are present, these take precedence over credentials present in the environment or in credentials file. Currently only works with s3 paths.\n  * It supports 'aws_access_key_id', 'aws_secret_access_key', 'aws_session_token', 'endpoint_url', 'aws_region', 'profile_name' as keys.\n  * If nothing is given is, credentials are fetched from the environment variables.",
        "node_102": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nEnabling CORS in GCP\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP  Enabling CORS in GCP  Table of contents \n            * Next Steps \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Next Steps \n\n# Enabling CORS in GCP\u00b6\n\nIn order to visualize Deep Lake datasets stored in your own GCP buckets in the\nDeep Lake app, please enable Cross-Origin Resource Sharing (CORS) in the\nbuckets containing the Deep Lake dataset and any linked data, by inserting the\nsnippet below in the CORS section of the Permissions tab for the bucket:\n\n    \n    \n    [\n        {\n          \"origin\": [\"https://app.activeloop.ai\"],\n          \"method\": [\"GET\", \"HEAD\"],\n          \"responseHeader\": [\"*\"],",
        "node_535": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_359": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_400": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_633": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_183": "question = queries[0]\n    output_image = \"image.jpg\"\n    img = Image.fromarray(colpali_results[0][\"image\"][0])\n    img.save(output_image)\n    \n\nThe following code opens `\"image.jpg\"` in binary mode, encodes it to a base64\nstring, and passes it with `question` to the `generate_VQA` function, which\nreturns an answer based on the image.\n\n    \n    \n    import base64\n    \n    with open(output_image, \"rb\") as image_file:\n        base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n    \n    answer = generate_VQA(base64_image, question)\n    print(answer)\n    \n\nOutput:\n\n    \n    \n    'As time approaches infinity, the voltage modeled by n^6 will eventually stabilize at the equilibrium potential for potassium (EK), which is represented at approximately -90 mV on the graph.'\n    \n\nWe've now gained a solid understanding of multi-modal data processing,\nadvanced retrieval techniques, and hybrid search methods using state-of-the-\nart models like ColPali. With these skills, you're equipped to tackle complex,\nreal-world applications that require deep insights from both text and image\ndata.\n\nKeep experimenting, stay curious, and continue building innovative\nsolutions\u2014this is just the beginning of what's possible in the field of AI-\ndriven search and information retrieval.\n\n**To learn more about Deep Lake v4, visit theofficial blog post and\ndocumentation.**\n\nBack to top",
        "node_348": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_107": "* Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG  RAG  Table of contents \n      * Load the Data from Deep Lake \n      * 1) Create the Dataset and Use an Inverted Index for Filtering \n        * Extract the data \n        * Add the data to the dataset \n        * Search for the restaurant using a specific word \n        * Show the results \n      * 2) Create the Dataset and use BM25 to Retrieve the Data \n        * Add data to the dataset \n        * Search for the restaurant using a specific sentence \n        * Show the results \n      * 3) Create the Dataset and use Vector Similarity Search \n        * Create the dataset and add the columns \n        * Search for the restaurant using a specific sentence \n      * 4) Explore Results with Hybrid Search \n        * Search for the correct restaurant using a specific sentence \n        * Show the scores \n        * Normalize the score \n        * Fusion method \n        * Show the results \n        * Let's run a search on a multiple dataset \n        * Comparison of Sync vs Async Query Performance \n      * 5) Integrating Image Embeddings for Multi-Modal Search \n        * Create the embedding function for images \n        * Create a new dataset to save the images \n        * Convert the URLs into images \n        * Search similar",
        "node_734": "MaxSim allows the system to compare different types of\ndata, like text and images, and find the most relevant matches. This helps\nretrieve results that are contextually accurate and meaningful, making it\nespecially useful for complex applications, like scientific and medical\nresearch, where a deep understanding of the content is essential.\n\nRecent advancements in Visual Language Models (VLMs), as highlighted in the\nColPali paper, demonstrate that VLMs can achieve recall rates on document\nretrieval benchmarks comparable to those of traditional OCR pipelines. End-to-\nend learning approaches are positioned to surpass OCR-based methods\nsignificantly. However, representing documents as a `bag of embeddings`\ndemands 30 times more storage than single embeddings. Deep Lake's format,\nwhich inherently supports n-dimensional arrays, enables this storage-intensive\napproach, and the 4.0 query engine introduces MaxSim operations.\n\nWith Deep Lake 4.0's 10x increase in storage efficiency, we can allocate some\nof these savings to store PDFs as 'bags of embeddings' processed at high\nspeeds. While this approach requires 30 times more storage than single\nembeddings, it allows us to capture richer document representations, bypassing\nOCR-based, manual feature engineering pipelines. This trade-off facilitates\nseamless integration within VLM/LLM frameworks, leading to more accurate and\ngenuinely multimodal responses.\n\nUnlike CLIP, which primarily focuses on aligning visual and text\nrepresentations, ColPali leverages advanced Vision Language Model (VLM)\ncapabilities to deeply understand both textual and visual content. This allows\nColPali to capture rich document structures\u2014like tables, figures, and\nlayouts\u2014directly from images without needing extensive preprocessing steps\nlike OCR or document segmentation. ColPali also utilizes a late interaction\nmechanism, which significantly improves retrieval accuracy by enabling more\ndetailed matching between query elements and document content. These features\nmake ColPali faster, more accurate, and especially effective for visually rich\ndocument retrieval, surpassing CLIP's capabilities in these areas\u200b.",
        "node_590": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_303": "Tags are created using deeplake.Dataset.tag.\n\n#####  `` id `property` \u00b6\n\n    \n    \n    id: str\n    \n\nThe unique identifier of the tag\n\n#####  `` name `property` \u00b6\n\n    \n    \n    name: str\n    \n\nThe name of the tag\n\n#####  `` open \u00b6\n\n    \n    \n    open() -> DatasetView\n    \n\nFetches the dataset corresponding to the tag\n\n#####  `` open_async \u00b6\n\n    \n    \n    open_async() -> Future\n    \n\nAsynchronously fetches the dataset corresponding to the tag and returns a\nFuture object.\n\n#####  `` version `property` \u00b6\n\n    \n    \n    version: str\n    \n\nThe version that has been tagged\n\n    \n    \n    # Open read-only dataset\n    ds = deeplake.open_read_only(\"s3://bucket/dataset\")\n    \n    # Access tag view\n    tag_view = ds.tags[\"v1.0\"]\n    print(f\"Tag: {tag_view.name}\")\n    print(f\"Version: {tag_view.version}\")\n    \n    # Open dataset at tag\n    tagged_ds = tag_view.open()\n    \n\n### TagsView\u00b6\n\n####  `` deeplake.TagsView \u00b6\n\nProvides access to the tags within a dataset.\n\nIt is returned by the deeplake.Dataset.tags property on a\ndeeplake.ReadOnlyDataset.\n\n#####  `` __getitem__ \u00b6\n\n    \n    \n    __getitem__(name: str) -> TagView\n    \n\nReturn a tag by name\n\n#####  `` __len__ \u00b6\n\n    \n    \n    __len__() -> int\n    \n\nThe total number of tags in the dataset\n\n#####  `` names \u00b6\n\n    \n    \n    names() -> list[str]\n    \n\nReturn a list of tag names\n\n    \n    \n    # Access read-only tags\n    tags_view = ds.tags\n    \n    # List tag names\n    for name in tags_view.names():\n        print(f\"Found tag: {name}\")\n    \n    # Get specific tag\n    tag_view = tags_view[\"v1.0\"]\n    \n\nBack to top",
        "node_91": "From the provided policy list, select the previously created policy from\nStep 1 and click `Next`\n\n6\\. Set the `name` and `description` for the role and click `Create role` at\nthe bottom.\n\n#### Step 3: Grant Access to AWS KMS Key (**only for buckets that are\nencrypted with customer managed KMS keys**)\u00b6\n\n1\\. Navigate to the bucket in the AWS S3 UI\n\n2\\. Open the bucket Properties\n\n3\\. Scroll down to Default encryption and copy the `AWS KMS key ARN`\n\n4\\. In the Policy creation step (Step 1, Sub-step 6), use the JSON below in\nthe policy statement, and replace `YOUR_KMS_KEY_ARN` with the copied Key ARN\nfor the encrypted bucket.\n\n    \n    \n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"s3:ListBucket\",\n             \"s3:GetBucketLocation\",\n                    \"s3:*Object*\"\n                ],\n                \"Resource\": [\n                    \"arn:aws:s3:::BUCKET_NAME\",\n                    \"arn:aws:s3:::BUCKET_NAME/*\"\n                ]\n            },\n            {\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"kms:Encrypt\",\n                    \"kms:Decrypt\",\n                    \"kms:ReEncrypt*\",\n                    \"kms:GenerateDataKey*\",\n                    \"kms:DescribeKey\"\n                ],\n                \"Resource\": [\n                    \"YOUR_KMS_KEY_ARN\u201d\n                ]\n            }\n        ]\n    }\n    \n\n#### Step 4: Enter the created AWS Role ARN (Step 2) into the Activeloop UI\u00b6\n\nSee the first video in the managed credentials overview\n\n## Next Steps\u00b6\n\n  * Enabling CORS in AWS S3\n\nBack to top",
        "node_473": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_745": "4. **Send API Request** : Using `requests.post`, the function sends the payload to the OpenAI API. If successful, it parses and returns the answer; otherwise, it returns `False`.\n\nThis approach enables an AI-powered visual analysis of images to generate\ncontextually relevant answers.\n\n    \n    \n    import json\n    \n    def generate_VQA(base64_image: str, question:str):\n    \n        system_prompt = f\"\"\"You are a visual language model specialized in analyzing images. Below is an image provided by the user along with a question. Analyze the image carefully, paying attention to details relevant to the question. Construct a clear and informative answer that directly addresses the user's question, based on visual cues.\n    \n        The output must be in JSON format with the following structure:\n        {{\n            \"answer\": \"The answer to the question based on visual analysis.\"\n        }}\n    \n        Here is the question: {question}\n        \"\"\"\n    \n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages = [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": system_prompt},\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                        },\n                    ],\n                }\n            ],\n            response_format={\"type\": \"json_object\"},\n        )\n    \n        try:\n    \n            response = response.choices[0].message.content\n            response = json.loads(response)\n            answer = response[\"answer\"]\n            return answer\n        except Exception as e:\n            print(f\"Error: {e}\")\n            return False\n    \n\nThis code sets `question` to the first item in `queries`, converts the first\nimage in `colpali_results` to an image format, and saves it as `\"image.jpg\"`.",
        "node_479": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_448": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_511": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_519": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_566": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_229": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nLabelbox Integration\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox  Labelbox  Table of contents \n        * Prerequisites \n        * Supported Labelbox Ontologies \n          * Video Ontology \n          * Uploading videos for annotation to Labelbox \n          * Creating a dataset from an annotated Labelbox project \n          * Re-fetching the annotations from Labelbox to the existing dataset \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Prerequisites \n  * Supported Labelbox Ontologies \n    * Video Ontology \n    * Uploading videos for annotation to Labelbox \n    * Creating a dataset from an annotated Labelbox project \n    * Re-fetching the annotations from Labelbox to the existing dataset \n\n# Labelbox Integration\u00b6\n\nThis document describes how to create Deep Lake datasets from Labelbox\nannotations. The API also allows you to update the dataset with new\nannotations.",
        "node_297": "Branches are created using deeplake.Dataset.branch.\n\n#####  `` __hash__ `class-attribute` \u00b6\n\n    \n    \n    __hash__: None = None\n    \n\n#####  `` base `property` \u00b6\n\n    \n    \n    base: tuple[str, str] | None\n    \n\nThe base branch id and version\n\n#####  `` delete \u00b6\n\n    \n    \n    delete() -> None\n    \n\nDeletes the branch from the dataset\n\n#####  `` id `property` \u00b6\n\n    \n    \n    id: str\n    \n\nThe unique identifier of the branch\n\n#####  `` name `property` \u00b6\n\n    \n    \n    name: str\n    \n\nThe name of the branch\n\n#####  `` open \u00b6\n\n    \n    \n    open() -> Dataset\n    \n\nOpens corresponding branch of the dataset\n\n#####  `` rename \u00b6\n\n    \n    \n    rename(new_name: str) -> None\n    \n\nRenames the branch within the dataset\n\n#####  `` timestamp `property` \u00b6\n\n    \n    \n    timestamp: datetime\n    \n\nThe branch creation timestamp\n\n    \n    \n    # Create branch\n    ds.branch(\"Branch1\")\n    \n    # Access branch\n    branch = ds.branches[\"Branch1\"]\n    print(f\"Branch: {branch.name}\")\n    print(f\"Created: {branch.timestamp}\")\n    print(f\"Base: {branch.base}\")\n    \n    # Open dataset at tag\n    branch_ds = branch.open()\n    \n    # Rename branch\n    branch.rename(\"Other Branch\")\n    \n    # Delete branch\n    branch.delete()\n    \n\n### Branches\u00b6\n\n####  `` deeplake.Branches \u00b6\n\nProvides access to the branches within a dataset.\n\nIt is returned by the deeplake.Dataset.branches property.",
        "node_308": "If `stuffs` is true, the following fields are added: \\- stuffs_bbox (bounding\nboxes): Bounding boxes for stuffs. \\- stuffs_classes (segment mask): Segment\nmasks for stuffs.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`embedding_size` |  `int` |  int Size of the embeddings. |  _required_  \n`quantize` |  `bool` |  bool, optional If true, quantize the embeddings to slightly decrease accuracy while greatly increasing query speed. Default is False. |  `False`  \n`objects` |  `bool` |  bool, optional Whether to include object-related fields. Default is True. |  `True`  \n`keypoints` |  `bool` |  bool, optional Whether to include keypoint-related fields. Default is False. |  `False`  \n`stuffs` |  `bool` |  bool, optional Whether to include stuff-related fields. Default is False. |  `False`  \n  \nExamples:\n\nCreate a dataset with the standard schema:\n\n    \n    \n    ds = deeplake.create(\"tmp://\", schema=deeplake.schemas.COCOImages(768))\n    \n\nCustomize the schema before creating the dataset:\n\n    \n    \n    schema = deeplake.schemas.COCOImages(768, objects=True, keypoints=True)\n    schema[\"image_embed\"] = schema.pop(\"embedding\")\n    schema[\"author\"] = types.Text()\n    ds = deeplake.create(\"tmp://\", schema=schema)\n    \n\nAdd a new field to the schema:\n\n    \n    \n    schema = deeplake.schemas.COCOImages(768)\n    schema[\"location\"] = types.Text()\n    ds = deeplake.create(\"tmp://\", schema=schema)\n    \n    \n    \n    # Basic COCO dataset\n    ds = deeplake.create(\"s3://bucket/dataset\",\n        schema=deeplake.schemas.COCOImages(768))\n    \n    # With keypoints and object detection\n    ds = deeplake.create(\"s3://bucket/dataset\",\n        schema=deeplake.schemas.",
        "node_467": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_576": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_307": "This schema includes the following fields: \\- id (uint64): Unique identifier\nfor each entry. \\- image (jpg image): The image data. \\- url (text): URL of\nthe image. \\- year (uint8): Year the image was captured. \\- version (text):\nVersion of the dataset. \\- description (text): Description of the image. \\-\ncontributor (text): Contributor of the image. \\- date_created (uint64):\nTimestamp when the image was created. \\- date_captured (uint64): Timestamp\nwhen the image was captured. \\- embedding (embedding): Embedding of the image.\n\\- license (text): License information. \\- is_crowd (bool): Whether the image\ncontains a crowd.\n\nIf `objects` is true, the following fields are added: \\- objects_bbox\n(bounding box): Bounding boxes for objects. \\- objects_classes (segment mask):\nSegment masks for objects.\n\nIf `keypoints` is true, the following fields are added: \\- keypoints_bbox\n(bounding box): Bounding boxes for keypoints. \\- keypoints_classes (segment\nmask): Segment masks for keypoints. \\- keypoints (2-dimensional array of\nuint32): Keypoints data. \\- keypoints_skeleton (2-dimensional array of\nuint16): Skeleton data for keypoints.\n\nIf `stuffs` is true, the following fields are added: \\- stuffs_bbox (bounding\nboxes): Bounding boxes for stuffs. \\- stuffs_classes (segment mask): Segment\nmasks for stuffs.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`embedding_size` |  `int` |  int Size of the embeddings. |  _required_  \n`quantize` |  `bool` |  bool, optional If true, quantize the embeddings to slightly decrease accuracy while greatly increasing query speed. Default is False. |  `False`  \n`objects` |  `bool` |  bool, optional Whether to include object-related fields. Default is True. |  `True`  \n`keypoints` |  `bool` |  bool, optional Whether to include keypoint-related fields. Default is False.",
        "node_120": "After appending, `ds_bm25.commit()` saves the changes, ensuring the new data\nis permanently stored in the dataset. Finally, `ds_bm25.summary()` provides a\nsummary of the dataset's updated structure and contents, allowing us to verify\nthat the data was added successfully.\n\n    \n    \n    ds_bm25.append({\n        \"restaurant_name\": restaurant_name,\n        \"restaurant_review\": restaurant_review,\n        \"owner_answer\": owner_answer\n    })\n    ds_bm25.commit()\n    ds_bm25.summary()\n    \n\nOutput:\n\n    \n    \n    Dataset(columns=(restaurant_name,restaurant_review,owner_answer), length=18625)\n    +-----------------+-----------------+\n    |     column      |      type       |\n    +-----------------+-----------------+\n    | restaurant_name |text (bm25 Index)|\n    +-----------------+-----------------+\n    |restaurant_review|text (bm25 Index)|\n    +-----------------+-----------------+\n    |  owner_answer   |text (bm25 Index)|\n    +-----------------+-----------------+\n    \n\n### Search for the restaurant using a specific sentence\u00b6\n\nWe define a query, `\"I want burritos\"`, to find relevant restaurant reviews in\nthe dataset. Using `ds_bm25.query()`, we search and rank entries in\n`restaurant_review` based on **BM25 similarity** to the query. The code orders\nresults by how well they match the query (`BM25_SIMILARITY`), from highest to\nlowest relevance, and limits the output to the top 10 results. The final list\nof results is stored in `view_bm25`.",
        "node_187": "in a Q&A App \n  * Accessing the Low Level Deep Lake API (Advanced) \n  * SelfQueryRetriever with Deep Lake \n\n# Using Deep Lake as a Vector Store in LangChain\u00b6\n\n## How to Use Deep Lake as a Vector Store in LangChain\u00b6\n\nDeep Lake can be used as a VectorStore in LangChain for building Apps that\nrequire filtering and vector search. In this tutorial we will show how to\ncreate a Deep Lake Vector Store in LangChain and use it to build a Q&A App\nabout the Twitter OSS recommendation algorithm. This tutorial requires\ninstallation of:\n\nInstall the main libraries:\n\n    \n    \n    pip install --upgrade --quiet  langchain-openai langchain-deeplake tiktoken\n    \n\n## Downloading and Preprocessing the Data\u00b6\n\nFirst, let's import necessary packages and make sure the Activeloop and OpenAI\nkeys are in the environmental variables `ACTIVELOOP_TOKEN`, `OPENAI_API_KEY`.\n\n    \n    \n    import os\n    import getpass\n    from langchain_openai import OpenAIEmbeddings\n    from langchain_deeplake.vectorstores import DeeplakeVectorStore\n    from langchain_community.document_loaders import TextLoader\n    from langchain_text_splitters import CharacterTextSplitter\n    from langchain.chains import RetrievalQA\n    from langchain_openai import ChatOpenAI\n    \n\nNext, we set up environmental variables\n\n    \n    \n    if \"OPENAI_API_KEY\" not in os.environ:\n        os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n    \n    if \"ACTIVELOOP_TOKEN\" not in os.environ:\n        os.environ[\"ACTIVELOOP_TOKEN\"] = getpass.getpass(\"activeloop token:\")\n    \n\nNext, let's clone the Twitter OSS recommendation algorithm:\n\n    \n    \n    !git clone https://github.com/twitter/the-algorithm\n    \n\nNext, let's load all the files from the repo into a list:\n\n    \n    \n    repo_path = '/the-algorithm'\n    \n    docs = []\n    for dirpath, dirnames, filenames in os.walk(repo_path):\n        for file in filenames:\n            try:\n                loader = TextLoader(os.path.join(dirpath, file), encoding='utf-8')\n                docs.extend(loader.load_and_split())\n            except Exception as e:\n                print(e)\n                pass\n    \n\n## A note on chunking text files\u00b6\n\nText files are typically split into chunks before creating embeddings.",
        "node_241": "See deeplake.open_read_only for opening the dataset in read only mode\n\nTo create a new dataset, see deeplake.create\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`url` |  `str` |  The URL of the dataset. URLs can be specified using the following protocols:\n\n  * `file://path` local filesystem storage\n  * `al://org_id/dataset_name` A dataset on app.activeloop.ai\n  * `azure://bucket/path` or `az://bucket/path` Azure storage\n  * `gs://bucket/path` or `gcs://bucket/path` or `gcp://bucket/path` Google Cloud storage\n  * `s3://bucket/path` S3 storage\n  * `mem://name` In-memory storage that lasts the life of the process\n\nA URL without a protocol is assumed to be a file:// URL |  _required_  \n`creds` |  `(dict, str)` |  The string `ENV` or a dictionary containing credentials used to access the dataset at the path.\n\n  * If 'aws_access_key_id', 'aws_secret_access_key', 'aws_session_token' are present, these take precedence over credentials present in the environment or in credentials file. Currently only works with s3 paths.\n  * It supports 'aws_access_key_id', 'aws_secret_access_key', 'aws_session_token', 'endpoint_url', 'aws_region', 'profile_name' as keys.\n  * To use credentials managed in your Activeloop organization, use they key 'creds_key': 'managed_key_name'. This requires the org_id dataset argument to be set.\n  * If nothing is given is, credentials are fetched from the environment variables. This is also the case when creds is not passed for cloud datasets\n\n|  `None`  \n`token` |  `str` |  Activeloop token, used for fetching credentials to the dataset at path if it is a Deep Lake dataset. This is optional, tokens are normally autogenerated.",
        "node_775": "An additional\ndistinction is that DVC primarily uses a command-line interface, whereas Deep\nLake is a Python package. Lastly, Deep Lake offers an API to easily connect\ndatasets to ML frameworks and other common ML tools and enables instant\ndataset visualization through Activeloop's visualization tool.\n\n**Deep Lake vs MosaicML MDS format**\n\n  * **Data Storage Format:** Deep Lake operates on a columnar storage format, whereas MDS utilizes a row-wise storage approach. This fundamentally impacts how data is read, written, and organized in each system.\n  * **Compression:** Deep Lake offers a more flexible compression scheme, allowing control over both chunk-level and sample-level compression for each column or tensor. This feature eliminates the need for additional compressions like zstd, which would otherwise demand more CPU cycles for decompressing on top of formats like jpeg.\n  * **Shuffling:** MDS currently offers more advanced shuffling strategies.\n  * **Version Control & Visualization Support:** A notable feature of Deep Lake is its native version control and in-browser data visualization, a feature not present for MosaicML data format. This can provide significant advantages in managing, understanding, and tracking different versions of the data.\n\n**Deep Lake vs TensorFlow Datasets (TFDS)**\n\nDeep Lake and TFDS seamlessly connect popular datasets to ML frameworks. Deep\nLake datasets are compatible with both PyTorch and TensorFlow, whereas TFDS\nare only compatible with TensorFlow. A key difference between Deep Lake and\nTFDS is that Deep Lake datasets are designed for streaming from the cloud,\nwhereas TFDS must be downloaded locally prior to use.",
        "node_90": "On the bottom right, click `Next: Tags` (create tags if needed) and `Next:\nPreview`, enter the policy `name` and `description`, and click `Create policy`\n\n#### Step 2: Create the AWS IAM Role \u00b6\n\n1\\. On the `IAM` page, in the left nav, open the `Roles` under `Access\nmanagement`, and click `Create role` on the right.\n\n3\\. Select `Custom trust policy` from the list of options.\n\n4\\. Replace the policy definition with the code below and click `Next`\n\n    \n    \n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": \n        [\n            {\n                \"Sid\": \"AllowAssumeRoleFromActiveloopSaaS\",\n                \"Effect\": \"Allow\",\n                \"Principal\": {\n                     \"AWS\": \"arn:aws:iam::597713067985:role/activeloop_backend\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n          }\n       ]\n    }\n    \n\n5\\. From the provided policy list, select the previously created policy from\nStep 1 and click `Next`\n\n6\\. Set the `name` and `description` for the role and click `Create role` at\nthe bottom.\n\n#### Step 3: Grant Access to AWS KMS Key (**only for buckets that are\nencrypted with customer managed KMS keys**)\u00b6\n\n1\\. Navigate to the bucket in the AWS S3 UI\n\n2\\. Open the bucket Properties\n\n3\\. Scroll down to Default encryption and copy the `AWS KMS key ARN`\n\n4\\. In the Policy creation step (Step 1, Sub-step 6), use the JSON below in\nthe policy statement, and replace `YOUR_KMS_KEY_ARN` with the copied Key ARN\nfor the encrypted bucket.",
        "node_653": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_385": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_361": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_474": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_464": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_282": "NOTE: Since segmentation masks often contain large amounts of data, it is\nrecommended to compress them using lz4.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`sample_compression` |  `str | None` |  How to compress each row's value. Possible values: lz4, null (default: null) |  `None`  \n`chunk_compression` |  `str | None` |  How to compress all the values stored in a single file. Possible values: lz4, null (default: null) |  `None`  \n  \nExamples:\n\n    \n    \n    ds.add_column(\"col1\", types.SegmentMask(sample_compression=\"lz4\"))\n    ds.append([{\"col1\": np.zeros((512, 512, 3))}])\n    \n    \n    \n    # Basic segmentation mask\n    ds.add_column(\"segmentation\", deeplake.types.SegmentMask())\n    \n    # With compression\n    ds.add_column(\"segmentation\", deeplake.types.SegmentMask(\n        dtype=\"uint8\",\n        sample_compression=\"lz4\"\n    ))\n    \n\n##  `` deeplake.types.BoundingBox \u00b6\n\n    \n    \n    BoundingBox(\n        dtype: DataType | str = \"float32\",\n        format: str | None = None,\n        bbox_type: str | None = None,\n    ) -> Type\n    \n\nStores an array of values specifying the bounding boxes of an image.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`dtype` |  `DataType | str` |  The datatype of values (default float32) |  `'float32'`  \n`format` |  `str | None` |  The bounding box format. Possible values: `ccwh`, `ltwh`, `ltrb`, `unknown` |  `None`  \n`bbox_type` |  `str | None` |  The pixel type.",
        "node_395": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_756": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.",
        "node_581": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_123": "If you prefer not to use a proprietary model, a high-performing\nmodel from this list is an excellent alternative.\n\nWe start by installing and importing the `openai` library to access OpenAI's\nAPI for generating embeddings. Next, we define the function\n`embedding_function`, which takes `texts` as input (either a single string or\na list of strings) and a model name, defaulting to `\"text-embedding-3-large\"`.\nThen, for each text, we replace newline characters with spaces to maintain\nclean, uniform text. Finally, we use `openai.embeddings.create()` to generate\nembeddings for each text and return a list of these embeddings, which can be\nused for cosine similarity comparisons.\n\n    \n    \n    !pip install openai\n    \n\nSets the OpenAI API key in the environment using `getpass`.\n\n    \n    \n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key: \")\n    \n    \n    \n    import openai\n    \n    def embedding_function(texts, model=\"text-embedding-3-large\"):\n        if isinstance(texts, str):\n            texts = [texts]\n    \n        texts = [t.replace(\"\\n\", \" \") for t in texts]\n        return [data.embedding for data in openai.embeddings.create(input = texts, model=model).data]\n    \n\n### Create the dataset and add the columns\u00b6\n\nNext, we add three columns to `vector_search`:\n\n  1. `embedding`: Stores vector embeddings with a dimension size of 3072, which will enable vector-based similarity searches.\n\n  2. `restaurant_name`: A text column with a **BM25 index** , optimizing it for relevance-based text search.\n\n  3. `restaurant_review`: Another text column with a **BM25 index** , also optimized for efficient and ranked search results.\n\n  4. `owner_answer`: A text column with an **inverted index** , allowing fast and efficient filtering based on specific owner answer.\n\nFinally, we use `vector_search.commit()` to save these new columns, ensuring\nthe dataset structure is ready for further data additions and queries.",
        "node_418": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_613": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_338": "This functionality is needed for the following use cases:\n\n  * Moving datasets between cloud providers \n  * Creating local copies of cloud datasets for faster access\n  * Backing up datasets to different storage providers\n  * Maintaining synchronized dataset replicas\n\n## Copying Datasets\u00b6\n\nCopy a dataset to a new location while preserving all data, metadata, and\nversion history:\n\n    \n    \n    # Copy between storage providers\n    deeplake.copy(\n        src=\"s3://source-bucket/dataset\",\n        dst=\"gcs://dest-bucket/dataset\", \n        dst_creds={\"credentials\": \"for-dest-storage\"}\n    )\n    \n    # Create local copy of cloud dataset  \n    deeplake.copy(\n        src=\"al://org/dataset\",\n        dst=\"./local/dataset\"\n    )\n    \n\n## Dataset Synchronization\u00b6\n\n### Pull Changes\u00b6\n\nSync a dataset with its source by pulling new changes:\n\n    \n    \n    # Create dataset copy\n    deeplake.copy(\"s3://source/dataset\", \"s3://replica/dataset\")\n    \n    replica_ds = deeplake.open(\"s3://replica/dataset\")\n    \n    # Later, pull new changes from source\n    replica_ds.pull(\n        url=\"s3://source/dataset\",\n        creds={\"aws_access_key_id\": \"key\", \"aws_secret_access_key\": \"secret\"}\n    )\n    \n    # Pull changes asynchronously\n    async def pull_async():\n        await replica_ds.pull_async(\"s3://source/dataset\") \n    \n\n### Push Changes\u00b6\n\nPush local changes to another dataset location:\n\n    \n    \n    # Make changes to dataset\n    ds.append({\"images\": new_images})\n    ds.commit()\n    \n    # Push changes to replica\n    ds.push(\n        url=\"s3://replica/dataset\",\n        creds={\"aws_access_key_id\": \"key\", \"aws_secret_access_key\": \"secret\"}\n    )\n    \n    # Push changes asynchronously  \n    async def push_async():\n        await ds.push_async(\"s3://replica/dataset\")\n    \n\n## Synchronization Example\u00b6\n\n    \n    \n    # Initial dataset creation\n    source_ds = deeplake.",
        "node_243": "ds = deeplake.open(\"s3://bucket/my_dataset\",\n        creds = {\"aws_access_key_id\": id, \"aws_secret_access_key\": key})\n    \n    # Load dataset stored in your cloud using Deep Lake managed credentials.\n    ds = deeplake.open(\"s3://bucket/my_dataset\",\n        creds = {\"creds_key\": \"managed_creds_key\"}, org_id = \"my_org_id\")\n    \n    ds = deeplake.open(\"s3://bucket/path/to/dataset\")\n    \n    ds = deeplake.open(\"azure://bucket/path/to/dataset\")\n    \n    ds = deeplake.open(\"gcs://bucket/path/to/dataset\")\n    \n\n###  `` deeplake.open_read_only \u00b6\n\n    \n    \n    open_read_only(\n        url: str,\n        creds: dict[str, str] | None = None,\n        token: str | None = None,\n    ) -> ReadOnlyDataset\n    \n\nOpens an existing dataset in read-only mode.\n\nSee deeplake.open for opening datasets for modification.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`url` |  `str` |  The URL of the dataset. URLs can be specified using the following protocols:\n\n  * `file://path` local filesystem storage\n  * `al://org_id/dataset_name` A dataset on app.activeloop.ai\n  * `azure://bucket/path` or `az://bucket/path` Azure storage\n  * `gs://bucket/path` or `gcs://bucket/path` or `gcp://bucket/path` Google Cloud storage\n  * `s3://bucket/path` S3 storage\n  * `mem://name` In-memory storage that lasts the life of the process\n\nA URL without a protocol is assumed to be a file:// URL |  _required_  \n`creds` |  `(dict, str)` |  The string `ENV` or a dictionary containing credentials used to access the dataset at the path.",
        "node_261": "Examples:\n\n    \n    \n    ds.summary()\n    \n\n## Class Comparison\u00b6\n\n### Dataset\u00b6\n\n  * Full read-write access\n  * Can create/modify columns\n  * Can append/update data\n  * Can commit changes\n  * Can create version tags\n  * Can push/pull changes\n\n    \n    \n    ds = deeplake.create(\"s3://bucket/dataset\")\n    # or\n    ds = deeplake.open(\"s3://bucket/dataset\")\n    \n    # Can modify\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"labels\", deeplake.types.ClassLabel(\"int32\"))\n    ds.add_column(\"confidence\", \"float32\")\n    ds[\"labels\"].metadata[\"class_names\"] = [\"cat\", \"dog\"]   \n    ds.append([{\"images\": image_array, \"labels\": 0, \"confidence\": 0.9}])\n    ds.commit()\n    \n\n### ReadOnlyDataset\u00b6\n\n  * Read-only access\n  * Cannot modify data or schema\n  * Can view all data and metadata\n  * Can execute queries\n  * Returned by `open_read_only()`\n\n    \n    \n    ds = deeplake.open_read_only(\"s3://bucket/dataset\")\n    \n    # Can read\n    image = ds[\"images\"][0]\n    metadata = ds.metadata\n    \n    # Cannot modify\n    # ds.append([.])  # Would raise error\n    \n\n### DatasetView\u00b6\n\n  * Read-only access\n  * Cannot modify data\n  * Optimized for query results\n  * Direct integration with ML frameworks\n  * Returned by `query()`\n\n    \n    \n    # Get view through query\n    view = ds.query(\"SELECT *\")\n    \n    # Access data\n    image = view[\"images\"][0]\n    \n    # ML framework integration\n    torch_dataset = view.pytorch()\n    tf_dataset = view.tensorflow()\n    \n\n## Examples\u00b6\n\n### Querying Data\u00b6\n\n    \n    \n    # Using Dataset\n    ds = deeplake.open(\"s3://bucket/dataset\")\n    results = ds.",
        "node_703": "'}, score=0.013747293509625419),\n     '5136': Document(id='5136', data={'restaurant_name': 'Scratch', 'restaurant_review': 'Just had drinks. They were good!'}, score=0.024505473374994282),\n     '17426': Document(id='17426', data={'restaurant_name': \"St. Stephen's Green\", 'restaurant_review': 'Good drinks an easy going bartenders'}, score=0.024579178342433523),\n     '17444': Document(id='17444', data={'restaurant_name': \"St. Stephen's Green\", 'restaurant_review': 'Good drinks, good food'}, score=0.02475096426920331),\n     '2496': Document(id='2496', data={'restaurant_name': 'Seasons Noodles & Dumplings Garden', 'restaurant_review': 'Comfort food, excellent service! Feel like back to home.'}, score=0.005430922978171003),\n     '4022': Document(id='4022', data={'restaurant_name': 'Eureka! Mountain View', 'restaurant_review': 'Good drinks and burgers'}, score=0.0246334319067476),\n     '3518': Document(id='3518', data={'restaurant_name': 'Olympus Caffe & Bakery', 'restaurant_review': 'I like the garden to sit down with friends and have a drink.'}, score=0.08915287739833623),\n     '17502': Document(id='17502', data={'restaurant_name': \"St.",
        "node_630": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_168": "By contrast, after myocardial infarction, susceptible dogs showed significant decrease in all measures of heart rate variability. Before myocardial infarction, no differences were seen between susceptible and resistant dogs. However, 30 days after infarction, epidemiologic analysis of the coefficient of variance showed high sensitivity and specificity (88% and 80%, respectively), predicting susceptibility. Therefore, results of analysis of 30 min of beat to beat heart period at rest 30 days after myocardial infarction are highly predictive for increased risk of sudden death. \\n5\\tMultiple organ failure: inflammatory priming and activation sequences promote autologous tissue injury. Systemic inflammation promotes multiple organ failure through the induction of diffuse microvascular leak. Inflammatory cells such as neutrophil\n    Text: Risk for sudden death was assessed 1 month after myocardial infarction by a protocol in which exercise and myocardial ischemia were combined; dogs that developed ventricular fibrillation were classified at high risk for sudden death (susceptible) and the survivors were considered low risk (resistant). In resistant dogs, myocardial infarction did not affect any measure of heart rate variability: 1) mean RR interval, 2) standard deviation of the mean RR interval, and 3) the coefficient of variance (standard deviation/RR interval). By contrast, after myocardial infarction, susceptible dogs showed significant decrease in all measures of heart rate variability. Before myocardial infarction, no differences were seen between susceptible and resistant dogs. However, 30 days after infarction, epidemiologic analysis of the coefficient of variance showed high sensitivity and specificity (88% and 80%, respectively), predicting susceptibility. Therefore, results of analysis of 30 min of beat to beat heart period at rest 30 days after myocardial infarction are highly predictive for increased risk of sudden death. \\n5\\tMultiple organ failure: inflammatory priming and activation sequences promote autologous tissue injury.\n    Text: However, no paired studies have been reported to examine heart rate variability before and after myocardial infarction.",
        "node_574": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_482": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_521": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_631": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_582": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_784": "We\nwould like to thank William Silversmith @SeungLab for his awesome cloud-volume\ntool.\n\n## About\n\nDatabase for AI. Store Vectors, Images, Texts, Videos, etc. Use with\nLLMs/LangChain. Store, query, version, & visualize any AI data. Stream data in\nreal-time to PyTorch/TensorFlow. https://activeloop.ai\n\nactiveloop.ai\n\n### Topics\n\npython  data-science  machine-learning  ai  computer-vision  deep-learning\ntensorflow  cv  image-processing  ml  pytorch  datasets  multi-modal  datalake\nmlops  vector-search  vector-database  large-language-models  llm  langchain\n\n### Resources\n\nReadme\n\n### License\n\nApache-2.0 license\n\n### Security policy\n\nSecurity policy\n\nActivity\n\nCustom properties\n\n### Stars\n\n**8.5k** stars\n\n### Watchers\n\n**93** watching\n\n### Forks\n\n**656** forks\n\nReport repository\n\n##  Releases 238\n\nv4.1.17 \ud83c\udf08 Latest\n\nMar 31, 2025\n\n\\+ 237 releases\n\n##  Packages 0\n\nNo packages published  \n\n##  Used by 3.2k\n\n  *   *   *   *   *   *   *   * \n\\+ 3,216\n\n##  Contributors 95\n\n  *   *   *   *   *   *   *   *   *   *   *   *   *   * \n\n\\+ 81 contributors\n\n## Languages\n\n  * Python 100.0%\n\n## Footer\n\n(C) 2025 GitHub, Inc.",
        "node_534": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_147": "4. **Generate and Parse the Response** : \n\n     * Using `client.chat.completions.create()`, the system and user prompts are sent to the LLM (specified as `gpt-4o-mini`).\n\n     * The response is parsed as JSON, extracting the `answer` field. If parsing fails, `False` is returned.\n\n    \n    \n    import json\n    from openai import OpenAI\n    \n    client = OpenAI()\n    \n    def generate_question(question:str, information:list):\n        system_prompt = f\"\"\"You are a helpful assistant specialized in providing answers to questions about restaurants. Below is a question from a user, along with the top four relevant information chunks about restaurants from a Deep Lake database. Using these chunks, construct a clear and informative answer that addresses the question, incorporating key details without repeating information.\n        The output must be in JSON format with the following structure:\n        {{\n            \"answer\": \"The answer to the question.\"\n        }}\n    \n        \"\"\"\n    \n        user_prompt = f\"Here is a question from a user: {question}\\n\\nHere are the top relevant information about restaurants {information}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},\n            ],\n            response_format={\"type\": \"json_object\"},\n        )\n    \n        try:\n            response = response.choices[0].message.content\n            response = json.loads(response)\n            questions = response[\"answer\"]\n            return questions\n        except:\n            return False\n    \n\nThis function takes a restaurant-related question and retrieves the best\nresponse based on the given context. It completes the RAG process by combining\nrelevant information and LLM-generated content into a concise answer.",
        "node_671": "* Search for the restaurant using a specific sentence \n      * 4) Explore Results with Hybrid Search \n        * Search for the correct restaurant using a specific sentence \n        * Show the scores \n        * Normalize the score \n        * Fusion method \n        * Show the results \n        * Let's run a search on a multiple dataset \n        * Comparison of Sync vs Async Query Performance \n      * 5) Integrating Image Embeddings for Multi-Modal Search \n        * Create the embedding function for images \n        * Create a new dataset to save the images \n        * Convert the URLs into images \n        * Search similar images \n        * Performing a similar image search based on a specific image \n        * Show similar images and the their respective restaurants \n      * 6) ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT \n      * 7) Discover Restaurants Using ColPali and the Late Interaction Mechanism \n        * Download the ColPali model \n        * Create a new dataset to store the ColPali embeddings \n        * Save the data in the dataset \n        * Chat with images \n        * Retrieve the most similar images \n        * VQA: Visual Question Answering \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    *",
        "node_151": "Awesome food. I love their wet Chili Verde burritos\n    \n\n### Comparison of Sync vs Async Query Performance\u00b6\n\nThis code performs an asynchronous query on a Deep Lake dataset. It begins by\nopening the dataset asynchronously using `await deeplake.open_async()`,\nspecifying `org_id` and `dataset_name_vs`.\n\n    \n    \n    ds_async = await deeplake.open_async(f\"al://{org_id}/{dataset_name_vs}\")\n    ds_async_results = ds_async.query_async(tql_vs).result()\n    \n\nThis following code compares the execution times of synchronous and\nasynchronous queries on a Deep Lake dataset:\n\n  * First, it records the start time `start_sync` for the synchronous query, executes the query with `vector_search.query(tql_vs)`, and then records the end time `end_sync`. It calculates and prints the total time taken for the synchronous query by subtracting `start_sync` from `end_sync`.\n  * Next, it measures the asynchronous query execution by recording `start_async`, running `vector_search.query_async(tql_vs).result()` to execute and retrieve the query result asynchronously, and then recording `end_async`. The asynchronous query time is calculated as the difference between `end_async` and `start_async`, and is printed.\n\nThe code executes two queries both synchronously and asynchronously, measuring\nthe execution time for each method. In the synchronous part, the queries are\nexecuted one after the other, and the execution time is recorded. In the\nasynchronous part, the queries are run concurrently using `asyncio.gather()`\nto parallelize the asynchronous calls, and the execution time is also\nmeasured. The \"speed factor\" is then calculated by comparing the execution\ntimes, showing how much faster the asynchronous execution is compared to the\nsynchronous one. Using `asyncio.gather()` allows the asynchronous queries to\nrun in parallel, reducing the overall execution time.",
        "node_759": "Skip to content\n\n## Navigation Menu\n\nToggle navigation\n\nSign in\n\n  * Product \n\n    * GitHub Copilot\n\nWrite better code with AI\n\n    * GitHub Advanced Security\n\nFind and fix vulnerabilities\n\n    * Actions\n\nAutomate any workflow\n\n    * Codespaces\n\nInstant dev environments\n\n    * Issues\n\nPlan and track work\n\n    * Code Review\n\nManage code changes\n\n    * Discussions\n\nCollaborate outside of code\n\n    * Code Search\n\nFind more, search less\n\nExplore\n\n    * All features \n    * Documentation \n    * GitHub Skills \n    * Blog \n\n  * Solutions \n\nBy company size\n\n    * Enterprises \n    * Small and medium teams \n    * Startups \n    * Nonprofits \n\nBy use case\n\n    * DevSecOps \n    * DevOps \n    * CI/CD \n    * View all use cases \n\nBy industry\n\n    * Healthcare \n    * Financial services \n    * Manufacturing \n    * Government \n    * View all industries \n\nView all solutions\n\n  * Resources \n\nTopics\n\n    * AI \n    * DevOps \n    * Security \n    * Software Development \n    * View all \n\nExplore\n\n    * Learning Pathways \n    * Events & Webinars \n    * Ebooks & Whitepapers \n    * Customer Stories \n    * Partners \n    * Executive Insights \n\n  * Open Source \n\n    * GitHub Sponsors\n\nFund open source developers\n\n    * The ReadME Project\n\nGitHub community articles\n\nRepositories\n\n    * Topics \n    * Trending \n    * Collections \n\n  * Enterprise \n\n    * Enterprise platform\n\nAI-powered developer platform\n\nAvailable add-ons\n\n    * GitHub Advanced Security\n\nEnterprise-grade security features\n\n    * Copilot for business\n\nEnterprise-grade AI features\n\n    * Premium Support\n\nEnterprise-grade 24/7 support\n\n  * Pricing\n\nSearch or jump to.",
        "node_472": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_203": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nAsync Data Loader\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader  DataLoader  Table of contents \n        * Overview \n        * Dataset Structure \n        * Sequential data fetching \n        * Asynchronous Data Fetching \n        * Benchmark results \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Overview \n  * Dataset Structure \n  * Sequential data fetching \n  * Asynchronous Data Fetching \n  * Benchmark results \n\n# Async Data Loader\u00b6\n\n## Overview\u00b6\n\nThis document describes the implementation of a custom DataLoader for handling\ndata retrieval using `deeplake.Dataset` with `PyTorch`. The DataLoader\nsupports both sequential and asynchronous data fetching, with the asynchronous\napproach being optimized for performance and speed.\n\n## Dataset Structure\u00b6\n\nThe dataset comprises pairs of images and their respective masks.",
        "node_634": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_486": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_705": "Awesome food. I love their wet Chili Verde burritos'}, score=0.00522136854599736)}\n    \n\nWe sort the results dictionary by each document's combined score in descending\norder, ensuring that the highest-ranking documents appear first.\n\n    \n    \n    sorted_documents = dict(sorted(results.items(), key=lambda item: item[1].score, reverse=True))\n    print(sorted_documents)\n    \n\nOutput:\n\n    \n    \n    {'3518': Document(id='3518', data={'restaurant_name': 'Olympus Caffe & Bakery', 'restaurant_review': 'I like the garden to sit down with friends and have a drink.'}, score=0.3566115095933449),\n     '17502': Document(id='17502', data={'restaurant_name': \"St. Stephen's Green\", 'restaurant_review': 'Nice place for a drink'}, score=0.10612380842648524),\n     '17444': Document(id='17444', data={'restaurant_name': \"St. Stephen's Green\", 'restaurant_review': 'Good drinks, good food'}, score=0.09900385707681324),\n     '4022': Document(id='4022', data={'restaurant_name': 'Eureka! Mountain View', 'restaurant_review': 'Good drinks and burgers'}, score=0.0985337276269904),\n     '17426': Document(id='17426', data={'restaurant_name': \"St. Stephen's Green\", 'restaurant_review': 'Good drinks an easy going bartenders'}, score=0.09831671336973409),\n     '5136': Document(id='5136', data={'restaurant_name': 'Scratch', 'restaurant_review': 'Just had drinks. They were good!'}, score=0.09802189349997713),\n     '2637': Document(id='2637', data={'restaurant_name': 'Mifen101 \u82b1\u6eaa\u7c73\u7c89\u738b', 'restaurant_review': 'Feel like I\u2019m back in China.",
        "node_142": "Awesome food. I love their wet Chili Verde burritos'}, score=0.00522136854599736)}\n    \n\nWe sort the results dictionary by each document's combined score in descending\norder, ensuring that the highest-ranking documents appear first.\n\n    \n    \n    sorted_documents = dict(sorted(results.items(), key=lambda item: item[1].score, reverse=True))\n    print(sorted_documents)\n    \n\nOutput:\n\n    \n    \n    {'3518': Document(id='3518', data={'restaurant_name': 'Olympus Caffe & Bakery', 'restaurant_review': 'I like the garden to sit down with friends and have a drink.'}, score=0.3566115095933449),\n     '17502': Document(id='17502', data={'restaurant_name': \"St. Stephen's Green\", 'restaurant_review': 'Nice place for a drink'}, score=0.10612380842648524),\n     '17444': Document(id='17444', data={'restaurant_name': \"St. Stephen's Green\", 'restaurant_review': 'Good drinks, good food'}, score=0.09900385707681324),\n     '4022': Document(id='4022', data={'restaurant_name': 'Eureka! Mountain View', 'restaurant_review': 'Good drinks and burgers'}, score=0.0985337276269904),\n     '17426': Document(id='17426', data={'restaurant_name': \"St. Stephen's Green\", 'restaurant_review': 'Good drinks an easy going bartenders'}, score=0.09831671336973409),\n     '5136': Document(id='5136', data={'restaurant_name': 'Scratch', 'restaurant_review': 'Just had drinks. They were good!'}, score=0.09802189349997713),\n     '2637': Document(id='2637', data={'restaurant_name': 'Mifen101 \u82b1\u6eaa\u7c73\u7c89\u738b', 'restaurant_review': 'Feel like I\u2019m back in China.",
        "node_176": "figQA_dataset = \"figQA_dataset\"\n    fig_qa = deeplake.open_read_only(f\"al://activeloop/{figQA_dataset}\")\n    figure_images = [Image.fromarray(el[\"image\"]) for el in fig_qa]\n    questions = [el[\"question\"] for el in  fig_qa]\n    \n\n### Create a new dataset to store the ColPali embeddings\u00b6\n\nWe create a Deep Lake dataset named `\"tabqa_colpali\"` for ColPali's table-\nbased question answering. Stored in `vector_search_images`, it includes an\n`embedding`** column for 2D float arrays, a `question` column for text, and an\n`image` column for table images. After defining the structure,\n`vector_search_images.commit()` saves the setup, optimizing it for ColPali's\nmulti-modal retrieval in table QA tasks.\n\n    \n    \n    late_interaction_dataset_name = \"figQA_colpali\"\n    vector_search_images = deeplake.create(f\"al://{org_id}/{late_interaction_dataset_name}\")\n    \n    vector_search_images.add_column(name=\"embedding\", dtype=types.Array(types.Float32(),dimensions=2))\n    vector_search_images.add_column(name=\"question\", dtype=types.Text())\n    vector_search_images.add_column(name=\"image\", dtype=types.Image(dtype=types.UInt8()))\n    \n    vector_search_images.commit()\n    \n\n### Save the data in the dataset\u00b6\n\nWe batch-process and store ColPali embeddings for table-based question\nanswering. Using a `batch_size` of 2, we take the first 10 tables and\nquestions from `table_qa`. For each pair, if `question` is a single string,\nit's converted to a list. The `table_image` is processed in batches, passed\nthrough `processor` and ColPali, and embeddings are generated without\ngradients. These embeddings are stored as lists and appended with each\nquestion and image to `vector_search_images`.Finally,\n`vector_search_images.commit()` saves everything for efficient retrieval.",
        "node_353": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_682": "Next, we create a new dataset with the\nspecified name and location in Deep Lake storage.\n\nWe then add two columns to the dataset: `restaurant_name` and\n`restaurant_review`. Both columns use a BM25 index, which optimizes them for\nrelevance-based searches, enhancing the ability to rank results based on how\nwell they match search terms.\n\nFinally, we use `ds_bm25.commit()` to save these changes to the dataset and\n`ds_bm25.summary()` to display an overview of the dataset's structure and\ncontents.\n\nIf you don't have a token yet, you can sign up and then log in on the official\nActiveloop website, then click the `Create API token` button to obtain a new\nAPI token. Here, under `Select organization`, you can also find your\norganization ID(s).\n\n    \n    \n    import os, getpass\n    os.environ[\"ACTIVELOOP_TOKEN\"] = getpass.getpass(\"Activeloop API token: \")\n    \n    \n    \n    org_id = \"<your_org_id>\" \n    dataset_name_bm25 = \"bm25_test\"\n    \n    ds_bm25 = deeplake.create(f\"al://{org_id}/{dataset_name_bm25}\")\n    \n    \n    \n    # Add columns to the dataset\n    ds_bm25.add_column(\"restaurant_name\", types.Text(index_type=types.BM25))\n    ds_bm25.add_column(\"restaurant_review\", types.Text(index_type=types.BM25))\n    ds_bm25.add_column(\"owner_answer\", types.Text(index_type=types.BM25))\n    ds_bm25.commit()\n    ds_bm25.summary()\n    \n\nOutput:\n\n    \n    \n    Dataset(columns=(restaurant_name,restaurant_review,owner_answer), length=0)\n    +-----------------+-----------------+\n    |     column      |      type       |\n    +-----------------+-----------------+\n    | restaurant_name |text (bm25 Index)|\n    +-----------------+-----------------+\n    |restaurant_review|text (bm25 Index)|\n    +-----------------+-----------------+\n    |  owner_answer   |text (bm25 Index)|\n    +-----------------+-----------------+\n    \n\n### Add data to the dataset\u00b6\n\nWe add data to the `ds_bm25` dataset by appending the two columns, filled with\nvalues from the lists we previously created.",
        "node_108": "* Search for the restaurant using a specific sentence \n      * 4) Explore Results with Hybrid Search \n        * Search for the correct restaurant using a specific sentence \n        * Show the scores \n        * Normalize the score \n        * Fusion method \n        * Show the results \n        * Let's run a search on a multiple dataset \n        * Comparison of Sync vs Async Query Performance \n      * 5) Integrating Image Embeddings for Multi-Modal Search \n        * Create the embedding function for images \n        * Create a new dataset to save the images \n        * Convert the URLs into images \n        * Search similar images \n        * Performing a similar image search based on a specific image \n        * Show similar images and the their respective restaurants \n      * 6) ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT \n      * 7) Discover Restaurants Using ColPali and the Late Interaction Mechanism \n        * Download the ColPali model \n        * Create a new dataset to store the ColPali embeddings \n        * Save the data in the dataset \n        * Chat with images \n        * Retrieve the most similar images \n        * VQA: Visual Question Answering \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to",
        "node_202": "class_names = [item for item in os.listdir(dataset_folder) if\nos.path.isdir(os.path.join(dataset_folder, item))] files_list = [] for\ndirpath, dirnames, filenames in os.walk(dataset_folder): for filename in\nfilenames: files_list.append(os.path.join(dirpath, filename))\n\nNext, let's **create the dataset columns and upload data**.\n\nIn [ ]:\n\nCopied!\n\n    \n    \n    ds.add_column('images', dtype = types.Image(sample_compression = \"jpg\"))\n    ds.add_column('labels', dtype = types.Array( dtype = types.UInt32(), dimensions=1))\n    \n    # Iterate through the files and append to Deep Lake dataset\n    for file in files_list:\n        label_text = os.path.basename(os.path.dirname(file))\n        label_num = class_names.index(label_text)\n    \n        #Append data to the tensors\n        ds.append({'images': [open(file, \"rb\").read()], 'labels': [label_num]})\n    \n\nds.add_column('images', dtype = types.Image(sample_compression = \"jpg\"))\nds.add_column('labels', dtype = types.Array( dtype = types.UInt32(),\ndimensions=1)) # Iterate through the files and append to Deep Lake dataset for\nfile in files_list: label_text = os.path.basename(os.path.dirname(file))\nlabel_num = class_names.index(label_text) #Append data to the tensors\nds.append({'images': [open(file, \"rb\").read()], 'labels': [label_num]})\n\nIn [68]:\n\nCopied!\n\n    \n    \n    ds.summary()\n    \n\nds.summary()\n\n    \n    \n    Dataset(columns=(images,labels), length=0)\n    +------+------------------------------------------+\n    |column|                   type                   |\n    +------+------------------------------------------+\n    |images|array(dtype=uint8, shape=[None,None,None])|\n    +------+------------------------------------------+\n    |labels|    array(dtype=uint32, shape=[None])     |\n    +------+------------------------------------------+\n    \n    \n\nNotebook\n\nBack to top",
        "node_322": "This will prevent accidental modifications to the\ndataset and will improve the performance of the data access.\n\n### Prefer batch access instead of row by row access\u00b6\n\nIf you need to iterate over the dataset or specific column, prefer using\nbatches instead of row by row access.\n\n    \n    \n    # Process all columns with batches. Fast approach.\n    batch_size = 500\n    for batch in ds.batches(batch_size):\n        print(batch[\"column1\"])\n    \n    # Process single column with batches. Fast approach.\n    column = ds[\"column1\"]\n    for i in range(0, len(column), batch_size):\n        print(column[i:i+batch_size])\n    \n    # Process all columns row by row. Slower than batch access.\n    for row in ds:\n        print(row[\"column1\"])\n    \n    # Process single column row by row. Slower than batch access.\n    for i in range(len(column)):\n        print(column[i])\n    \n\n### Use `query` for complex data filtering and search\u00b6\n\nDeep Lake supports SQL-like queries to filter and search the data. If you need\nto filter the data based on multiple columns or complex conditions, prefer\nusing the deeplake.DatasetView.query or deeplake.query method, instead of\ndoing manual iteration and filtering.\n\n### Avoid accessing the data of the whole column\u00b6\n\nThe column data can be accessed directly by `ds[\"column_name\"][:]`. For the\nlarge datasets this can lead to memory issues. Prefer divide the data into\nbatches and process them separately.\n\n### Consider using async data access\u00b6\n\nDeep Lake supports async data access and query. If your workflow allows async\nprocessing and benefits from that, consider using async data access. Please\nrefer to the Async Data Loader guide for the details.\n\n## Storage and Data Management\u00b6\n\n### Understand the storage differences\u00b6\n\nDeep Lake supports multiple storage backends, which are differentiated by url\nschema.\n\n  * In memory datasets: `mem://dataset_id`. These datasets are stored in memory and are not persisted. They are useful for temporary data storage and testing.\n  * Local datasets: `file://path/to/dataset`. These datasets are stored on the local disk.",
        "node_676": "An inverted index is a data structure commonly used in search engines and\ndatabases to facilitate fast full-text searches. Unlike a row-wise search,\nwhich scans each row of a document or dataset for a search term, an inverted\nindex maps each unique word or term to the locations (such as document IDs or\nrow numbers) where it appears. This setup allows for very efficient retrieval\nof information, especially in large datasets.\n\nFor small datasets with up to 1,000 documents, row-wise search can provide\nefficient performance without needing an inverted index. For medium-sized\ndatasets (10,000+ documents), inverted indexes become useful, particularly if\nsearch queries are frequent. For large datasets of 100,000+ documents, using\nan inverted index is essential to ensure efficient query processing and meet\nperformance expectations.\n\n    \n    \n    import deeplake\n    from deeplake import types\n    \n    # Create a dataset\n    inverted_index_dataset = \"local_inverted_index\"\n    ds = deeplake.create(f\"file://{inverted_index_dataset}\")\n    \n\n### Extract the data\u00b6\n\nThis code extracts restaurant details from `scraped_data` into separate lists:\n\n  1. **Initialize Lists** : `restaurant_name`, `restaurant_review` and `owner_answer` are initialized to store respective data for each restaurant.\n\n  2. **Populate Lists** : For each entry (`el`) in `scraped_data`, the code appends:\n\n     * `el['restaurant_name']` to `restaurant_name`\n     * `el['restaurant_review']` to `restaurant_review`\n     * `el['owner_answer']` to `owner_answer`\n\nAfter running, each list holds a specific field from all restaurants, ready\nfor further processing.\n\n    \n    \n    restaurant_name = []\n    restaurant_review = []\n    owner_answer = []\n    images = []\n    for el in scraped_data:\n        restaurant_name.append(el['restaurant_name'])\n        restaurant_review.append(el['restaurant_review'])\n        owner_answer.append(el['owner_answer'])\n    \n\n### Add the data to the dataset\u00b6\n\nWe add the collected restaurant names and reviews to the dataset `ds`.",
        "node_67": "We\nwould like to thank William Silversmith @SeungLab for his awesome cloud-volume\ntool.\n\n## About\n\nDatabase for AI. Store Vectors, Images, Texts, Videos, etc. Use with\nLLMs/LangChain. Store, query, version, & visualize any AI data. Stream data in\nreal-time to PyTorch/TensorFlow. https://activeloop.ai\n\nactiveloop.ai\n\n### Topics\n\npython  data-science  machine-learning  ai  computer-vision  deep-learning\ntensorflow  cv  image-processing  ml  pytorch  datasets  multi-modal  datalake\nmlops  vector-search  vector-database  large-language-models  llm  langchain\n\n### Resources\n\nReadme\n\n### License\n\nApache-2.0 license\n\n### Security policy\n\nSecurity policy\n\nActivity\n\nCustom properties\n\n### Stars\n\n**8.5k** stars\n\n### Watchers\n\n**93** watching\n\n### Forks\n\n**656** forks\n\nReport repository\n\n##  Releases 238\n\nv4.1.17 \ud83c\udf08 Latest\n\nMar 31, 2025\n\n\\+ 237 releases\n\n##  Packages 0\n\nNo packages published  \n\n##  Used by 3.2k\n\n  *   *   *   *   *   *   *   * \n\\+ 3,216\n\n##  Contributors 95\n\n  *   *   *   *   *   *   *   *   *   *   *   *   *   * \n\n\\+ 81 contributors\n\n## Languages\n\n  * Python 100.0%\n\n## Footer\n\n(C) 2025 GitHub, Inc.",
        "node_612": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_329": "open(\"al://org_name/dataset_name\")\n    result = ds.query(\"SELECT * WHERE id > 10\")\n    \n    # Query across datasets (requires FROM)\n    result = deeplake.query('SELECT * FROM \"al://my_org/dataset_name\" WHERE id > 10')\n    \n\n### Query Syntax\u00b6\n\n#### String Values\u00b6\n\nString literals must use single quotes:\n\n    \n    \n    SELECT * WHERE contains(column_name, 'text_value')\n    \n\n#### Special Characters\u00b6\n\nColumn or dataset names with special characters need double quotes:\n\n    \n    \n    SELECT * WHERE contains(\"column-name\", 'text_value')\n    SELECT * FROM \"al://my_org/dataset\" WHERE id > 10\n    \n\nTip\n\nWhen writing queries in Python, remember to properly escape quotes:\n\n    \n    \n    # Using escape characters\n    query = \"SELECT * WHERE contains(\\\"column-name\\\", 'text_value')\"\n    \n    # Using different quote types\n    query = 'SELECT * WHERE contains(\"column-name\", \\'text_value\\')'\n    \n    # Using triple quotes\n    query = \"\"\"\n        SELECT * WHERE contains(\"column-name\", 'text_value')\n    \"\"\"\n    \n\n## Vector Operations\u00b6\n\n### Similarity Search\u00b6\n\nTQL provides multiple methods for vector similarity search:\n\n    \n    \n    -- Cosine similarity (higher is more similar)\n    SELECT * \n    ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[0.1, 0.2, .]) DESC\n    LIMIT 100\n    \n    -- L2 norm/Euclidean distance (lower is more similar)\n    SELECT * \n    ORDER BY L2_NORM(embeddings - ARRAY[0.1, 0.2, .]) ASC\n    LIMIT 100\n    \n    -- L1 norm/Manhattan distance\n    SELECT * \n    ORDER BY L1_NORM(embeddings - ARRAY[0.1, 0.2, .]) ASC\n    LIMIT 100\n    \n    -- L\u221e norm/Chebyshev distance\n    SELECT * \n    ORDER BY LINF_NORM(embeddings - ARRAY[0.1, 0.2, .",
        "node_355": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_242": "* If 'aws_access_key_id', 'aws_secret_access_key', 'aws_session_token' are present, these take precedence over credentials present in the environment or in credentials file. Currently only works with s3 paths.\n  * It supports 'aws_access_key_id', 'aws_secret_access_key', 'aws_session_token', 'endpoint_url', 'aws_region', 'profile_name' as keys.\n  * To use credentials managed in your Activeloop organization, use they key 'creds_key': 'managed_key_name'. This requires the org_id dataset argument to be set.\n  * If nothing is given is, credentials are fetched from the environment variables. This is also the case when creds is not passed for cloud datasets\n\n|  `None`  \n`token` |  `str` |  Activeloop token, used for fetching credentials to the dataset at path if it is a Deep Lake dataset. This is optional, tokens are normally autogenerated. |  `None`  \n  \nExamples:\n\n    \n    \n    # Load dataset managed by Deep Lake.\n    ds = deeplake.open(\"al://organization_id/dataset_name\")\n    \n    # Load dataset stored in your cloud using your own credentials.\n    ds = deeplake.open(\"s3://bucket/my_dataset\",\n        creds = {\"aws_access_key_id\": id, \"aws_secret_access_key\": key})\n    \n    # Load dataset stored in your cloud using Deep Lake managed credentials.\n    ds = deeplake.open(\"s3://bucket/my_dataset\",\n        creds = {\"creds_key\": \"managed_creds_key\"}, org_id = \"my_org_id\")\n    \n    ds = deeplake.open(\"s3://bucket/path/to/dataset\")\n    \n    ds = deeplake.open(\"azure://bucket/path/to/dataset\")\n    \n    ds = deeplake.open(\"gcs://bucket/path/to/dataset\")\n    \n\n###  `` deeplake.open_read_only \u00b6\n\n    \n    \n    open_read_only(\n        url: str,\n        creds: dict[str, str] | None = None,\n        token: str | None = None,\n    ) -> ReadOnlyDataset\n    \n\nOpens an existing dataset in read-only mode.",
        "node_452": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_406": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_522": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_445": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_704": "Feel like back to home.'}, score=0.005430922978171003),\n     '4022': Document(id='4022', data={'restaurant_name': 'Eureka! Mountain View', 'restaurant_review': 'Good drinks and burgers'}, score=0.0246334319067476),\n     '3518': Document(id='3518', data={'restaurant_name': 'Olympus Caffe & Bakery', 'restaurant_review': 'I like the garden to sit down with friends and have a drink.'}, score=0.08915287739833623),\n     '17502': Document(id='17502', data={'restaurant_name': \"St. Stephen's Green\", 'restaurant_review': 'Nice place for a drink'}, score=0.02653095210662131),\n     '11383': Document(id='11383', data={'restaurant_name': 'Ludwigs Biergarten Mountain View', 'restaurant_review': 'Beer is fresh tables are big feel like a proper beer garden'}, score=0.011447537567869991),\n     '10788': Document(id='10788', data={'restaurant_name': 'Casa Lupe', 'restaurant_review': 'Run by a family that makes you feel like part of the family. Awesome food. I love their wet Chili Verde burritos'}, score=0.00522136854599736)}\n    \n\nWe sort the results dictionary by each document's combined score in descending\norder, ensuring that the highest-ranking documents appear first.\n\n    \n    \n    sorted_documents = dict(sorted(results.items(), key=lambda item: item[1].score, reverse=True))\n    print(sorted_documents)\n    \n\nOutput:\n\n    \n    \n    {'3518': Document(id='3518', data={'restaurant_name': 'Olympus Caffe & Bakery', 'restaurant_review': 'I like the garden to sit down with friends and have a drink.'}, score=0.3566115095933449),\n     '17502': Document(id='17502', data={'restaurant_name': \"St.",
        "node_552": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_290": "query_async('''\n        SELECT * FROM \"mem://embeddings\"\n        ORDER BY COSINE_SIMILARITY(vector, ARRAY[0.1, 0.2, 0.3]) DESC\n    ''')\n    \n    # Do other work while query runs\n    prepare_training()\n    \n    # Get results when needed\n    results = future.result()\n    \n\nWith async/await:\n\n    \n    \n    async def search_similar():\n        results = await deeplake.query_async('''\n            SELECT * FROM \"mem://images\"\n            ORDER BY COSINE_SIMILARITY(embedding, ARRAY[0.1, 0.2, 0.3]) DESC\n            LIMIT 100\n        ''')\n        return results\n    \n    async def main():\n        similar = await search_similar()\n    \n\nNon-blocking check:\n\n    \n    \n    future = deeplake.query_async(\n        \"SELECT * FROM dataset WHERE train_split = 'train'\"\n    )\n    \n    if future.is_completed():\n        train_data = future.result()\n    else:\n        print(\"Query still running.\")\n    \n\n## Vector Search\u00b6\n\nSearch by vector similarity:\n\n    \n    \n    # Cosine similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = deeplake.query(f\"\"\"\n        SELECT *\n        FROM \"s3://bucket/embeddings\"\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Text Search\u00b6\n\nText search using BM25 or keyword matching:\n\n    \n    \n    # Semantic search using BM25\n    results = deeplake.query(\"\"\"\n        SELECT *\n        FROM \"s3://bucket/documents\"\n        ORDER BY BM25_SIMILARITY(text, 'search query') DESC\n        LIMIT 10\n    \"\"\")\n    \n    # Keyword search using CONTAINS\n    results = deeplake.query(\"\"\"\n        SELECT *\n        FROM \"s3://bucket/metadata\"\n        WHERE CONTAINS(keywords, 'specific term')\n    \"\"\")\n    \n\n## Array Operations\u00b6\n\nOperate on multidimensional arrays:\n\n    \n    \n    # Select specific array dimensions\n    results = deeplake.query(\"\"\"\n        SELECT features[:,",
        "node_267": "Parameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`index` |  `int | slice` |  Can be:\n\n  * int: Single item index\n  * slice: Range of indices (e.g., 0:10)\n\n|  _required_  \n`value` |  `Any` |  The data to store. Must match the column's data type. |  _required_  \n  \nExamples:\n\n    \n    \n    # Update single item\n    column[0] = new_image\n    \n    # Update range\n    column[0:32] = new_batch\n    \n\n####  `` get_async \u00b6\n\n    \n    \n    get_async(index: int | slice | list | tuple) -> Future\n    \n\nAsynchronously retrieve data from the column. Useful for large datasets or\nwhen loading multiple items in ML pipelines.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`index` |  `int | slice | list | tuple` |  Can be:\n\n  * int: Single item index\n  * slice: Range of indices\n  * list/tuple: Multiple specific indices\n\n|  _required_  \n  \nReturns:\n\nName | Type | Description  \n---|---|---  \n`Future` |  `Future` |  A Future object that resolves to the requested data.  \n  \nExamples:\n\n    \n    \n    # Async batch load\n    future = column.get_async(slice(0, 32))\n    batch = future.result()\n    \n    # Using with async/await\n    async def load_batch():\n        batch = await column.get_async(slice(0, 32))\n        return batch\n    \n\n####  `` set_async \u00b6\n\n    \n    \n    set_async(index: int | slice, value: Any) -> FutureVoid\n    \n\nAsynchronously set data in the column. Useful for large updates or when\nmodifying multiple items in ML pipelines.",
        "node_422": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_415": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_165": "**Text Extraction** : \n\n     * The text data from the medical dataset is extracted into a list (`medical_text`) by iterating over the dataset and pulling the `text` field for each entry.\n  2. **Batch Embedding Generation** : \n\n     * The text data is processed in batches of 1,000 entries using the ColBERT model (`ckpt.docFromText`), which generates embeddings for each batch.\n\n     * The embeddings are appended to a list (`all_vectors`) for later use.\n\n  3. **Efficient Processing** :\n\n     * Batching ensures efficient processing, especially when dealing with large datasets, as it prevents memory overload and speeds up embedding generation.\n\n    \n    \n    all_vectors = []\n    medical_text = [el[\"text\"] for el in medical_dataset]\n    \n    for i in range(0, len(medical_text), 1000):\n        chunk = medical_text[i:i+1000]\n        vectors_chunk = ckpt.docFromText(chunk)\n        all_vectors.extend(vectors_chunk)\n    \n    \n    \n    list_of_embeddings = [vector.tolist() for vector in all_vectors]\n    len(list_of_embeddings)\n    \n\nWe convert the embeddings into Python lists for compatibility with Deep Lake\nstorage and checks the total number of embeddings. Each embedding from\nall_vectors is transformed using `.tolist()`, creating list_of_embeddings, and\n`len(list_of_embeddings)` confirms the total count matches the processed text\nentries.\n\n    \n    \n    medical_dataset[\"embedding\"][0:len(list_of_embeddings)] = list_of_embeddings\n    medical_dataset.commit()\n    \n\nThis code performs a semantic search using ColBERT embeddings, leveraging the\nMaxSim operator, executed directly in the cloud (as described in the `index-\non-the-lake` section), for efficient similarity computations.\n\n  1. **Query Embedding** : The query is embedded with `ckpt.queryFromText` and converted into a format compatible with TQL queries.\n\n    \n    \n    query_vectors = ckpt.queryFromText([\"What were the key risk factors for the development of posthemorrhagic/postoperative epilepsy in the study?\"])[0]\n    query_vectors = query_vectors.tolist()\n    \n\n  1.",
        "node_247": "* If nothing is given is, credentials are fetched from the environment variables. This is also the case when creds is not passed for cloud datasets\n\n|  _required_  \n`token` |  `str` |  Activeloop token, used for fetching credentials to the dataset at path if it is a Deep Lake dataset. This is optional, tokens are normally autogenerated. |  `None`  \n  \nExamples:\n\n    \n    \n    ds = deeplake.like(src=\"az://bucket/existing/to/dataset\",\n       dest=\"s3://bucket/new/dataset\")\n    \n\n## Dataset Class\u00b6\n\nThe main class providing full read-write access.\n\n###  `` deeplake.Dataset \u00b6\n\nBases: `DatasetView`\n\nDatasets are the primary data structure used in DeepLake. They are used to\nstore and manage data for searching, training, evaluation.\n\nUnlike deeplake.ReadOnlyDataset, instances of `Dataset` can be modified.\n\n####  `` add_column \u00b6\n\n    \n    \n    add_column(\n        name: str,\n        dtype: DataType | str | Type | type | Callable,\n        default_value: Any = None,\n    ) -> None\n    \n\nAdd a new column to the dataset.\n\nAny existing rows in the dataset will have a `None` value for the new column\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`name` |  `str` |  The name of the column |  _required_  \n`dtype` |  `DataType | str | Type | type | Callable` |  The type of the column. Possible values include:\n\n  * Values from `deeplake.types` such as \"[deeplake.types.Int32][]()\"\n  * Python types: `str`, `int`, `float`\n  * Numpy types: such as `np.int32`\n  * A function reference that returns one of the above types\n\n|  _required_  \n`format` |  `DataFormat` |  The format of the column, if applicable. Only required when the dtype is [deeplake.types.DataType][].",
        "node_173": "The model is loaded using\n`ColPali.from_pretrained()`, with `torch_dtype=torch.bfloat16` for optimized\nmemory use and `\"cuda:0\"` as the device, or `\"mps\"` for Apple Silicon devices.\nAfter loading, we set the model to evaluation mode with `.eval()` to prepare\nit for inference tasks. The `ColPaliProcessor` is also initialized to handle\npreprocessing of images and texts, enabling seamless input preparation for the\nmodel. This setup readies ColPali for high-performance image and document\nprocessing.\n\nThe provided image illustrates the architecture of **ColPali** , a vision-\nlanguage model designed specifically for efficient document retrieval using\nboth visual and textual cues. Here's an overview of its workings and how it's\ndesigned to perform this task efficiently:\n\n  1. **Offline Document Encoding** : \n\n     * On the left side, we see the **offline** processing pipeline, where a document is fed into ColPali's **Vision Language Model (VLM)** .\n\n     * Each document undergoes encoding through a **vision encoder** (to handle images and visual content) and a **language model** (for textual understanding). These two modules generate multi-dimensional embeddings representing both visual and textual aspects of the document.\n\n     * The embeddings are stored in a pre-indexed format, making them ready for fast retrieval during the online phase.\n\n  2. **Online Query Processing** : \n\n     * On the right side, in the **online** section, user queries (such as \"What are ViTs?\") are processed through the **language model** to create a query embedding.\n\n     * ColPali uses a **late interaction mechanism** , where each part of the query embedding is compared with document embeddings through a **MaxSim** operation to find the most similar regions in the document's visual and textual content.\n\n  3. **Similarity Scoring** : \n\n     * ColPali calculates a **similarity score** based on the MaxSim results, which identifies the most relevant documents or document sections matching the query.",
        "node_609": "# \ud83c\udf0a Deep Lake: Multi-Modal AI Database\u00b6\n\nDeep Lake is a database specifically designed for machine learning and AI\napplications, offering efficient data management, vector search capabilities,\nand seamless integration with popular ML frameworks.\n\n## Key Features\u00b6\n\n### \ud83d\udd0d Vector Search & Semantic Operations\u00b6\n\n  * High-performance similarity search for embeddings\n  * BM25-based semantic text search\n  * Support for building RAG applications\n  * Efficient indexing strategies for large-scale search\n\n### \ud83d\ude80 Optimized for Machine Learning\u00b6\n\n  * Native integration with PyTorch and TensorFlow\n  * Efficient batch processing for training\n  * Built-in support for common ML data types (images, embeddings, tensors)\n  * Automatic data streaming with smart caching\n\n### \u2601\ufe0f Cloud-Native Architecture\u00b6\n\n  * Native support for major cloud providers:\n    * Amazon S3\n    * Google Cloud Storage\n    * Azure Blob Storage\n  * Cost-efficient data management\n  * Data versioning and lineage tracking\n\n## Quick Installation\u00b6\n\n    \n    \n    pip install deeplake\n    \n\n## Basic Usage\u00b6\n\n    \n    \n    import deeplake\n    \n    # Create a dataset\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings,",
        "node_750": "First,\ndownload and unzip the small classification dataset below called the _animals\ndataset_.\n\nIn [57]:\n\nCopied!\n\n    \n    \n    # Download dataset\n    from IPython.display import clear_output\n    # !wget https://github.com/activeloopai/examples/blob/main/colabs/starting_data/animals.tar\n    !curl -L -o animals.tar https://github.com/activeloopai/examples/blob/main/colabs/starting_data/animals.tar\n    \n\n# Download dataset from IPython.display import clear_output # !wget\nhttps://github.com/activeloopai/examples/blob/main/colabs/starting_data/animals.tar\n!curl -L -o animals.tar\nhttps://github.com/activeloopai/examples/blob/main/colabs/starting_data/animals.tar\n\n    \n    \n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100  167k    0  167k    0     0   426k      0 --:--:-- --:--:-- --:--:--  425k\n    \n\nIn [56]:\n\nCopied!\n\n    \n    \n    # Unzip to './animals' folder\n    !tar -xvf ./animals.tar\n    \n\n# Unzip to './animals' folder !tar -xvf ./animals.tar\n\n    \n    \n    tar: Error opening archive: Unrecognized archive format\n    \n\nIn [ ]:\n\nCopied!\n\n    \n    \n    animals\n    - cats\n      - image_1.jpg\n      - image_2.jpg\n    - dogs\n      - image_3.jpg\n      - image_4.jpg\n    \n\nanimals \\- cats \\- image_1.jpg \\- image_2.jpg \\- dogs \\- image_3.jpg \\-\nimage_4.jpg\n\nNow that you have the data, you can **create a Deep Lake`Dataset`** and\ninitialize its tensors. Running the following code will create a Deep Lake\ndataset inside of the `./animals_dl` folder.\n\nIn [58]:\n\nCopied!",
        "node_128": "Review: Great burritos\n    Restaurant name: Los Amigos\n    Review: Nice place real good burritos.\n    Restaurant name: La Coste\u00f1a\n    Review: Awesome burritos\n    \n\nIf we want to filter for a specific owner answer, such as **Thank you** , we\nset `word = \"Thank you\"` to define the desired owner answer. Here, we're using\nan **inverted index** on the `owner_answer` column to efficiently filter\nresults based on this owner answer.\n\n    \n    \n    word = \"Thank you\"\n    query_vs = f\"\"\"\n        SELECT *, cosine_similarity(embedding, ARRAY[{str_query}]) as score\n        FROM (\n            SELECT *, ROW_NUMBER() AS row_id\n        )\n    \n        WHERE CONTAINS(owner_answer, '{word}') \n        ORDER BY cosine_similarity(embedding, ARRAY[{str_query}]) DESC \n    \n        LIMIT 3\n    \"\"\"\n    view_vs = vector_search.query(query_vs)\n    for row in view_vs:\n        print(f\"Restaurant name: {row['restaurant_name']} \\nReview: {row['restaurant_review']} \\nOwner Answer: {row['owner_answer']}\")\n    \n\nOutput:\n\n    \n    \n    Restaurant name: Taqueria La Espuela\n    Review: My favorite place for super burrito and horchata\n    Owner Answer: Thank you for your continued support!\n    Restaurant name: Chaat Bhavan Mountain View\n    Review: Great place with good food\n    Owner Answer: Thank you for your positive feedback! We're thrilled to hear that you had a great experience at our restaurant and enjoyed our delicious food. Your satisfaction is our priority, and we can't wait to welcome you back for another wonderful dining experience.\n    \n    Thanks,\n    Team Chaat Bhavan\n    Restaurant name: Chaat Bhavan Mountain View\n    Review: Good food.\n    Owner Answer: Thank you for your 4-star rating! We're glad to hear that you had a positive experience at our restaurant. Your feedback is valuable to us, and we appreciate your support.",
        "node_514": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_21": "Chat in on our website: to claim the access!\n\n## \ud83d\udc69\u200d\ud83d\udcbb Comparisons to Familiar Tools\n\n**Deep Lake vs Chroma**\n\nBoth Deep Lake & ChromaDB enable users to store and search vectors\n(embeddings) and offer integrations with LangChain and LlamaIndex. However,\nthey are architecturally very different. ChromaDB is a Vector Database that\ncan be deployed locally or on a server using Docker and will offer a hosted\nsolution shortly. Deep Lake is a serverless Vector Store deployed on the\nuser\u2019s own cloud, locally, or in-memory. All computations run client-side,\nwhich enables users to support lightweight production apps in seconds. Unlike\nChromaDB, Deep Lake\u2019s data format can store raw data such as images, videos,\nand text, in addition to embeddings. ChromaDB is limited to light metadata on\ntop of the embeddings and has no visualization. Deep Lake datasets can be\nvisualized and version controlled. Deep Lake also has a performant dataloader\nfor fine-tuning your Large Language Models.\n\n**Deep Lake vs Pinecone**\n\nBoth Deep Lake and Pinecone enable users to store and search vectors\n(embeddings) and offer integrations with LangChain and LlamaIndex. However,\nthey are architecturally very different. Pinecone is a fully-managed Vector\nDatabase that is optimized for highly demanding applications requiring a\nsearch for billions of vectors. Deep Lake is serverless. All computations run\nclient-side, which enables users to get started in seconds.",
        "node_450": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_343": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_724": "It defines a grid with 3 columns and\ncalculates the required number of rows based on the number of images. A figure\nwith subplots is created, where each image is displayed in a cell with its\nrestaurant name and similarity score shown as the title, and axes turned off\nfor a cleaner look. Any extra cells, if present, are hidden to avoid empty\nspaces. Finally, `plt.tight_layout()` arranges the grid, and `plt.show()`\ndisplays the images in a well-organized layout, highlighting the most similar\nimages along with their metadata.\n\n    \n    \n    import matplotlib.pyplot as plt\n    from PIL import Image\n    import numpy as np\n    \n    def show_images(similar_images: list[dict]):\n        # Define the number of rows and columns for the grid\n        num_columns = 3\n        num_rows = (len(similar_images) + num_columns - 1) // num_columns  # Calculate the required number of rows\n    \n        # Create the grid\n        fig, axes = plt.subplots(num_rows, num_columns, figsize=(15, 5 * num_rows))\n        axes = axes.flatten()  # Flatten for easier access to cells\n    \n        for idx, el in enumerate(similar_images):\n            img = Image.fromarray(el[\"image\"])\n            axes[idx].imshow(img)\n            axes[idx].set_title(f\"Restaurant: {el['restaurant_name']}, Similarity: {el['score']:.4f}\")\n            axes[idx].axis('off')  # Turn off axes for a cleaner look\n    \n        # Remove empty axes if the number of images doesn't fill the grid\n        for ax in axes[len(similar_images):]:\n            ax.axis('off')\n    \n        plt.tight_layout()\n        plt.show()\n    \n    show_images(similar_images_result)\n    \n\nOutput:\n\n## 6) ColBERT: Efficient and Effective Passage Search via Contextualized Late\nInteraction over BERT\u00b6\n\nColBERT is a model designed to efficiently retrieve and rank passages by\nleveraging the power of deep language models like BERT, but with a unique\napproach called **late interaction**.",
        "node_599": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_233": "However it will be supported in the future.\nIn the meantime, you can keep the annotations in a separate dataset. There are\nonly 2 requirements:\n\n  * The dataset should have the same length as the dataset that you have created from Labelbox.\n  * The dataset should have the same `labelbox_meta` metadata as the dataset that you have created from Labelbox.\n\nThen you can UNION the two datasets.\n\nBack to top",
        "node_14": "213 Commits  \n.github| .github|  |   \npython/deeplake| python/deeplake|  |   \n.gitignore| .gitignore|  |   \n.pre-commit-config.yaml| .pre-commit-config.yaml|  |   \nCONTRIBUTING.md| CONTRIBUTING.md|  |   \nLICENSE| LICENSE|  |   \nREADME.md| README.md|  |   \nSECURITY.md| SECURITY.md|  |   \nView all files  \n  \n## Repository files navigation\n\n  * README\n  * Apache-2.0 license\n  * Security\n\n  \n\n# Deep Lake: Database for AI\n\n###  **Docs** \u2022 **Get Started** \u2022 **API Reference** \u2022 **LangChain & VectorDBs\nCourse** \u2022 **Blog** \u2022 **Whitepaper** \u2022 **Slack** \u2022 **Twitter**\n\n## What is Deep Lake?\n\nDeep Lake is a Database for AI powered by a storage format optimized for deep-\nlearning applications. Deep Lake can be used for:\n\n  1. Storing and searching data plus vectors while building LLM applications\n  2. Managing datasets while training deep learning models\n\nDeep Lake simplifies the deployment of enterprise-grade LLM-based products by\noffering storage for all data types (embeddings, audio, text, videos, images,\ndicom, pdfs, annotations, and more), querying and vector search, data\nstreaming while training models at scale, data versioning and lineage, and\nintegrations with popular tools such as LangChain, LlamaIndex, Weights &\nBiases, and many more.",
        "node_714": "Awesome food. I love their wet Chili Verde burritos\n    \n\n### Comparison of Sync vs Async Query Performance\u00b6\n\nThis code performs an asynchronous query on a Deep Lake dataset. It begins by\nopening the dataset asynchronously using `await deeplake.open_async()`,\nspecifying `org_id` and `dataset_name_vs`.\n\n    \n    \n    ds_async = await deeplake.open_async(f\"al://{org_id}/{dataset_name_vs}\")\n    ds_async_results = ds_async.query_async(tql_vs).result()\n    \n\nThis following code compares the execution times of synchronous and\nasynchronous queries on a Deep Lake dataset:\n\n  * First, it records the start time `start_sync` for the synchronous query, executes the query with `vector_search.query(tql_vs)`, and then records the end time `end_sync`. It calculates and prints the total time taken for the synchronous query by subtracting `start_sync` from `end_sync`.\n  * Next, it measures the asynchronous query execution by recording `start_async`, running `vector_search.query_async(tql_vs).result()` to execute and retrieve the query result asynchronously, and then recording `end_async`. The asynchronous query time is calculated as the difference between `end_async` and `start_async`, and is printed.\n\nThe code executes two queries both synchronously and asynchronously, measuring\nthe execution time for each method. In the synchronous part, the queries are\nexecuted one after the other, and the execution time is recorded. In the\nasynchronous part, the queries are run concurrently using `asyncio.gather()`\nto parallelize the asynchronous calls, and the execution time is also\nmeasured. The \"speed factor\" is then calculated by comparing the execution\ntimes, showing how much faster the asynchronous execution is compared to the\nsynchronous one. Using `asyncio.gather()` allows the asynchronous queries to\nrun in parallel, reducing the overall execution time.",
        "node_628": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_658": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_346": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_556": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_163": "* `ColBERTConfig` provides configuration options for the ColBERT model, such as directory paths and other settings.\n\n  2. **Initializing the Checkpoint** : \n\n     * `\"colbert-ir/colbertv2.0\"` specifies the name of the pretrained checkpoint to load. This could be a path to a local model file or a remote model identifier, depending on your setup.\n\n     * `ColBERTConfig(root=\"experiments\")` sets the root directory where model-related experiments will be saved or accessed. This is useful for organizing logs, results, and intermediate files.\n\n  3. **Purpose** : \n\n     * The `ckpt` object now contains the pretrained ColBERT model and its configuration, ready to be used for tasks like ranking or embedding documents in information retrieval pipelines.\n\nThis step sets up the foundation for using ColBERT's capabilities in semantic\nsearch and ranking tasks efficiently.\n\n    \n    \n    from colbert.modeling.checkpoint import Checkpoint\n    from colbert.infra import ColBERTConfig\n    \n    ckpt = Checkpoint(\n        \"colbert-ir/colbertv2.0\", colbert_config=ColBERTConfig(root=\"experiments\")\n    )\n    \n\nIn this example, we copy, structure, and process a **medical dataset** to\ngenerate embeddings for text documents using a pretrained ColBERT model.\n\n  1. **Dataset Copy and Setup** : \n\n     * The `deeplake.copy()` function duplicates the `medical_dataset` from the Activeloop repository into your organization's workspace.\n\n     * `deeplake.open()` then opens the dataset for modifications, allowing us to add or manipulate columns.\n\n  2. **Adding an Embedding Column** : \n\n     * A new column named `embedding` is added to the dataset with the data type `types.Array(types.Float32(), dimensions=2)`, preparing it to store 2D embeddings generated from the medical text.",
        "node_234": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nDataset Classes\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset  Dataset  Table of contents \n      * Creation Methods \n        * create \n        * open \n        * open_read_only \n        * like \n      * Dataset Class \n        * Dataset",
        "node_531": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_598": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_220": "02 / 8)\n    lr_config = dict(warmup=None)\n    log_config = dict(interval=50)\n    checkpoint_config = dict(interval=5000)\n    \n    runner = dict(type=\"IterBasedRunner\", max_iters=100000, max_epochs=None)\n    device = \"cuda\"\n    \n\n## Training\u00b6\n\nNow we can start the training:\n\n    \n    \n    if __name__ == \"__main__\":\n        current_loc = os.getcwd()\n        cfg_file = f\"{current_loc}/seg_mask_config.py\"\n    \n        # Read the config file\n        cfg = Config.fromfile(cfg_file)\n        cfg.model.decode_head.num_classes = 81\n        cfg.model.auxiliary_head.num_classes = 81\n    \n        # build segmentor\n        model = mmseg_deeplake.build_segmentor(\n            cfg.model\n        )\n    \n        # Create work directory\n        mmcv.mkdir_or_exist(os.path.abspath(cfg.work_dir))\n    \n        # train_segmentor\n        mmseg_deeplake.train_segmentor(\n            model,\n            cfg,\n            distributed=True,  # Set to True for multi-GPU training\n            validate=True, # Set to True if you have validation data\n        )\n    \n\n## Deep Lake Integration Benefits\u00b6\n\n  1. **Efficient Mask Handling** : Deep Lake efficiently stores and loads segmentation masks, which can be large and memory-intensive.\n\n  2. **Automatic Format Conversion** : Deep Lake handles conversion between different mask formats (binary, RLE, polygon) automatically.\n\n  3. **Smart Batching** : Deep Lake's dataloader handles variable-sized images and masks efficiently.\n\n  4. **Memory Management** : Data is loaded on-demand, preventing out-of-memory issues with large datasets.\n\n  5. **Distributed Training Support** : Seamless integration with MMSegmentation's distributed training.",
        "node_29": "A full comparison\narticle can be found here.\n\n**Deep Lake vs HuggingFace** Deep Lake and HuggingFace offer access to popular\ndatasets, but Deep Lake primarily focuses on computer vision, whereas\nHuggingFace focuses on natural language processing. HuggingFace Transforms and\nother computational tools for NLP are not analogous to features offered by\nDeep Lake.  **Deep Lake vs WebDatasets** Deep Lake and WebDatasets both offer\nrapid data streaming across networks. They have nearly identical steaming\nspeeds because the underlying network requests and data structures are very\nsimilar. However, Deep Lake offers superior random access and shuffling, its\nsimple API is in python instead of command-line, and Deep Lake enables simple\nindexing and modification of the dataset without having to recreate it.\n**Deep Lake vs Zarr** Deep Lake and Zarr both offer storage of data as chunked\narrays. However, Deep Lake is primarily designed for returning data as arrays\nusing a simple API, rather than actually storing raw arrays (even though\nthat's also possible). Deep Lake stores data in use-case-optimized formats,\nsuch as jpeg or png for images, or mp4 for video, and the user treats the data\nas if it's an array, because Deep Lake handles all the data processing in\nbetween. Deep Lake offers more flexibility for storing arrays with dynamic\nshape (ragged tensors), and it provides several features that are not naively\navailable in Zarr such as version control, data streaming, and connecting data\nto ML Frameworks.",
        "node_285": "A sequence maintains the order of its values, making it suitable for time-\nseries data like videos (sequences of images).\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`nested_type` |  `DataType | str | Type` |  DataType | str | Type The data type of the values in the sequence. Can be any data type, not just primitive types. |  _required_  \n  \nReturns:\n\nName | Type | Description  \n---|---|---  \n`Type` |  `Type` |  A new sequence data type.  \n  \nExamples:\n\nCreate a sequence of images:\n\n    \n    \n    ds.add_column(\"col1\", types.Sequence(types.Image(sample_compression=\"jpg\")))\n    \n    \n    \n    # Sequence of images (e.g., video frames)\n    ds.add_column(\"frames\", deeplake.types.Sequence(\n        deeplake.types.Image(sample_compression=\"jpeg\")\n    ))\n    \n    # Sequence of embeddings\n    ds.add_column(\"token_embeddings\", deeplake.types.Sequence(\n        deeplake.types.Embedding(768)\n    ))\n    \n    # Add data\n    ds.append([{\n        \"frames\": [frame1, frame2, frame3],  # List of images\n        \"token_embeddings\": [emb1, emb2, emb3]  # List of embeddings\n    }])\n    \n\n##  `` deeplake.types.Link \u00b6\n\n    \n    \n    Link(type: Type) -> Type\n    \n\nA link to an external resource. The value returned will be a reference to the\nexternal resource rather than the raw data.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`type` |  `Type` |  The type of the linked data |  _required_  \n  \nExamples:\n\n    \n    \n    ds.add_column(\"col1\", types.Link(types.Image()))\n    \n\nBack to top",
        "node_470": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_558": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_680": "The portions are monstrous. The wet burritos are as big as a football.\n    \n\nAI data retrieval systems today face 3 challenges: `limited modalities`, `lack\nof accuracy`, and `high costs at scale`. Deep Lake 4.0 fixes this by enabling\ntrue multi-modality, enhancing accuracy, and reducing query costs by 2x with\nindex-on-the-lake technology.\n\nConsider a scenario where we store all our data locally on a computer.\nInitially, this may be adequate, but as the volume of data grows, managing it\nbecomes increasingly challenging. The computer's storage becomes limited, data\naccess slows, and sharing information with others is less efficient.\n\nTo address these challenges, we can transition our data storage to the cloud\nusing Deep Lake. Designed specifically for handling large-scale datasets and\nAI workloads, Deep Lake enables up to 10 times faster data access. With cloud\nstorage, hardware limitations are no longer a concern: Deep Lake offers ample\nstorage capacity, secure access from any location, and streamlined data\nsharing.\n\nThis approach provides a robust and scalable infrastructure that can grow\nalongside our projects, minimizing the need for frequent hardware upgrades and\nensuring efficient data management.\n\n## 2) Create the Dataset and use BM25 to Retrieve the Data\u00b6\n\nOur advanced `\"Index-On-The-Lake\"` technology enables sub-second query\nperformance directly from object storage, such as `S3`, using minimal compute\npower and memory resources. Achieve up to `10x greater cost efficiency`\ncompared to in-memory databases and `2x faster performance` than other object\nstorage solutions, all without requiring additional disk-based caching.\n\nWith Deep Lake, you benefit from rapid streaming columnar access to train deep\nlearning models directly, while also executing sub-second indexed queries for\nretrieval-augmented generation.\n\nIn this stage, the system uses BM25 for a straightforward lexical search. This\napproach is efficient for retrieving documents based on exact or partial\nkeyword matches.",
        "node_458": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_97": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\nAzure Workload Identities\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities  Azure Workload Identities  Table of contents \n            * Authenticating Using Workload Identities Instead of User Credentials \n            * Step 1: Define the workload identity in Azure \n            * Step 2: Attached the Azure Managed Identity to your workload \n            * Step 3: Create a Deep Lake Workload Identity using the Azure Managed Identity \n            * Step 4: Run the workload \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader",
        "node_647": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_284": "Parameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`compression` |  `str` |  How to compress each row's value. Possible values: dcm, nii, nii.gz |  _required_  \n  \nExamples:\n\n    \n    \n    ds.add_column(\"col1\", types.Medical(compression=\"dcm\"))\n    \n    with open(\"path/to/dicom/file.dcm\", \"rb\") as f:\n        bytes_data = f.read()\n        ds.append([{\"col1\": bytes_data}])\n    \n\n##  `` deeplake.types.Struct \u00b6\n\n    \n    \n    Struct(fields: dict[str, DataType | str]) -> DataType\n    \n\nDefines a custom datatype with specified keys.\n\nSee deeplake.types.Dict for a type that supports different key/value pairs per\nvalue.\n\nParameters:\n\nName | Type | Description | Default  \n---|---|---|---  \n`fields` |  `dict[str, DataType | str]` |  A dict where the key is the name of the field, and the value is the datatype definition for it |  _required_  \n  \nExamples:\n\n    \n    \n    ds.add_column(\"col1\", types.Struct({\n       \"field1\": types.Int16(),\n       \"field2\": \"text\",\n    }))\n    \n    ds.append([{\"col1\": {\"field1\": 3, \"field2\": \"a\"}}])\n    print(ds[0][\"col1\"][\"field1\"]) # Output: 3\n    \n    \n    \n    # Define fixed structure with specific types\n    ds.add_column(\"info\", deeplake.types.Struct({\n        \"id\": deeplake.types.Int64(),\n        \"name\": \"text\",\n        \"score\": deeplake.types.Float32()\n    }))\n    \n    # Add data\n    ds.append([{\n        \"info\": {\n            \"id\": 1,\n            \"name\": \"sample\",\n            \"score\": 0.95\n        }\n    }])\n    \n\n##  `` deeplake.types.Sequence \u00b6\n\n    \n    \n    Sequence(nested_type: DataType | str | Type) -> Type\n    \n\nCreates a sequence type that represents an ordered list of other data types.\n\nA sequence maintains the order of its values, making it suitable for time-\nseries data like videos (sequences of images).",
        "node_446": "create(\"s3://my-bucket/dataset\")  # or local path\n    \n    # Add data columns\n    ds.add_column(\"images\", deeplake.types.Image())\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(768))\n    ds.add_column(\"labels\", deeplake.types.Text())\n    \n    # Add data\n    ds.append([{\n        \"images\": image_array,\n        \"embeddings\": embedding_vector,\n        \"labels\": \"cat\"\n    }])\n    \n    # Vector similarity search\n    text_vector = ','.join(str(x) for x in search_vector)\n    results = ds.query(f\"\"\"\n        SELECT *\n        ORDER BY COSINE_SIMILARITY(embeddings, ARRAY[{text_vector}]) DESC\n        LIMIT 100\n    \"\"\")\n    \n\n## Common Use Cases\u00b6\n\n### Deep Learning Training\u00b6\n\n    \n    \n    # PyTorch integration\n    from torch.utils.data import DataLoader\n    \n    loader = DataLoader(ds.pytorch(), batch_size=32, shuffle=True)\n    for batch in loader:\n        images = batch[\"images\"]\n        labels = batch[\"labels\"]\n        # training code.\n    \n\n### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.",
        "node_746": "question = queries[0]\n    output_image = \"image.jpg\"\n    img = Image.fromarray(colpali_results[0][\"image\"][0])\n    img.save(output_image)\n    \n\nThe following code opens `\"image.jpg\"` in binary mode, encodes it to a base64\nstring, and passes it with `question` to the `generate_VQA` function, which\nreturns an answer based on the image.\n\n    \n    \n    import base64\n    \n    with open(output_image, \"rb\") as image_file:\n        base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n    \n    answer = generate_VQA(base64_image, question)\n    print(answer)\n    \n\nOutput:\n\n    \n    \n    'As time approaches infinity, the voltage modeled by n^6 will eventually stabilize at the equilibrium potential for potassium (EK), which is represented at approximately -90 mV on the graph.'\n    \n\nWe've now gained a solid understanding of multi-modal data processing,\nadvanced retrieval techniques, and hybrid search methods using state-of-the-\nart models like ColPali. With these skills, you're equipped to tackle complex,\nreal-world applications that require deep insights from both text and image\ndata.\n\nKeep experimenting, stay curious, and continue building innovative\nsolutions\u2014this is just the beginning of what's possible in the field of AI-\ndriven search and information retrieval.\n\n**To learn more about Deep Lake v4, visit theofficial blog post and\ndocumentation.**\n\nBack to top",
        "node_484": "Skip to content\n\nYou're not viewing the latest version.  **Click here to go to latest.**\n\n\ud83c\udf0a Deep Lake: Multi-Modal AI Database\n\nInitializing search\n\n  * Getting Started  Getting Started \n    * Quickstart \n    * Authentication \n    * Storage and Credentials  Storage and Credentials \n      * Storage and Credentials \n      * Storage Options \n      * Managed credentials  Managed credentials \n        * Overview \n        * AWS  AWS \n          * Enabling CORS in S3 \n          * Provisioning Role-Based Access \n        * Azure  Azure \n          * Enabling CORS in Azure \n          * Provisioning Federated Credentials \n          * Azure Workload Identities \n        * GCP  GCP \n          * Enabling CORS in GCP \n          * Provisioning Federated Credentials \n  * User Guides  User Guides \n    * RAG \n    * VectorStore \n    * Deep Learning  Deep Learning \n      * Quickstart \n      * DataLoader \n      * Object Detection with MM \n      * Segmentation with MM \n    * Migrating to Deep Lake v4 \n    * Annotations  Annotations \n      * Labelbox \n  * API Reference  API Reference \n    * Dataset \n    * Column \n    * Types \n    * Query \n    * Version Control \n    * Schemas \n    * Metadata \n  * Advanced  Advanced \n    * Best Practices \n    * TQL Syntax \n    * Visualize Datasets \n    * Synchronize Datasets \n\nTable of contents\n\n  * Key Features \n    * \ud83d\udd0d Vector Search & Semantic Operations \n    * \ud83d\ude80 Optimized for Machine Learning \n    * \u2601\ufe0f Cloud-Native Architecture \n  * Quick Installation \n  * Basic Usage \n  * Common Use Cases \n    * Deep Learning Training \n    * RAG Applications \n    * Computer Vision \n  * Next Steps \n  * Resources \n  * Why Deep Lake?",
        "node_491": "### RAG Applications\u00b6\n\n    \n    \n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    # Store text and embeddings\n    ds.add_column(\"text\", deeplake.types.Text(index_type=deeplake.types.BM25))\n    ds.add_column(\"embeddings\", deeplake.types.Embedding(1536))\n    \n    # Semantic search\n    results = ds.query(\"\"\"\n        SELECT text\n        ORDER BY BM25_SIMILARITY(text, 'machine learning') DESC\n        LIMIT 10\n    \"\"\")\n    \n\n### Computer Vision\u00b6\n\n    \n    \n    # Store images and annotations\n    ds = deeplake.create(\"s3://my-bucket/dataset\")  # or local path\n    ds.add_column(\"images\", deeplake.types.Image(sample_compression=\"jpeg\"))\n    ds.add_column(\"boxes\", deeplake.types.BoundingBox())\n    ds.add_column(\"masks\", deeplake.types.SegmentMask(sample_compression='lz4'))\n    \n    # Add data\n    ds.append({\n        \"images\": imgs,\n        \"boxes\": bboxes,\n        \"masks\": smasks\n    })\n    \n\n## Next Steps\u00b6\n\n  * Check out our Quickstart Guide for detailed setup\n  * Explore RAG Applications\n  * See Deep Learning Integration\n\n## Resources\u00b6\n\n  * GitHub Repository\n  * API Reference\n  * Community Support\n\n## Why Deep Lake?\u00b6\n\n  * **Performance** : Optimized for ML workloads with efficient data streaming\n  * **Scalability** : Handle billions of samples directly from the cloud\n  * **Flexibility** : Support for all major ML frameworks and cloud providers\n  * **Cost-Efficiency** : Smart storage management and compression\n  * **Developer Experience** : Simple, intuitive API with comprehensive features\n\nBack to top",
        "node_43": "# Search code, repositories, users, issues, pull requests.\n\nSearch\n\nClear\n\nSearch syntax tips\n\n#  Provide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\nInclude my email address so I can be contacted\n\nCancel  Submit feedback\n\n#  Saved searches\n\n## Use saved searches to filter your results more quickly\n\nName\n\nQuery\n\nTo see all available qualifiers, see our documentation.\n\nCancel  Create saved search\n\nSign in\n\nSign up  Reseting focus\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\n{{ message }}\n\nactiveloopai  / **deeplake ** Public\n\n  * Notifications  You must be signed in to change notification settings\n  * Fork 656\n  * Star  8.5k\n\nDatabase for AI. Store Vectors, Images, Texts, Videos, etc. Use with\nLLMs/LangChain. Store, query, version, & visualize any AI data. Stream data in\nreal-time to PyTorch/TensorFlow. https://activeloop.ai\n\nactiveloop.ai\n\n### License\n\nApache-2.0 license\n\n8."
    },
    "relevant_docs": {
        "7b8a3efc-586a-4ae4-b8f5-37d32ef0e9a1": [
            "node_366"
        ],
        "d3329133-d58f-4fd6-8428-4433957d2123": [
            "node_592"
        ],
        "9a660b28-5dbc-4743-b08e-7d9bad0204d8": [
            "node_412"
        ],
        "328ce107-2ae6-4150-8251-cd430136c44e": [
            "node_632"
        ],
        "5ebb5419-5c6e-49e7-a040-6148a32d8585": [
            "node_154"
        ],
        "915ab960-5eec-48bf-8f09-6909d6ca83d7": [
            "node_214"
        ],
        "64a72ae7-1168-4dbf-a073-7ad8655665a7": [
            "node_362"
        ],
        "df2cb9b3-216f-4778-b0fb-6b4925cb7fd4": [
            "node_110"
        ],
        "34339025-c9e2-471a-b299-ebd139130473": [
            "node_291"
        ],
        "6fbe6469-1eb8-4636-b7f1-c058f8def23d": [
            "node_64"
        ],
        "d54d7704-ed33-41ab-9936-a1d80a26ba0f": [
            "node_500"
        ],
        "49b457df-0824-4e29-9609-9acebb0aa147": [
            "node_184"
        ],
        "f661e35d-33f6-4c18-b47d-347fd4a335e5": [
            "node_371"
        ],
        "5bc7184f-c221-4301-a4ae-bbe0d6a2f377": [
            "node_350"
        ],
        "c6e5e12d-55b2-43d5-b1e7-cb49a7f31307": [
            "node_119"
        ],
        "2f424ee8-bb94-423d-b976-9f08e770a233": [
            "node_428"
        ],
        "e8ffaf99-9b97-4d22-87e3-e5d321731254": [
            "node_300"
        ],
        "2dbe503c-e399-4ec9-be7a-965ba6bd95bf": [
            "node_550"
        ],
        "c16a5d09-960a-47d2-ac06-fc12f144a5af": [
            "node_427"
        ],
        "aebd4d1b-021a-47c0-8b2b-971ea30c85ae": [
            "node_42"
        ],
        "98991fcc-bb2e-4ad3-93ab-1aabbf066140": [
            "node_75"
        ],
        "0e6a6ae6-5499-444e-add0-88a8504e326d": [
            "node_246"
        ],
        "42db95a1-b46d-4767-a0c6-f9938df0628d": [
            "node_271"
        ],
        "15022be8-becd-4afb-b3c4-7d22c38a4139": [
            "node_789"
        ],
        "ba5fe3dd-a1fb-4575-ad64-beff7a89f8ec": [
            "node_549"
        ],
        "a602a0c3-ae5a-47a7-8a9f-40baa75378e0": [
            "node_223"
        ],
        "60f7d60d-370b-4bba-8e99-06af50c9ef12": [
            "node_344"
        ],
        "8000b42c-a065-4b9a-a2f1-f410d949c21f": [
            "node_258"
        ],
        "d9a9a78b-ed89-4532-b66c-44c97bc6da52": [
            "node_118"
        ],
        "6e73435e-dbba-450b-a89b-7acb023df89e": [
            "node_192"
        ],
        "eb4839b8-220b-4ea9-b564-f80d5c3f9e26": [
            "node_629"
        ],
        "db697bcd-987c-476b-8274-022a620a81bb": [
            "node_787"
        ],
        "1473bf81-c12f-4413-8508-4c4e2ab9a6cf": [
            "node_401"
        ],
        "1015b99e-6673-4c5d-b203-aa590c90936c": [
            "node_496"
        ],
        "3b5876d7-a8b8-479a-88f0-b5006d000b8b": [
            "node_358"
        ],
        "9034725b-73e1-4704-9e3b-5267970bec68": [
            "node_561"
        ],
        "dc7eeac7-5176-46b0-98e3-dcccda01757b": [
            "node_46"
        ],
        "7505e933-65c3-477c-ba92-f241b529a03b": [
            "node_527"
        ],
        "9284b072-f884-4311-b693-3e3cd276b3a3": [
            "node_602"
        ],
        "a61e8222-1b12-4158-923c-993c56295e40": [
            "node_567"
        ],
        "a8ddab24-0a6e-4cfa-8225-4f6f6375da6d": [
            "node_105"
        ],
        "0ca15ac0-cc91-4643-aac0-d38ff306350f": [
            "node_555"
        ],
        "07e65184-1fc6-49e2-80b2-83548e81f934": [
            "node_345"
        ],
        "008b4397-063e-40a6-977f-10ebf7a7e8f3": [
            "node_339"
        ],
        "1529f05c-8f00-4e3a-bbbd-fda29c9b1a4c": [
            "node_715"
        ],
        "2d9cdc22-517d-46d4-a6b3-2871bacbc1ad": [
            "node_129"
        ],
        "20fbc071-69eb-4bfc-a1d1-40d866137c53": [
            "node_573"
        ],
        "b904f6de-0bbc-464d-8f50-2bae1cef09b6": [
            "node_778"
        ],
        "90605e6b-91b2-4e4b-b46e-6e55be2baabc": [
            "node_641"
        ],
        "46f17f13-ad87-4c1b-9791-1d08d03399b8": [
            "node_106"
        ],
        "c5d6a0a1-604b-46aa-a894-9abc9b40af9a": [
            "node_451"
        ],
        "203b7343-6ceb-4ef3-89f7-74872c975951": [
            "node_116"
        ],
        "785c58a2-34d5-4c4e-9819-ecea5c87ad95": [
            "node_149"
        ],
        "9e8ee018-115e-45a0-8631-e7a51d609640": [
            "node_283"
        ],
        "8158a3e7-5418-4123-9a91-409316e982bc": [
            "node_538"
        ],
        "1830e125-9862-415b-9a91-d86f7c1acc12": [
            "node_144"
        ],
        "96cb3f78-6dc7-4294-95c2-799baa55dc24": [
            "node_637"
        ],
        "411e1e2b-e659-49b2-841a-10b63dbb9cd5": [
            "node_295"
        ],
        "8640fb7f-040e-4c59-9489-8e1acff61eae": [
            "node_711"
        ],
        "5489ef74-f3ad-4f27-ab1d-548f2d442140": [
            "node_749"
        ],
        "48365491-db28-44eb-bdeb-ca6f30536d6e": [
            "node_569"
        ],
        "cd95ceb2-4b66-4dc8-907e-a523402c8c1d": [
            "node_439"
        ],
        "0c82fd92-4ec1-4bbc-a2c5-d8acd757db10": [
            "node_513"
        ],
        "4c6093e2-67c5-4e2a-88b9-90afbf1574d5": [
            "node_380"
        ],
        "6ce50ec8-fd00-4877-a9a7-159eba0bf446": [
            "node_757"
        ],
        "0994b35c-324d-451e-9c1d-a50408599536": [
            "node_640"
        ],
        "61b7ed59-96e0-4dfc-8bd3-fe22c6294251": [
            "node_498"
        ],
        "d4b6b2f8-70cc-4d7a-bafd-35ee239a8c56": [
            "node_731"
        ],
        "a5d51fde-59be-47bc-b852-c0e682936052": [
            "node_536"
        ],
        "42b94693-3b62-4322-a4b8-bdf757198bf5": [
            "node_730"
        ],
        "6fd71488-3765-430d-8750-e51d1d1d925b": [
            "node_342"
        ],
        "f898fda8-c9eb-4f8a-a13d-2e624b8d0dc7": [
            "node_697"
        ],
        "554d4ba2-7fb4-49df-99af-46323291079f": [
            "node_780"
        ],
        "0ccb54d9-af81-4161-871b-a703ac23b195": [
            "node_403"
        ],
        "2771be83-7f77-468d-8c5d-107f3bd37615": [
            "node_654"
        ],
        "f63b7593-f91b-446d-a052-a02cc2391ab7": [
            "node_657"
        ],
        "50b0cd84-d6cd-4d15-b585-93208f637c3c": [
            "node_0"
        ],
        "af93fc28-e866-4ad0-bce4-7a036b3a34ae": [
            "node_699"
        ],
        "22ebd979-05a2-403e-8fa1-c23225fa1b1e": [
            "node_205"
        ],
        "6e849a0e-b6e1-4f3f-8f94-21f3b8c92efc": [
            "node_294"
        ],
        "807b1ca0-b144-45d2-8ec1-ae54181eba52": [
            "node_171"
        ],
        "aeecc384-d0db-4adc-9111-4ce0f094cd95": [
            "node_254"
        ],
        "6940a10c-441d-4b3a-a447-fe4d2a94739a": [
            "node_572"
        ],
        "5cc6f171-c63b-41f0-9dd6-1305267f2857": [
            "node_502"
        ],
        "e389d19f-d3bf-4fe5-8d64-54ce06290d05": [
            "node_135"
        ],
        "6f201a9d-f203-4317-860d-57e8adb58af2": [
            "node_319"
        ],
        "711a9052-8063-43a4-8390-73f9b64c8eb6": [
            "node_425"
        ],
        "b07e9ca9-9639-403f-bfbc-889627d758f5": [
            "node_347"
        ],
        "60f25c3b-865c-4a97-a597-5292033aaf89": [
            "node_74"
        ],
        "2fef89cd-fc94-489c-99e1-86715965baf6": [
            "node_553"
        ],
        "1ba9ea02-5956-4df5-850f-eb3076d641a5": [
            "node_677"
        ],
        "fd7afb55-15b2-4b73-9ea4-74e199667994": [
            "node_174"
        ],
        "e8d8f54a-c3b7-47a0-8dc7-e7999ad0d912": [
            "node_407"
        ],
        "1089b003-926c-4a3c-954d-c8517fce1c0d": [
            "node_615"
        ],
        "4f564b38-0fe7-4342-868d-8e85b1bc04cd": [
            "node_302"
        ],
        "fe60a9ad-d6ef-4aba-bc24-10da3d5e7864": [
            "node_50"
        ],
        "67e4a5dd-d8b7-4298-9821-34bc64c4e097": [
            "node_433"
        ],
        "b0091aa8-39f3-4ce5-9c7c-3a328125e7c5": [
            "node_528"
        ],
        "49a7cec3-263e-4ea0-9b9b-7ee9365cd5cc": [
            "node_753"
        ],
        "c7c8cd5e-bc01-421c-a540-ab4008045f80": [
            "node_393"
        ],
        "096ef874-6f31-4cba-8248-255ddd0a088f": [
            "node_185"
        ],
        "3bd14a0d-2295-4f66-a6fe-e9ceecbb58ec": [
            "node_564"
        ],
        "608cefa6-9203-45c1-b755-517b421e4dd1": [
            "node_47"
        ],
        "05874434-34f6-4d99-8fa8-65eb85fc24be": [
            "node_386"
        ],
        "38436ba7-c7c3-4144-9cd3-e78a64f2c292": [
            "node_683"
        ],
        "e2d1be18-d49e-44b0-afad-09543c227297": [
            "node_143"
        ],
        "43387e42-b0ea-4a47-a11c-d24cc02eda96": [
            "node_515"
        ],
        "ec1a5f01-99bc-449a-9b60-ad8a02aa9a5d": [
            "node_378"
        ],
        "6a20718f-2d48-4f91-ac79-578a9a1b32b8": [
            "node_517"
        ],
        "c37da5ab-4c34-46cb-bb59-78f7c295e0fa": [
            "node_457"
        ],
        "7e8e0c55-a642-4a97-bd44-24bcb9e27aa8": [
            "node_504"
        ],
        "6e6b6059-4500-47a3-8309-d276a4e72671": [
            "node_155"
        ],
        "2bb8fd59-978c-423c-a99d-2c9e5f290ed4": [
            "node_396"
        ],
        "8144a67f-fb01-4df4-8a35-0af16d5f3192": [
            "node_557"
        ],
        "d5c4e059-8ed9-482b-a86c-52bf0d7bf05d": [
            "node_44"
        ],
        "d9f87f6f-42ec-49a8-95bc-72f1204476cf": [
            "node_690"
        ],
        "c82093ca-6f05-4018-a2c9-3b3f8ce51d8c": [
            "node_360"
        ],
        "87a863e5-6d6f-4ba7-ab3d-57eaef5f5c6a": [
            "node_419"
        ],
        "442c1906-a4a2-4169-832b-6830657b1784": [
            "node_435"
        ],
        "e458496b-69ca-4505-b804-c89329258c27": [
            "node_305"
        ],
        "646ce726-4a1a-4ad7-87ae-b20deef683cb": [
            "node_584"
        ],
        "1a189cbf-85e0-4eed-8d70-72ffda3958ae": [
            "node_751"
        ],
        "d54f08a8-681c-4843-983d-9ae6fa5ae68a": [
            "node_204"
        ],
        "66fb1368-5532-49c4-b32d-ed12c4ca9e8f": [
            "node_210"
        ],
        "ba09e090-a7c5-405a-991c-d9f8dd61d3da": [
            "node_5"
        ],
        "e2842bb7-2e3a-4320-a71a-17abd6e4753a": [
            "node_109"
        ],
        "e94fad44-3b49-4d9e-be64-420b1ebe4adc": [
            "node_611"
        ],
        "4ed20af8-66d2-42b5-8459-0a90daa27802": [
            "node_25"
        ],
        "3d0af99a-5102-4447-b7d7-8cfc3c62b3d9": [
            "node_15"
        ],
        "c1c758eb-b55f-4b5b-b508-7c1240c8cdff": [
            "node_11"
        ],
        "55381596-1c2d-4f25-aa26-b90b3ceffb5a": [
            "node_537"
        ],
        "bac45f0d-7031-4d78-8c17-2a54e0b4f000": [
            "node_213"
        ],
        "a8f3f022-e16c-4aae-97e9-7997aaa9abc7": [
            "node_224"
        ],
        "f65b10f5-bf0f-4218-ae90-255079ce83c3": [
            "node_601"
        ],
        "23142ac0-4a16-46d9-b6b5-ddcb73c7462b": [
            "node_381"
        ],
        "ae8bf696-509d-4f31-845b-b6a32584bf0f": [
            "node_99"
        ],
        "672bd33f-cb79-457d-a2f0-0d7e988f962e": [
            "node_22"
        ],
        "a71a33a6-8270-4d2b-8ce8-09e57806987e": [
            "node_79"
        ],
        "707c31e4-608f-4a0e-ac2f-923044ce1508": [
            "node_252"
        ],
        "92c9e722-254d-4bdc-9f67-e1b5d7830fb0": [
            "node_82"
        ],
        "143ef2b2-68ab-4070-ba55-a12c76a7a2e0": [
            "node_402"
        ],
        "86097e99-6975-4444-83d6-94f7ca4cdf0b": [
            "node_388"
        ],
        "99de4feb-4184-445a-9dd0-dd341ec6d116": [
            "node_456"
        ],
        "26c3f436-45df-4fe0-a10b-e4d7b6180b79": [
            "node_777"
        ],
        "3f464795-4cdb-44ab-b5a3-ff2fcaa4fc53": [
            "node_55"
        ],
        "b2bef92f-884d-4e94-90b8-09222f6722af": [
            "node_237"
        ],
        "da587849-e4d4-4117-9dee-965d4d48a112": [
            "node_177"
        ],
        "27e0454c-8aba-4304-836c-cfbec2fd8cce": [
            "node_591"
        ],
        "9f0dce07-ba35-4b8a-acc6-c462c391197c": [
            "node_370"
        ],
        "f883e5c1-b1ce-4ae8-bba8-ad66c4a22106": [
            "node_708"
        ],
        "15f8f5aa-e7a1-44db-ab0c-300493df28b4": [
            "node_230"
        ],
        "2cf7ccb0-83a4-4b6d-8333-79fa5d3c48ed": [
            "node_776"
        ],
        "0f2e235a-972a-4cc2-8d14-46e5fb2a2b5c": [
            "node_589"
        ],
        "096c4e99-1980-40bb-871f-a2d7b5441d13": [
            "node_539"
        ],
        "3a0ff086-76c9-4bcb-93a7-1428c647770a": [
            "node_140"
        ],
        "1381de24-4994-482a-8482-cc98826c4aa5": [
            "node_65"
        ],
        "f39b2604-bcf9-47fd-ae4f-4dfa5349fb74": [
            "node_656"
        ],
        "c0fb563a-71b6-44b4-be75-245387066cca": [
            "node_694"
        ],
        "b870f5d9-7415-4ae1-a1bb-cbd17adc6390": [
            "node_281"
        ],
        "23fa3883-7d1e-4047-b23d-66aeb73b607f": [
            "node_733"
        ],
        "8f036f5a-1e2e-4073-acf8-ff49266dc94f": [
            "node_545"
        ],
        "5d96d9a5-636f-47b6-a098-f590b6ef33d1": [
            "node_791"
        ],
        "2b3cdbb1-9942-4b2d-948e-98008a1d8363": [
            "node_666"
        ],
        "d1aca9d9-585e-41f6-95aa-10e69daaa0bb": [
            "node_114"
        ],
        "fabcf0ae-3c29-4f03-b4be-daaf77089e05": [
            "node_643"
        ],
        "1dc7c0bc-12f6-4e3c-ba59-bdbdedc2530d": [
            "node_497"
        ],
        "c5f43a5f-5eac-459c-8377-e282f33b07b8": [
            "node_351"
        ],
        "6d19bdba-b684-4dbc-95ec-fce5adb1bc4b": [
            "node_736"
        ],
        "5e801d22-b7ca-43fc-ae9e-187d16114ff5": [
            "node_607"
        ],
        "fc2bc1d3-685f-40dc-8a2d-3ef483ca02e0": [
            "node_309"
        ],
        "d41570eb-eb80-46e6-9d7d-f66a402b716b": [
            "node_742"
        ],
        "7d49ab7f-73a1-4ba0-88b8-f0463229b209": [
            "node_621"
        ],
        "c94d5ea6-9e56-4420-94f5-eaf7d98653fb": [
            "node_516"
        ],
        "8b8b2015-f3c0-4110-9888-04dec4f14256": [
            "node_604"
        ],
        "4a71b35a-5616-45b3-89ff-9ec3ac5d77f8": [
            "node_662"
        ],
        "b08d2c8e-64a0-4043-81ea-8283ffc62897": [
            "node_73"
        ],
        "b7af0128-2bcf-4824-a205-54c0cb28d9c0": [
            "node_28"
        ],
        "0e8956c5-6b05-45b9-94c1-7675dda99faf": [
            "node_126"
        ],
        "602d80cc-0b03-48bf-b6f1-61bf895e4c71": [
            "node_248"
        ],
        "dbfb339a-8d3c-422d-9735-5a889fb76a72": [
            "node_408"
        ],
        "f2151a1d-ee79-43db-bf46-41ad209a82eb": [
            "node_695"
        ],
        "17caab14-33d0-4954-9521-069e4ee87fe9": [
            "node_199"
        ],
        "4b7c11f1-db1a-4f4f-83db-ead19db48fc9": [
            "node_779"
        ],
        "76d8e554-a1dd-45cb-81c0-b28c60dd6fbe": [
            "node_414"
        ],
        "1764d90b-95ae-4b78-bc61-331b485d0e53": [
            "node_532"
        ],
        "d27832f9-f2b6-44cb-8003-1eab725d9a8b": [
            "node_172"
        ],
        "59a526ad-3daa-4a34-bf0e-0985be8ec738": [
            "node_363"
        ],
        "151f3a52-7ed6-42e0-9585-ae188b8c4faa": [
            "node_523"
        ],
        "45de60a2-f5d0-4fd2-b1f8-ee0dab67a589": [
            "node_61"
        ],
        "8b7e8d45-e808-4cb6-bd87-f73d6e81fd3b": [
            "node_316"
        ],
        "d82a387f-7c3b-4fe7-acb2-6b43cdcc56a8": [
            "node_182"
        ],
        "175f3a96-c3b0-4733-8136-391446ac5e47": [
            "node_740"
        ],
        "a40f45e9-21bb-45d1-8727-e7076343e4a5": [
            "node_340"
        ],
        "f841497e-49d7-4aaf-87e2-703ed4cf3ebe": [
            "node_785"
        ],
        "4bca71a9-2a8b-419e-8005-a0a3525b3bd9": [
            "node_781"
        ],
        "9009bee9-d14d-45c7-a163-a2f46b9c88b7": [
            "node_769"
        ],
        "6f3dcc07-dcf5-44af-a024-f19ab7a62453": [
            "node_139"
        ],
        "f7e32ce4-1da0-4b92-81a6-2e88c9fb7f8e": [
            "node_466"
        ],
        "1525d828-b470-4df6-829b-dc429371581d": [
            "node_391"
        ],
        "363c9944-cb72-4e62-a69b-dd59e4d5be9e": [
            "node_66"
        ],
        "100af6a7-03fb-41f4-ab57-823772205c2a": [
            "node_650"
        ],
        "8292fdb7-329d-499a-88d7-a37dfadb2b40": [
            "node_783"
        ],
        "c97f5148-3be9-43b9-b873-12652579337d": [
            "node_354"
        ],
        "22a704ce-d766-4fd5-82d5-54e7ffb20ba1": [
            "node_170"
        ],
        "afc71ec7-389d-4701-a022-2a38ce41a9c1": [
            "node_317"
        ],
        "4a56b1e5-fc3d-4efe-8b43-f4c3a84aba9e": [
            "node_49"
        ],
        "c4f58c10-7fe3-4596-b94f-b3d58cc1da60": [
            "node_166"
        ],
        "bff1c2f6-bd0d-454b-b291-b6dbfe29cab3": [
            "node_438"
        ],
        "f7c49b02-0793-4777-856f-30a312dcc1d3": [
            "node_352"
        ],
        "52695421-7042-469c-a028-51bb8c6780d2": [
            "node_228"
        ],
        "e19e8dd2-088c-4b39-a0ac-beafb78a14eb": [
            "node_434"
        ],
        "72a102ef-480a-4947-8a79-320bef748b03": [
            "node_685"
        ],
        "fcc3681b-2340-4441-9491-dd84d5726626": [
            "node_304"
        ],
        "ee4c7fc9-009e-4a00-b7d9-2a7ca0a4f995": [
            "node_245"
        ],
        "9a8d1f6e-fc97-4f30-acb2-d78fcce89b73": [
            "node_729"
        ],
        "d07235fe-4d8c-4c6e-9cca-7d02d50baebb": [
            "node_96"
        ],
        "7829f183-cf30-4bab-b70a-d56b5c06e19d": [
            "node_447"
        ],
        "27a2d622-a98d-4705-890c-1fde49cf0877": [
            "node_298"
        ],
        "f7cde762-76a5-4744-a18c-fcf5e6a4e6d3": [
            "node_26"
        ],
        "095bb659-9c92-4fbb-81af-5bd2770a0006": [
            "node_667"
        ],
        "8ae7c066-1e8e-4cf4-8759-52cc467b46f0": [
            "node_626"
        ],
        "7680a7f4-dc5e-488d-9d23-ff0e570d3a60": [
            "node_256"
        ],
        "1e9c4b0c-7510-4f41-910d-7dee83dde03e": [
            "node_56"
        ],
        "4e2534f2-c5da-4503-8a8d-e849958e9bb0": [
            "node_193"
        ],
        "2243a3f7-1f4b-4fa4-945d-8963b79c906c": [
            "node_620"
        ],
        "caff1f62-d690-4b06-8ab8-4859e3607fb9": [
            "node_13"
        ],
        "b42ba1ea-faf0-4659-973c-0d8f3e364f72": [
            "node_501"
        ],
        "e9788fe3-4969-4bac-96d0-9b3b0fb46e2e": [
            "node_717"
        ],
        "c2bde0ac-9319-49aa-857b-df0a90589523": [
            "node_337"
        ],
        "75a7d22f-6a5a-4f4f-bda7-63bf4ffc5c66": [
            "node_518"
        ],
        "ae663efd-9c2a-4f83-aec7-ed6200fe20ab": [
            "node_763"
        ],
        "721f007b-0827-4d7d-8f28-d6f4187ff6e3": [
            "node_570"
        ],
        "0a849e78-4e9f-491b-a757-113e5132171d": [
            "node_625"
        ],
        "e24aa1d6-8a36-48b6-91ae-c9686e3b5f4a": [
            "node_651"
        ],
        "77057bd8-6bb3-4755-8ac6-1a13755a2b7f": [
            "node_719"
        ],
        "b93eabcc-a058-4d07-b325-56a384a8e19f": [
            "node_735"
        ],
        "a34fcefb-4ba6-43db-8804-6514e71533be": [
            "node_336"
        ],
        "4347f00e-f843-496e-a56e-32f2c71ae6d8": [
            "node_463"
        ],
        "1510b271-bdd8-4fd3-b10e-b81230671707": [
            "node_78"
        ],
        "489ff7a2-7a9b-4814-9e4e-c45dc78e11d6": [
            "node_579"
        ],
        "d4ad3b6a-2959-4778-b7c2-b215fdb0b74a": [
            "node_280"
        ],
        "bccab39c-b77b-4ad8-a042-20471786ef88": [
            "node_196"
        ],
        "b2b6381a-7a34-46b2-99e2-be78e9593000": [
            "node_483"
        ],
        "04f6f868-29a5-495c-9315-dae249148e40": [
            "node_399"
        ],
        "536e15e3-8223-4f06-a925-d2c31183a7c0": [
            "node_8"
        ],
        "202854ec-86df-49fe-9b24-e5f51bd14fd8": [
            "node_19"
        ],
        "3a319d2c-4952-4f61-a49d-10c37b5ad9a8": [
            "node_270"
        ],
        "36b73d2a-7ea1-42b3-85c7-01915a8845a1": [
            "node_760"
        ],
        "501c1ad9-6593-4350-9ca9-4b1f2750ad24": [
            "node_533"
        ],
        "c789a0f7-67b0-4f4b-9185-37c88ec8c117": [
            "node_87"
        ],
        "7034607b-719d-47d7-85ea-03b7e893fc69": [
            "node_661"
        ],
        "fbabff75-2cc5-410e-893f-7d0143c2152b": [
            "node_712"
        ],
        "405d3779-990a-4b42-ab11-553058c1bdf9": [
            "node_80"
        ],
        "f59ec565-394f-4e6d-acb1-935ac1041700": [
            "node_417"
        ],
        "0028e2ca-cd3a-49a2-89a2-4bcc2ee65711": [
            "node_373"
        ],
        "9c28cb17-de67-4067-9827-4f9e10d9b27e": [
            "node_188"
        ],
        "1ed8b85e-9265-4346-872e-18f10473dbec": [
            "node_526"
        ],
        "821f6235-c87f-47ea-870f-216154e7cdad": [
            "node_413"
        ],
        "408c2365-508e-4389-8402-d61e4bab9f43": [
            "node_48"
        ],
        "90e77d24-bdcb-4516-9bc9-48b7be9c37d2": [
            "node_701"
        ],
        "9d8fc254-de74-4a42-bb06-e41a60044fb0": [
            "node_175"
        ],
        "6f623aa1-64cd-4746-a402-3fdeecaadcd3": [
            "node_181"
        ],
        "caea8ab7-e1b9-45f2-b785-4f701ac66388": [
            "node_59"
        ],
        "733ff232-4b3a-4123-afe1-5c404ca4ab85": [
            "node_324"
        ],
        "f8fe5e0f-f30d-4fa2-a881-389245d85592": [
            "node_273"
        ],
        "31d31c1c-4c8a-433b-86bf-f226cf74151a": [
            "node_741"
        ],
        "fd69d20c-ebbc-48e2-ad96-4903e1bc5ef8": [
            "node_752"
        ],
        "603e0ed8-476a-48b8-8d05-30c14cbefae9": [
            "node_578"
        ],
        "0fbbfd91-6af4-491e-b9a6-b4264e1d3798": [
            "node_541"
        ],
        "38402f33-a582-46bb-a84d-29d2e14518d0": [
            "node_326"
        ],
        "f290ce16-2013-47f4-932a-c1a0e8987d84": [
            "node_377"
        ],
        "4c0d16d7-8004-441f-bc5e-35db271d768f": [
            "node_540"
        ],
        "85546611-a298-4de6-a36c-00347876029e": [
            "node_449"
        ],
        "45b870c1-1a57-47de-abea-dfcd07cc7774": [
            "node_226"
        ],
        "66c6e72a-0f2c-47cd-a1cd-6d4034790331": [
            "node_365"
        ],
        "eb9c9d69-94f1-48d0-9ee5-d95f1bff4adb": [
            "node_141"
        ],
        "80a7917e-24e8-43fb-a7e4-07eaae2459f1": [
            "node_543"
        ],
        "75a9eec5-3e85-40fa-a63d-b854a55536a7": [
            "node_296"
        ],
        "8916f31e-d5a4-4391-a62d-f71296056593": [
            "node_382"
        ],
        "501aa46e-7289-4882-a441-12bb1b0cdbca": [
            "node_164"
        ],
        "88295cde-c862-42ec-a6e8-48d23d2f103a": [
            "node_335"
        ],
        "ff831781-0363-41f0-9a48-dfdc933e2278": [
            "node_37"
        ],
        "95c0321e-c72e-4258-a7e0-3ba00e1a3bdf": [
            "node_710"
        ],
        "02a41428-e5f2-4cea-ad1b-7f4378530d09": [
            "node_94"
        ],
        "b6548a5c-1de0-4919-9e03-be81858e04d4": [
            "node_218"
        ],
        "96d2e64f-ed7b-4f7a-ba32-dbdc98a5d29f": [
            "node_313"
        ],
        "c440a139-30ba-4052-851f-d35a15240689": [
            "node_191"
        ],
        "8372f0e1-1882-424f-bd91-2504196da41b": [
            "node_767"
        ],
        "63e54eb1-59c1-4250-beb5-b3082874f9ea": [
            "node_490"
        ],
        "46a067f2-86c4-4334-a3e8-a355ed624118": [
            "node_499"
        ],
        "2e0e53af-c6da-4e87-831b-ff0447dead2d": [
            "node_272"
        ],
        "b74515f3-7d64-46e8-aee5-4458a87c8629": [
            "node_263"
        ],
        "3691566f-2474-4213-a820-04edd615f460": [
            "node_544"
        ],
        "2328f33a-4a6e-44aa-b55e-0aa1ecbdc168": [
            "node_311"
        ],
        "9954c15f-a5b2-478a-a22b-d0b8ebc687a9": [
            "node_437"
        ],
        "a6bacf5a-32fa-4587-a959-083fa2e6451f": [
            "node_251"
        ],
        "117bc711-d219-4ad3-8639-82333b2448a4": [
            "node_18"
        ],
        "11ac046d-2169-4280-bc96-8bf595d13371": [
            "node_30"
        ],
        "0a6d482f-d166-4b29-9363-69a4b48f61a9": [
            "node_384"
        ],
        "2dddc65b-669b-49b7-ace4-72ade5b587a5": [
            "node_392"
        ],
        "61410a75-02b3-458d-aac9-ab6ef0084fc4": [
            "node_364"
        ],
        "bbc6b839-1982-4855-974a-3466ff92c21b": [
            "node_52"
        ],
        "0b1b3f1e-f6d5-4502-84f4-3dd60907aa80": [
            "node_306"
        ],
        "bf383667-54fa-468b-9556-75d6c4251027": [
            "node_286"
        ],
        "7a7e76c6-27f7-4312-a706-6125f0df172c": [
            "node_679"
        ],
        "489afe5c-34bb-426c-807b-15a93984c089": [
            "node_520"
        ],
        "30e28d31-b37c-49cb-a79a-8b31112e2a9b": [
            "node_494"
        ],
        "23dd1ca6-9e59-4008-8c68-fd2bd5f311ad": [
            "node_312"
        ],
        "38dd61f2-3aaa-4d6f-bc0f-73ed4a33c173": [
            "node_692"
        ],
        "c8a26e75-56ff-41a9-baa2-ca76e58ae89f": [
            "node_357"
        ],
        "ee49b661-68da-42f3-8817-35677954994c": [
            "node_510"
        ],
        "6fcc61b0-b86f-4915-bbbb-0ff39939ed33": [
            "node_264"
        ],
        "d010ba14-1509-42eb-9475-0c8cde258194": [
            "node_563"
        ],
        "349bb2cc-9301-4028-85ed-15d0eabf2213": [
            "node_542"
        ],
        "5c4d6304-25af-496c-992a-e3d7d8cd5986": [
            "node_546"
        ],
        "5dd7de3b-5675-49f5-a58c-667e7d83af21": [
            "node_503"
        ],
        "71702e39-3845-4305-94c3-9ae929d35d53": [
            "node_200"
        ],
        "5544a4b5-5214-409a-99dd-893090d75bc1": [
            "node_635"
        ],
        "b7bd123a-e0be-45c0-9d8a-9607417a6627": [
            "node_134"
        ],
        "2763f044-c6a1-41f2-a619-d8e9a7ecfadc": [
            "node_600"
        ],
        "f8ebdd42-d381-42f1-8cbe-1cbb5148ce50": [
            "node_687"
        ],
        "3db278d2-332f-42a7-8d52-97849a100db5": [
            "node_610"
        ],
        "5621f5f1-1955-474c-ab26-80d3dca4b030": [
            "node_93"
        ],
        "f7d5927f-ec2f-4a77-b31f-280e00606398": [
            "node_583"
        ],
        "d28ca814-47b0-4269-8d67-8d24307ac181": [
            "node_112"
        ],
        "9b30cd4a-8adf-4d5b-8194-93efe88fb4cf": [
            "node_772"
        ],
        "474fdb5d-bb71-4553-bd72-102bfeb8beed": [
            "node_443"
        ],
        "43dfb027-3b4f-4452-906c-43f900375965": [
            "node_100"
        ],
        "142df164-1261-42b7-9c92-21e9db8cea2c": [
            "node_568"
        ],
        "821ee8c2-b785-40d5-8a27-f35d16bc644b": [
            "node_693"
        ],
        "272c1b52-1faa-4b8a-8c9e-b73b9e953c5e": [
            "node_70"
        ],
        "86dd27f1-f575-452e-a81a-449aa851c291": [
            "node_571"
        ],
        "6fc7accf-bccf-4c7f-959d-ac3a48003a9f": [
            "node_782"
        ],
        "4f8ec1c8-4266-428b-8618-7cd771a3a4e6": [
            "node_138"
        ],
        "4bfb2868-7fbc-4744-aa76-6f33fb42afad": [
            "node_279"
        ],
        "4e08043a-bf2d-499f-9506-05fc9c75421b": [
            "node_405"
        ],
        "1d699e5c-c5e9-4317-96be-f11c39041aa6": [
            "node_770"
        ],
        "3e5cbd3d-fd69-4f75-aced-2d769d8a6551": [
            "node_455"
        ],
        "cba49c3d-e90f-45bb-9352-471000bf748b": [
            "node_453"
        ],
        "ada89592-ddf1-4725-8e99-5fdb99bbcf59": [
            "node_72"
        ],
        "eb9ceba5-12f8-4eb1-952b-b0dfa475d39e": [
            "node_221"
        ],
        "dee83bb3-6c1e-493f-ad62-c1b52469dd8d": [
            "node_169"
        ],
        "6bca4ecf-0d0b-46ba-9592-f7ab13594e35": [
            "node_723"
        ],
        "56d94236-a737-49e2-be2d-4ed832e396e2": [
            "node_209"
        ],
        "7bbd2855-3c1e-4213-b148-21fb8709e324": [
            "node_398"
        ],
        "ab636cd2-d97c-488a-8bae-521fb6670de9": [
            "node_266"
        ],
        "141c96ec-4cd9-4582-b9dc-9376b1b0e858": [
            "node_121"
        ],
        "c90d7446-652d-4db3-ae53-3e734c7d7058": [
            "node_54"
        ],
        "a7bc20c0-de85-4734-a1a5-236da4473165": [
            "node_34"
        ],
        "1d00e05d-d467-4180-bc07-06af6ae9a3b2": [
            "node_649"
        ],
        "4c6ccf84-5f36-4f8e-a8e6-e5cf4b13e163": [
            "node_1"
        ],
        "bf981461-c2fe-42fb-8cf3-c7f7948ff2f6": [
            "node_530"
        ],
        "c1e04431-37b3-4a08-802d-7db862050481": [
            "node_156"
        ],
        "ff2ee5c1-6ec1-4944-b317-316168aba33e": [
            "node_721"
        ],
        "4353b30a-b35d-4b13-8388-580079876ccd": [
            "node_287"
        ],
        "2b8e97f9-6254-45d5-baab-35cc7154deff": [
            "node_102"
        ],
        "de127110-e84b-4862-9f78-313e933afb9e": [
            "node_535"
        ],
        "a83b36df-3b5b-4ac6-b0a4-d48a9a854453": [
            "node_359"
        ],
        "07db0c84-74da-4a4f-ad8d-e89d9fb06444": [
            "node_400"
        ],
        "5d726516-e522-4c3c-94d0-03d047898fad": [
            "node_633"
        ],
        "87ac07d3-3aa8-4282-ad57-ac116126d484": [
            "node_183"
        ],
        "d1a4dc2e-4377-4782-9f18-45408106aa1d": [
            "node_348"
        ],
        "843bd722-9bc8-4e4d-886e-79daf009e64e": [
            "node_107"
        ],
        "023b50d2-fe4a-4287-a26f-533bb97b168c": [
            "node_734"
        ],
        "d77c8dfa-d5d3-4bb3-b5c4-1fdf03aa63f4": [
            "node_590"
        ],
        "60df6eb2-851f-4764-ab14-f0cf22c1e307": [
            "node_303"
        ],
        "453a1fa1-06b3-4f5c-ae25-178765d4098f": [
            "node_91"
        ],
        "157d84b5-a9b8-4a01-ad63-01c4c4cc9b96": [
            "node_473"
        ],
        "cc087a00-dc37-43e2-a587-443629ca53e8": [
            "node_745"
        ],
        "aaccffc4-6c57-4f74-aa49-4cef8977fa47": [
            "node_479"
        ],
        "daac09ce-7f60-4355-9cbb-ae9b51ec5e62": [
            "node_448"
        ],
        "3d957d2f-5954-4afe-a336-21d73059a7b4": [
            "node_511"
        ],
        "317168ea-6565-4e26-b3bb-b68f572164d7": [
            "node_519"
        ],
        "fdf9e079-76ba-4e02-8581-be910fb7f0ee": [
            "node_566"
        ],
        "660e33d6-1165-4e64-93ab-b77fc0c9a824": [
            "node_229"
        ],
        "5c5045e9-ba06-4c95-acb8-88c0d7076977": [
            "node_297"
        ],
        "ccdad3c3-6616-41e8-bfb1-b06d13a2774f": [
            "node_308"
        ],
        "988fd7cd-b0d0-4709-9312-1005e8401760": [
            "node_467"
        ],
        "dfe8f3e9-722f-48a8-9e50-d722551f1092": [
            "node_576"
        ],
        "0a82e564-6a6b-44a6-8404-81669d657422": [
            "node_307"
        ],
        "d0c3b659-2576-4b58-8993-d778db068d8e": [
            "node_120"
        ],
        "4963477f-b6fd-47bd-a81d-2571334dd57d": [
            "node_187"
        ],
        "6c8fee5f-3229-4b50-8324-30a72e1a3f26": [
            "node_241"
        ],
        "931e3f07-24a8-41cb-b120-f162d8004946": [
            "node_775"
        ],
        "b3092744-0dbb-42cf-8c55-05d5109c8d74": [
            "node_90"
        ],
        "1e344f4f-12c0-47f9-bca9-b36654fb8678": [
            "node_653"
        ],
        "e21882ee-65ab-4c72-bddb-d91a43473d9f": [
            "node_385"
        ],
        "2475ff94-7594-4984-af71-e888b944c784": [
            "node_361"
        ],
        "2ae00bc1-0bd4-4c5a-bf8e-12949242e2bb": [
            "node_474"
        ],
        "f64fff76-91cc-4434-9687-25ba0d88537d": [
            "node_464"
        ],
        "4cea187e-0103-4b99-a19a-b30042a890c2": [
            "node_282"
        ],
        "d4a4c1a5-e088-4345-afcc-d3ea8a5873ca": [
            "node_395"
        ],
        "1a7b9607-ac7f-45e0-b668-28a8a92cd992": [
            "node_756"
        ],
        "6d9ae381-bfad-41e2-a4f1-02951e6f29e9": [
            "node_581"
        ],
        "86b729c1-f62f-4e2c-846f-7612979df0fd": [
            "node_123"
        ],
        "f313e035-0e3c-45bb-9751-0da3a6bb20b1": [
            "node_418"
        ],
        "5132719e-a445-4e66-9dcb-7f5673780074": [
            "node_613"
        ],
        "0b2bfe9f-a14e-4b94-a6f2-76a982a3626b": [
            "node_338"
        ],
        "fabfbb41-0afe-4e9e-9627-126a7590c171": [
            "node_243"
        ],
        "e46f900a-0aca-480d-b243-f31507b017d3": [
            "node_261"
        ],
        "3d3c323e-f235-4b6c-b20e-37c1665a2bdc": [
            "node_703"
        ],
        "f8d2188b-bc18-45ba-9576-6f8ac017a33d": [
            "node_630"
        ],
        "b1019398-c335-4729-88c3-76ae770adede": [
            "node_168"
        ],
        "763a3cf8-7e5a-4bf5-871b-f47a42cc7bff": [
            "node_574"
        ],
        "3b06ad53-a086-4b39-971d-52ae51f34f41": [
            "node_482"
        ],
        "7ed94873-74d4-4d55-802c-6a3f898f3984": [
            "node_521"
        ],
        "245102a9-4365-4200-bca0-1b2b13c3718a": [
            "node_631"
        ],
        "c08a0406-db6e-4fba-951e-ac9931ab798c": [
            "node_582"
        ],
        "823c65ce-19c5-4ee5-81aa-96117dfd98f0": [
            "node_784"
        ],
        "a87fe19a-6aec-41f2-9e97-d10d0fca5fed": [
            "node_534"
        ],
        "8558fe16-2d48-4bfe-a7b7-403d1b2e24f5": [
            "node_147"
        ],
        "59267e35-8670-4d3b-a6d3-81a90712d5e0": [
            "node_671"
        ],
        "c62284f6-7233-4305-bbc3-ff14b13fb209": [
            "node_151"
        ],
        "7b1438ed-43be-4a56-a23d-9baafe0c55c7": [
            "node_759"
        ],
        "0314b4d6-b81c-4505-a1bf-45cbd921e96b": [
            "node_472"
        ],
        "d9430fb0-6f8d-4146-b8cd-3f9db29ab2af": [
            "node_203"
        ],
        "776020e8-dc1f-4814-83f3-b0e7a4801840": [
            "node_634"
        ],
        "d940f088-5fe3-4727-a92b-4a3366ac09a7": [
            "node_486"
        ],
        "4dbfeacc-37ca-49ad-80ac-7a572f16cdd6": [
            "node_705"
        ],
        "0b67b601-6e1f-4b38-a963-17d9c57f7a04": [
            "node_142"
        ],
        "79e5b999-0e35-47f9-aa6e-8a4426ed7611": [
            "node_176"
        ],
        "ea27271b-69db-4956-860f-5b3a529deebe": [
            "node_353"
        ],
        "df9981f9-e9d1-4ad1-86b4-5402e6815cfe": [
            "node_682"
        ],
        "c87e839f-bbba-4110-a250-cb1ab09e949b": [
            "node_108"
        ],
        "d435a10a-b99f-403f-9e0a-fc2c5141668e": [
            "node_202"
        ],
        "9972e665-a441-4fa6-9b50-00c18f01215f": [
            "node_322"
        ],
        "203e710e-3857-40dd-b7d6-a4e62e25ed5a": [
            "node_676"
        ],
        "2e2dc69a-5d85-479d-9117-e405c5c5d54d": [
            "node_67"
        ],
        "093dd74c-7a30-42da-8167-d2e146f23b94": [
            "node_612"
        ],
        "a2022dc6-7d2f-499d-acb4-8672c8422989": [
            "node_329"
        ],
        "54ea2c71-440b-4de4-a9da-4926170f19d7": [
            "node_355"
        ],
        "f35af8a0-b5c4-4a12-b871-60368b993e32": [
            "node_242"
        ],
        "62b5f3b7-be31-46ac-a2be-78d313264b75": [
            "node_452"
        ],
        "ffb7b5a1-4d71-44e5-bc4e-59957957ff18": [
            "node_406"
        ],
        "6d7db650-23b1-4269-98ef-6dc3925aa8c7": [
            "node_522"
        ],
        "f9b323f2-c738-4159-909a-08691ccb0a13": [
            "node_445"
        ],
        "698c7a35-274a-4dce-969a-8721e95afe83": [
            "node_704"
        ],
        "2cebcdfa-b8e0-4349-86b8-68554a245a55": [
            "node_552"
        ],
        "818c7367-3215-474b-b2bb-8e36f24c555e": [
            "node_290"
        ],
        "be5155f5-fb34-4e6b-83c6-0c8d18ba7da8": [
            "node_267"
        ],
        "4d9b79e8-b682-47a3-bf13-268db9d3e66c": [
            "node_422"
        ],
        "3d5b3f79-23d0-4025-940a-1eacd5a12512": [
            "node_415"
        ],
        "0d50b93b-ebb5-4efe-88a4-006c23f68991": [
            "node_165"
        ],
        "3d5ba4a5-df4b-40d7-82d5-7f9b482b8ed8": [
            "node_247"
        ],
        "c7e0b77c-0495-49ff-b3d9-d62ca752cddd": [
            "node_173"
        ],
        "eb7f1cd1-a5c3-4974-92d2-7def6fa42763": [
            "node_609"
        ],
        "ca3a9bd3-d42e-4e53-879e-8841861c8cf0": [
            "node_750"
        ],
        "2094a3de-efa8-4a54-b429-19e45cd3d035": [
            "node_128"
        ],
        "b52c3f64-9b01-4f02-87a8-d8c1c1abb558": [
            "node_514"
        ],
        "79321473-bd14-4b6d-85f2-0c1f8f5981b4": [
            "node_21"
        ],
        "ffd891c8-86a9-4498-87be-610891becbed": [
            "node_450"
        ],
        "260deea8-40e8-4990-ad10-9ba036fcac9d": [
            "node_343"
        ],
        "15d16beb-728c-46ad-97af-0975ba4038a0": [
            "node_724"
        ],
        "b291ca08-91f0-4908-97e6-3c49cd94f79d": [
            "node_599"
        ],
        "cf6c58ff-07e6-44af-825e-606665d47cac": [
            "node_233"
        ],
        "82e7614f-499a-45af-9f20-529cc29c7f5c": [
            "node_14"
        ],
        "51bee377-a3af-4588-9b81-be364e227a1b": [
            "node_714"
        ],
        "1d375278-a498-4e82-857b-bf7282a9e49b": [
            "node_628"
        ],
        "17da4412-7d93-4677-971a-2ab7f82475fd": [
            "node_658"
        ],
        "bd2f66a9-bc61-46e2-a9ba-41346ee6fec2": [
            "node_346"
        ],
        "8295cc69-4c87-41ce-b4d4-590714242aed": [
            "node_556"
        ],
        "d7828361-e1e4-4ffb-9a1a-0b0f6918100a": [
            "node_163"
        ],
        "1921b802-dc3a-4910-8d17-e13fbfd30dd4": [
            "node_234"
        ],
        "c3eda931-423e-4cad-8ba9-fb80a288120b": [
            "node_531"
        ],
        "f751aa91-a960-4e80-a7a9-19869a5c7694": [
            "node_598"
        ],
        "69953bcf-62ee-42bc-a3d8-892898de1072": [
            "node_220"
        ],
        "e3244299-254b-47f1-9a13-a17412434d00": [
            "node_29"
        ],
        "513b4ce3-020d-4adc-bdb6-b02db3a4bb18": [
            "node_285"
        ],
        "954f980f-e59f-42cf-8eba-c5846f45597a": [
            "node_470"
        ],
        "1b8b6cbe-b8d8-4e8d-8be1-ce61016ddb1c": [
            "node_558"
        ],
        "be34069e-748f-4742-b06a-96d3a5f46f13": [
            "node_680"
        ],
        "f310e93f-686a-4db8-939b-fe064341d510": [
            "node_458"
        ],
        "3722f3dc-3558-4773-b5e5-b5c7679915b7": [
            "node_97"
        ],
        "7ee9d7d2-f43a-47f0-8d04-089dcd6c831a": [
            "node_647"
        ],
        "48f45117-f77d-4611-806a-567063f18558": [
            "node_284"
        ],
        "5c6e2b0e-e4e5-439c-992a-067b0199f3f2": [
            "node_446"
        ],
        "6ea9dc6b-6f91-41a4-a23c-2d00e5485c37": [
            "node_746"
        ],
        "83279bae-431f-42a7-a7e8-353ae987fb0e": [
            "node_484"
        ],
        "091b6ca6-9799-4cac-bf4e-5b7160faa9e8": [
            "node_491"
        ],
        "f1c1feef-e840-4bf5-b775-e8bb059293d9": [
            "node_43"
        ]
    },
    "mode": "text"
}